# DSM cockpit review

## You (2025-08-26T23:23:45.218000+00:00)

Produce an expert, field-realistic critique of the **DM Cockpit (Summary)** section of a PRD for a Palo Alto Networks District Sales Manager (DSM). Determine what helps a DSM *defend commit on facts*, where the design is thin or risky, and the smallest set of changes that most improves: **WAPE ≤ 8–10% at T-14, Commit Accuracy ≥ 90% at T-7, inspection time ↓ 40%, slip from Commit ↓ 30%.**

## Inputs (paste the subsection + refs)

* **PRIMARY INPUT — DM COCKPIT TEXT**
  _____
DM Cockpit (Summary)

Primary JTBD: “Scan path-to-plan, spot risk, act.”

Sections & content:

Path to Plan: Closed, Commit, Best Case vs Plan; delta vs last week.

Coverage & Activation (leading signals):

%Territories filled, #Reps in seat, %Reps ramped (capacity → pipeline presence).

%Reps in Deals (by pillar) and %Accts w/ active pipe—most proximate predictors of bookings.

Technical Motion Quality: Tech Win %, POV Win %, Median POV Duration (evaluation outcomes + cycle time).

Exception List (click-to-inspect):

Stale stage > X days; close-date churn; Commit without Champion/EB; unmapped paper process; InfoSec not started; low POC engagement; renewal ≤120 days with negative health.

Team strip (rep cards): Attainment band, active deals, hygiene flags, coaching cue.

AI roles:

Summarize: “What changed since last week? Which drivers explain variance?”

Assist: “Suggest 3 actions for highest-risk deals and who owns them.”
(Automations like agenda generation live in Forecast; keep Cockpit light.)

Success signals (for design): cockpit adoption ≥ 85%, inspection time ↓ 40%, % Commit with evidence ≥ 95%.


_____
* **REFERENCE A — Visual Anchor (optional but preferred)**
  A cockpit screenshot/layout reference (filters, path-to-plan header, rep/district strip, on-track/critical chips). Treat this **only as UX grammar**; do **not** trust its numbers.

* **REFERENCE B — Metric Dictionary (optional but preferred)**
  Short text listing metric semantics and causal chain (e.g., %Reps in Deals, %Accts w/ Active Pipe, Tech Win %, POV Win %, Median POV Duration; coverage/readiness → pipeline presence → attainment; exec engagement & technical quality shift conversion/ASP).


## Assumptions & Defaults

* Live **inside CRM**; we **unify**, not re-platform.
* **Evidence > opinion**: Commit entries require proof (Champion/EB, Decision Criteria, Paper Path, InfoSec/Legal, POV/Tech-Win).
* **AI modes**: Summarize (variance, drivers), Assist (next actions w/ owner), Automate (low-risk, reversible, auditable).
* **Delta-first** UI: show WoW changes everywhere; exceptions open a right-rail **Evidence & Actions** drawer.
* If a reference is missing, proceed on industry-standard DSM heuristics and **flag assumptions**.

## 5) Analytic Tasks (do all, in order)

1. **DSM Value Snapshot (≤7 bullets)**
   What in this cockpit lets you confirm or defend commit in <5 minutes? Name sections/tiles and why (Path-to-Plan, Activation, Technical Motion, Exceptions, Team strip).
2. **Gap Analysis (prioritized)**
   What’s missing/ambiguous for a decisive weekly inspection? For each gap:
   **Gap • Why it matters • Evidence source (CRM fields/notes, POV, InfoSec) • Best UI affordance (badge/chip/right-rail slot) • Likely impact on WAPE/Commit.**
3. **Metrics & Thresholds Check**
   For each proposed metric (**%Reps in Deals, %Accts w/ Active Pipe, Tech Win %, POV Win %, Median POV Duration** and any others in the input):

   * Is it predictive at DSM level?
   * Propose **RAG bands** (with rationale; mark as *inferred* if based on field heuristics).
   * Specify the **DSM action when red** (owner, due-date, next artifact in evidence trail).
4. **Exception Design Review**
   Do exceptions cover top slip drivers (stale stage, close-date churn, commit w/o Champion/EB, unmapped paper path, InfoSec not started, low POC engagement, renewal ≤120 days w/ negative health)? What **1–2** key exceptions are missing for PANW? For each exception: list the **minimal evidence slots** the right rail must show.
5. **AI Modes Scope & Boundaries**

   * **Summarize:** two weekly questions it must answer (variance + causal drivers).
   * **Assist:** top 3 actions per highest-risk deal, with **owner** (Rep/SE/CSM/DSM) and artifact (email/task/note).
   * **Automate:** what stays in Cockpit (safe, low-risk, undoable) vs what defers to Forecast & Inspection (e.g., call agenda scaffolds).
6. **Adoption & Load**
   Estimate inspection time using this cockpit; identify click/read hotspots and propose **one-tap** fixes. Call out any **new admin work** the design might accidentally create and how to avoid it.

## 6) Return Format (strict)

Output **only** the following Markdown sections, in this order:

**A. Executive Brief (≤120 words)**

* One-paragraph verdict on decision-readiness & forecast integrity.

**B. Scores (0–5)**

* Decision Readiness: *x.x*
* Evidence Rigor: *x.x*
* Forecast Impact: *x.x*
* Activation Coverage: *x.x*
* Exception Handling: *x.x*
* Adoption & Load: *x.x*
  *(1–2 sentences total on the biggest score driver.)*

**C. What’s Good (≤5 bullets)**

**D. Gaps & Enhancements (table)**
\| Gap | Why it Matters | Evidence Needed | Proposed UI | Expected Impact on WAPE/Commit |

**E. Top 5 Actions to Ship in v1 (ranked)**
For each: **Action • Owner • Effort (S/M/L) • Risk • Why now**

**F. Open Questions (assumptions to resolve)**

**G. Appendix — Metric RAG Bands & Actions**
List each metric with **R/G/A thresholds**, **owner**, and **next step** when red.

> **Do not include your scratchpad or chain-of-thought.** Keep explanations crisp and tied to outcomes.

## 7) Warnings & Guardrails

* If any data or reference is missing, **state the assumption explicitly** and proceed; do **not** fabricate numbers.
* Distinguish **field-proven patterns** from opinion/speculation. Tag speculative items with “*(speculative)*”.
* Tie every enhancement to a measurable outcome (WAPE, Commit accuracy, inspection time, slip).
* **Automation must be reversible and auditable;** any irreversible step is out of scope for Cockpit.
* Respect **scope: DM Cockpit only**; if suggesting a Forecast/Hygiene feature, label it “*belongs in module X*”.

## 8) Evaluation Rubric (for self-check)

* **Decision Readiness:** Can a DSM defend commit in <5 minutes from this cockpit?
* **Evidence Rigor:** Are Commit entries anchored to Champion/EB, Decision Criteria, Paper Path, InfoSec/Legal, POV/Tech-Win?
* **Forecast Impact:** Would the changes plausibly move **WAPE ≤10% (T-14)** and **Commit ≥90% (T-7)**?
* **Activation Coverage:** Do coverage/readiness signals (by pillar) map to pipeline presence & bookings?
* **Exception Handling:** Do exceptions reflect top slip drivers with one-click inspection?
* **Adoption & Load:** Does the design cut **inspection time \~40%** with no net new admin work?

## 9) Final Checklist

* [ ] I used delta-first reasoning and exception-first scanning.
* [ ] Every gap has **why**, **evidence**, **UI**, **impact**.
* [ ] Each metric has **RAG bands** and a **DSM action**.
* [ ] I separated Cockpit vs Forecast/Hygiene scopes.
* [ ] I proposed exactly **Top 5** v1 actions with owner & effort.
* [ ] I stated assumptions for any missing references.













## ChatGPT (2025-08-26T23:24:39.774003+00:00)

**A. Executive Brief (≤120 words)**
Good start: the cockpit orients on Path-to-Plan, activation, technical motion, and exceptions—useful to scan and ask “what changed?” But it’s not yet decision-ready: Commit lacks an explicit **evidence trail** (Champion/EB, decision criteria, paper/InfoSec path), evaluation quality is shown only as rates (no DoR/KPI scoreboard), and there’s no **transact path** view (CSP/partner/vehicle, signers, calendar). Add WoW deltas everywhere, stage-aging thresholds, and one-tap right-rail **Evidence & Actions**. These fixes would let a DSM defend Commit on facts in <5 minutes and materially improve WAPE (T-14) and Commit accuracy (T-7), given the proven linkage of activation and technical motion to conversion and cycle time. 【7:0†Source.file†L33-L37】

---

**B. Scores (0–5)**
- Decision Readiness: *3.2*
- Evidence Rigor: *2.6*
- Forecast Impact: *3.4*
- Activation Coverage: *4.0*
- Exception Handling: *3.5*
- Adoption & Load: *3.8*
*Biggest driver: absence of a Commit evidence trail and transact-path/calendar view depresses decision readiness and forecast integrity.*

---

**C. What’s Good (≤5 bullets)**
- **Path-to-Plan + WoW delta** at the top aligns to the DSM scan.
- **Activation signals** (% Reps in Deals; % Accts w/ active pipe) are predictive at district level. fileciteturn1file2L39-L45 fileciteturn1file3L51-L56
- **Technical motion** tiles (Tech Win, POV Win, Median POV Duration) speak to conversion and timing. fileciteturn1file0L5-L10 fileciteturn1file0L14-L19 fileciteturn1file0L23-L28
- **Exception list** hits several top slip drivers (stale stage, close-date churn, Commit w/o Champion/EB, InfoSec not started).
- **Team strip** with hygiene cues encourages manage-by-exception.

---

**D. Gaps & Enhancements (table)**
| Gap | Why it Matters | Evidence Needed | Proposed UI | Expected Impact on WAPE/Commit |
|---|---|---|---|---|
| **Commit evidence trail missing** | DSM can’t defend Commit without proof of Champion/EB, decision criteria, paper/InfoSec path | CRM contacts & exec meetings, MEDDPICC fields, paper-process map, InfoSec/Legal status, recent EB/notes | Right-rail **Evidence & Actions** with mandatory slots and freshness chips; one-tap tasks to fill | **High** – reduces Commit error & slip; boosts T-7 accuracy |
| **Transact path not shown (CSP/private offer, reseller, co-op/GSA)** | Route to cash drives cycle time; late vehicle choice causes slip | Partner of record, vehicle chosen, quote/PO/PO# placeholders, marketplace status | “**Transact Plan**” chip + calendar of approvals; exception if TBD inside T-21 | **High** – slip ↓; WAPE ↓ via realistic quartering |
| **POV/POC DoR & KPI scoreboard absent** | Starting weak POVs inflates Best/Commit and extends cycles | DoR checklist (success criteria, data readiness, exit/rollback), scheduled checkpoints, KPI results | Badge “**POV not DoR**”; inline KPI scoreboard; exception when >7d without checkpoint | **High** – Commit quality ↑; cycle time ↓ |
| **Stage-aging thresholds not explicit** | Aging hides risk; Commit can be stale | Days-in-stage vs. cohort, last material activity | Aging thermometer + exception “Aging vs cohort” | **Medium–High** – earlier de-risking; WAPE ↓ |
| **Paper path not calendared** | Unmapped approvers/dates drive late slips | Signers, boards, target dates, booked meetings | Mini Gantt in right-rail; exception when any step “unscheduled” | **High** – slip from Commit ↓ |
| **Activation deltas missing outside header** | DSM needs motion, not just levels | WoW Δ for % Reps in Deals, % Accts active/pipe, Tech/POV wins | Tiny Δ chips/sparklines on each tile | **Medium** – faster inspection; accuracy ↑ via trend spotting |
| **Renewal-attach lens thin** | Upcoming renewals fuel attach/expand; risk if negative health | Renewal date/ATR, health score, attach opps in-flight | Attach panel on Cockpit; exception “Renewal ≤120d w/ no attach plan” | **Medium** – protects revenue; Commit mix quality ↑ |
| **Data-quality gate weak** | Commit without current artifacts bloats forecast | Timestamped artifacts; last update age | Auto-flag “Commit Missing Evidence”; one-tap demote to Best (auditable, reversible) | **Medium** – WAPE ↓; Commit hygiene ↑ |
| **Team-strip coaching not actionable** | Extra clicks to decide “what now” | Top risk per rep + next task | Per-rep “1 Next Move” button (owner, due date) | **Low–Medium** – inspection time ↓ |

---

**E. Top 5 Actions to Ship in v1 (ranked)**
1) **Add right-rail “Evidence & Actions” for Commit** • *Owner:* Cockpit PM/Eng • *Effort:* M • *Risk:* Low • *Why now:* Core to “forecast = facts”; biggest lift to Commit ≥90% at T-7.
2) **Transact Plan chip + calendarization** (CSP/private offer/partner/co-op) • *Owner:* Cockpit PM + Partner Ops • *Effort:* M • *Risk:* Medium (source data) • *Why now:* #1 slip driver late-Q; reduces Commit slips.
3) **POV DoR gate + KPI scoreboard** • *Owner:* PM/SE Ops • *Effort:* M • *Risk:* Medium (buy-in) • *Why now:* Directly improves technical conversion/cycle time. fileciteturn1file7L33-L37
4) **Stage-aging thermometer & exception** • *Owner:* Eng • *Effort:* S • *Risk:* Low • *Why now:* Cheap, high signal for aging-driven WAPE.
5) **Δ chips on activation/technical tiles** • *Owner:* Design/Eng • *Effort:* S • *Risk:* Low • *Why now:* Cuts inspection time ~40% (speculative) by making motion visible at-a-glance.

---

**F. Open Questions (assumptions to resolve)**
- Exact **Commit evidence gates** (which artifacts are mandatory vs. optional by segment?).
- **Stale-by-stage** thresholds and **close-date churn** limit.
- Definition of **POV DoR** and which KPIs are “pass” (e.g., loss/false-positive rates for CDSS, SASE latency).
- **Transact vehicles** supported per segment (CSP marketplaces, co-ops, GSA) and required fields.
- **Renewal health** signal of record and attach-rate definition.
- RAG bands below assume Enterprise norms; confirm SLED/Federal variants.

---

**G. Appendix — Metric RAG Bands & Actions**
- **% Reps in Deals** — *Predictive: High* fileciteturn1file2L39-L44
  - **R/G/A:** R <40% · A 40–69% · G ≥70% *(inferred)*
  - **Owner:** DSM
  - **When Red:** Assign target accounts/opps per rep; pair with SE; create 2 new meetings/rep in 7 days; log in CRM.

- **% Accts w/ Active Pipe (pillar)** — *Predictive: High* fileciteturn1file3L51-L56
  - **R/G/A:** R <15% · A 15–29% · G ≥30% *(inferred)*
  - **Owner:** DSM
  - **When Red:** Run micro-campaign to 10 named accts; schedule 3 exec meetings; open 5 MQOs in 14 days.

- **Tech Win Rate %** — *Predictive: High* fileciteturn1file0L5-L10
  - **R/G/A:** R <50% · A 50–69% · G ≥70% *(inferred)*
  - **Owner:** SE Mgr
  - **When Red:** Review evaluation plans; add reference architecture; inject ref customer; SE escalation.

- **POV Win Rate %** — *Predictive: High* fileciteturn1file0L14-L19
  - **R/G/A:** R <40% · A 40–59% · G ≥60% *(inferred)*
  - **Owner:** SE Mgr
  - **When Red:** Enforce DoR; mid-POV health check; swap/augment SE; tighten success criteria.

- **Median POV Duration (days)** — *Predictive: Medium (timing)* fileciteturn1file0L23-L28
  - **R/G/A:** R >45d · A 29–45d · G ≤28d *(inferred)*
  - **Owner:** SE Mgr
  - **When Red:** Cut scope; use demo-in-a-box; add day-1 value policy set; set exit date.

- **% Territories Filled** — *Predictive: Medium (coverage)* fileciteturn1file6L12-L17
  - **R/G/A:** R <85% · A 85–95% · G ≥95% *(inferred)*
  - **Owner:** DSM/TA
  - **When Red:** Prioritize backfills; redistribute key accounts; loaner coverage plan.

- **% Reps Ramped** — *Predictive: Medium* fileciteturn1file6L39-L44
  - **R/G/A:** R <50% · A 50–69% · G ≥70% *(inferred)*
  - **Owner:** DSM/Enablement
  - **When Red:** Weekly deal-assist; certification sprints; attach SE resources.

- **Open Pipe (context for quartering)** — *Predictive: High* fileciteturn1file12L55-L61
  - **R/G/A:** R <0.8× Commit need · A 0.8–1.1× · G ≥1.1× *(inferred)*
  - **Owner:** DSM
  - **When Red:** Add top-down pursuits; accelerate exec access; qualify out aged/unwinnable.

- **% Accts Platformized** — *Predictive: Medium (ASP/expand)* fileciteturn1file4L22-L27
  - **R/G/A:** R <15% · A 15–29% · G ≥30% *(inferred)*
  - **Owner:** DSM/CSM
  - **When Red:** Plan EBC/EOR; package multi-pillar POV; align partner.

- **CXO Meetings (6 mos)** — *Predictive: Important (access)* fileciteturn1file9L33-L38
  - **R/G/A:** R <15% · A 15–29% · G ≥30% *(inferred)*
  - **Owner:** DSM/AE
  - **When Red:** Book 3 EBC/EORs; exec sponsor outreach; add account plan.

> Notes: RAGs marked *(inferred)* are field-heuristic starting points; tune by segment. Technical motion → conversion and timing linkage supported by the metrics synthesis. fileciteturn1file7L33-L37

## You (2025-08-27T00:15:31.628000+00:00)

# O3 Prompt: DSM Review of **DM Cockpit — Platforms & Drill-Down (v1.4)**

## 1) Goal

Deliver an expert, field-realistic critique of the **Platforms & Drill-Down** cockpit for a Palo Alto Networks District Sales Manager (DSM). Determine what enables fast commit defense and coaching, where the design is thin or risky, and the **smallest high-leverage changes** that improve: **WAPE ≤ 8–10% (T-14), Commit Accuracy ≥ 90% (T-7), inspection time ↓ 40%, slip from Commit ↓ 30%.**

## 2) Execution Persona (for O3 to emulate)

You are a **Palo Alto Networks DSM** running weekly forecast/inspection in CRM. You manage regions and AEs across **CDSS, Cortex, Firewall, SASE, Unit 42**. You coach to MEDDPICC, watch paper path & InfoSec gates, and care about platform adoption by pillar. You operate **delta-first**: scan → spot risk → act in minutes.

## 3) Inputs

______________
DM Cockpit — Platforms & Drill-Down (v1.4)

## A) Current-Quarter Banner (no filters)

**Purpose:** one-glance velocity + timing.
**KPIs (CQ only):** Total Booked • Open Pipe • Tech Win % • POV Win % • Median POV Duration.    &#x20;

---

## B) Platform Rail (tiles as tabs) — **CDSS · Cortex · Firewall · SASE · Unit 42**

**Local filters:** Quarter | Product (tile selection = Product).
**Each tile shows (per selected platform):**

* **% Reps in Deals** (bar) • **% Accts w/ Active Pipe** (bar). &#x20;
* Optional mini stat (where defined): **% Reps Certified** for that pillar (CDSS NetSec; SASE/Cortex). If the pillar has no certification metric, the chip is hidden. &#x20;

> Note: “pillar blocks” apply the same definitions per product; Unit 42 inherits the same metric logic if data is tagged by that pillar.

---

## C) Regional Performance Table (drives the drill-down)

**Local filters:** Quarter | Product (inherited from the selected tile).
**Columns (region level, all visible at once):**

* **Activation/Coverage:** % Reps in Deals • % Accts Active Pipe • % Reps Certified (if applicable) • # Reps in Seat • % Territories Filled • # Last 3M Attrition.     &#x20;
* **Technical Motion:** Tech Win % • POV Win % • Median POV Duration.  &#x20;
* **Executive Engagement (snapshot):** % Accts w/ CXO (6m / 12m) • #EBCs (done / next 3m) • #EORs (done / next 3m).   &#x20;
* **Context (optional columns you can toggle on):** # Accts • # G2K • White Space • % Accts Active (3y) • % ASR • ATR • % Platformized.      &#x20;

**Behaviors**

* Click a region row to **expand AEs**.
* Sort on any column; sticky first column; “Show Δ” toggle overlays WoW change (display only).

---

## D) AE Drill-Down (within an expanded region)

**Columns (per AE, scoped to selected platform & quarter):**

* Attainment band (YTD) • Has Active Pipe (Y/N) • % Reps Certified (if defined for that pillar) • # Accts per Rep • Tech Win % • POV Win % • Median POV Duration.      &#x20;

**Row actions:** open the rep’s opportunity list (pre-filtered to the pillar & quarter), view exec-engagements, or open coaching pack (separate module; link only).

**Why this composition works:** it mirrors the causal chain already documented—coverage → pipeline presence → conversion/timing → attainment.&#x20;

---

## E) Neutral AI (descriptive only)

A small **“AI Observations”** icon appears on platform tiles and table headers. Clicking shows text like:

* “Observed: **% Reps in Deals (SASE) −8 pts QoQ**, while **POV Win % +5 pts**; **EBCs next 3m +3**.”
  No judgments or prescriptions—just deltas/outliers with pointers to the exact fields in the table/banner. (Matches the vision’s descriptive AI guidance.)&#x20;

---

## F) Interaction sketch (inspired by your screenshot)

1. **User lands** ➜ sees CQ banner.
2. **Chooses a platform tile** (e.g., Cortex) ➜ table rebinds to Cortex metrics for **all regions**.
3. **Scans the table** ➜ sorts by % Reps in Deals to find weak regions.
4. **Expands a region** ➜ sees all AEs at once, with Cortex-scoped activation + technical KPIs and attainment band.
5. Optional: flip to **context toggles** (ASR/ATR/Whitespace) for size-of-prize nuance.

---

## G) Component rules (for design & build)

* **Tiles:** show % Reps in Deals & % Accts Active Pipe; include % Reps Certified chip when defined for the pillar.  &#x20;
* **Tables:** default columns as listed; optional context columns are off by default to protect attention.
* **Filters:** Banner has none; each section owns **Quarter/Product**.
* **States:** empty (show definitions), loading skeletons, error (inline retry).
* **Accessibility:** tiles are tabs; table focus moves on selection; all metrics have tooltips echoing your doc definitions.

_________
## 4) Scope & Constraints (read carefully)

* **No new metrics.** Use **only** metrics explicitly present in the brief and/or optional definitions.

  * Allowed: layout/interaction changes, grouping, sort/filter behavior, delta/outlier flags, tooltips, evidence slots, thresholds/bands **for existing metrics only**.
  * Not allowed: proposing additional columns, KPIs, composite/derived metrics, or renaming metrics.
* **Module boundaries:** This prompt covers **Banner, Platform Rail, Regional Table, AE Drill-down, Neutral AI**. If suggesting Forecast/Hygiene features, label them “*belongs in module X*”.
* **AI behavior here is descriptive only** (observations/deltas/outliers with pointers), not prescriptive.

## 5) Analytic Tasks (perform all, in order)

1. **Decision Snapshot (≤7 bullets)**
   How a DSM would confirm/defend commit in <5 minutes using: **CQ Banner → Platform tile → Regional table → AE drill-down**. Call out why each step matters.
2. **Platform Rail Review**
   Does per-platform scanning of **% Reps in Deals** and **% Accts w/ Active Pipe** (and **% Reps Certified** where defined) surface weak regions fast enough? Recommend tab/tiles, sorting, and delta overlays **without introducing new metrics**.
3. **Regional Table Usability**
   Assess column set (activation/coverage, technical motion, exec engagement, optional context). Propose ordering, default sorts, sticky/Δ-toggle behavior, and attention safeguards. Keep all proposals within existing columns.
4. **AE Drill-down for Coaching**
   Evaluate whether AE-level columns (attainment band YTD, has active pipe, certification, accts/rep, tech/POV metrics) enable actionable coaching in 1–2 clicks. Suggest row actions/navigation and evidence slots—**no new KPIs**.
5. **Neutral AI (Descriptive) Scope**
   Specify the exact deltas/outlier patterns (WoW/QoQ) it should surface **using only existing metrics**, and how to point to the source fields (banner/table). No judgments or prescriptions here.
6. **Adoption & Load**
   Estimate inspection time and identify click/read hotspots. Propose one-tap fixes (e.g., default sorts, presets, saved views, sticky first column, tooltip definitions).

## 6) Return Format (strict)

Produce **only** the sections below, in order:

**A. Executive Brief (≤120 words)**
Verdict on decision-readiness and where the design most boosts forecast integrity.

**B. Scores (0–5)**

* Decision Readiness: *x.x*
* Platform Scan Efficacy: *x.x*
* Table/Drill-down Clarity: *x.x*
* Forecast Impact: *x.x*
* Adoption & Load: *x.x*
  *(1–2 sentences on the main score driver.)*

**C. What’s Working (≤5 bullets)**

**D. Gaps & Enhancements (table; no new metrics)**
\| Gap | Why it Matters | Uses Which Existing Metric(s) | Proposed UI/Behavior (not a new KPI) | Expected Impact on WAPE/Commit/Time |

**E. Top 5 Actions to Ship in v1 (ranked)**
Each: **Action • Owner • Effort (S/M/L) • Risk • Why now**
*(Actions must reference only existing metrics.)*

**F. Open Questions for PM**
Assumptions/ambiguities to resolve (definitions, data freshness, filter precedence, etc.).

**G. Appendix — RAG Bands & Triggers (existing metrics only)**
For each metric already in the brief: **Red/Amber/Green thresholds (clearly marked “assumed” if heuristic), owner, and the next UI step** when red (e.g., open evidence drawer, jump to opp list).

**H. Metric Integrity Check (required)**

* List every metric referenced in your suggestions.
* Confirm each appears in the pasted brief (Yes/No).
* Statement: **“No new metrics introduced: Yes/No.”**

> Do not include chain-of-thought. Keep explanations crisp and outcome-tied.

## 7) Guardrails

* **Absolutely no new metrics or renames.** If something feels missing, offer a **layout/interaction** workaround or tag as “*belongs in another module*”; do **not** add a KPI.
* If a metric definition is unclear, **state the assumption** and proceed; do not fabricate semantics.
* Keep AI to **descriptive observations** only in this module.
* Tie every enhancement to a measurable outcome (WAPE, Commit accuracy, inspection time, slip).

## 8) Evaluation Rubric (for self-check)

* Can a DSM validate exposure and drill to owners in <5 minutes using the existing metric set?
* Do proposed changes improve scan quality across platforms without adding metrics?
* Are table defaults and drill-downs obviously useful for weekly inspection/coaching?
* Does the design reduce inspection time with zero net new admin work?

## 9) Final Checklist (must verify before finishing)

* [ ] I used only metrics present in the pasted brief (and optional definitions).
* [ ] I introduced **no new metrics** or renames.
* [ ] Each gap includes **why**, **existing metric(s) used**, **UI/behavior**, **impact**.
* [ ] I kept AI suggestions **descriptive** and pointed to source fields.
* [ ] I proposed exactly **Top 5** v1 actions with owner & effort.
* [ ] I stated assumptions where definitions were missing or unclear.
