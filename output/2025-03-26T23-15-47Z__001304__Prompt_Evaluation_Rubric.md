# Prompt Evaluation Rubric

## You (2025-03-26T23:15:47.539000+00:00)

I'm pasting a set of instructions we used to evaluate scores. Please return this in a format I can use to create an engaging notion document


---

### 1. **Clarity of Reasoning Structure (1â€“10)**
*How clearly the prompt instructs multi-step, structured reasoning (chain-of-thought).*

- **1â€“3**: No evidence of step-by-step thinking or reasoning structure.
- **4â€“6**: Some hint at reasoning, but vague or underspecified.
- **7â€“8**: Clear multi-step guidance; reasoning is scaffolded.
- **9â€“10**: Elegantly structured reasoning chain, broken into steps or logical sub-tasks.

**Example:**
- Low (2): â€œHelp me decide what project to do.â€
- Mid (6): â€œThink step by step to evaluate which project idea is best.â€
- High (10): â€œList my three project ideas. For each, evaluate its feasibility, impact, and time required. Then recommend the best one and explain your reasoning step by step.â€

---

### 2. **Task Relevance & Purposefulness (1â€“10)**
*How well the prompt aligns with a real, useful workplace task.*

- **1â€“3**: Vague, unrealistic, or irrelevant to professional work.
- **4â€“6**: Plausible but generic or lacking clear utility.
- **7â€“8**: Describes a realistic and specific workplace scenario.
- **9â€“10**: Highly purposeful, valuable, and clearly defined for real-world work.

**Example:**
- Low (2): â€œBrainstorm something interesting.â€
- Mid (6): â€œHelp me write better emails.â€
- High (10): â€œUsing step-by-step reasoning, generate three options for a persuasive email to a hesitant stakeholder, then choose the strongest and explain why.â€

---

### 3. **Specificity & Context Provided (1â€“5)**
*How much relevant detail, framing, or parameters the prompt includes to guide the model.*

- **1**: No context; model must guess everything.
- **2â€“3**: Minimal or generic context.
- **4**: Some concrete details (e.g., goals, tone, audience).
- **5**: Rich, focused context tailored to the task.

**Example:**
- Low (1): â€œHelp me make a presentation.â€
- Mid (3): â€œHelp me make a presentation for my team.â€
- High (5): â€œHelp me make a 10-slide presentation for the marketing team on our Q1 metrics, emphasizing visual clarity and key takeaways.â€

---

### 4. **CoT Instruction Appropriateness (1â€“5)**
*How well the prompt cues chain-of-thought reasoning.*

- **1**: No cue to reason step-by-step.
- **2â€“3**: CoT is mentioned but weakly integrated.
- **4**: CoT instructions are structured and logical.
- **5**: Seamlessly embedded reasoning process (e.g., â€œFirstâ€¦ thenâ€¦ finallyâ€¦â€).

---

### 5. **Clarity of Language & Structure (1â€“5)**
*How well-written, professional, and understandable the prompt is.*

- **1**: Confusing, ungrammatical, or incoherent.
- **2â€“3**: Understandable but awkward or overly wordy.
- **4**: Clear, concise, and readable.
- **5**: Polished, well-structured, and professional.

---

### 6. **Creativity & Adaptability (1â€“5)**
*Does the prompt show original thinking or potential for reuse in other contexts?*

- **1**: Generic, formulaic, or rigid.
- **2â€“3**: Mild creativity or moderate flexibility.
- **4**: Creative framing or reusable with small changes.
- **5**: Highly adaptable and inventive.

---

## ChatGPT (2025-03-26T23:15:48.873153+00:00)

Hereâ€™s a polished version of your evaluation rubric, formatted for a clean and engaging **Notion document**. You can paste it directly into a Notion pageâ€”headings, bullet points, and spacing will render well in Notion's editor.

---

# ðŸ§  Prompt Evaluation Rubric

Use this rubric to assess the quality of prompts along multiple dimensionsâ€”reasoning clarity, workplace relevance, specificity, and more. Each category includes a 1â€“10 or 1â€“5 scale, anchored with examples.

---

## 1. ðŸ” **Clarity of Reasoning Structure** (1â€“10)
*How clearly does the prompt instruct multi-step, structured reasoning (chain-of-thought)?*

- **1â€“3**: No evidence of step-by-step thinking.
- **4â€“6**: Some hint at reasoning, but vague or underspecified.
- **7â€“8**: Clear multi-step guidance; reasoning is scaffolded.
- **9â€“10**: Elegantly structured chain-of-thought, broken into steps or sub-tasks.

**Examples:**
- **Low (2)**: â€œHelp me decide what project to do.â€
- **Mid (6)**: â€œThink step by step to evaluate which project idea is best.â€
- **High (10)**:
  > â€œList my three project ideas. For each, evaluate its feasibility, impact, and time required. Then recommend the best one and explain your reasoning step by step.â€

---

## 2. ðŸŽ¯ **Task Relevance & Purposefulness** (1â€“10)
*How well does the prompt align with a real, useful workplace task?*

- **1â€“3**: Vague, unrealistic, or irrelevant to professional work.
- **4â€“6**: Plausible but generic or lacking clear utility.
- **7â€“8**: Describes a realistic and specific workplace scenario.
- **9â€“10**: Highly purposeful and clearly valuable in a real-world context.

**Examples:**
- **Low (2)**: â€œBrainstorm something interesting.â€
- **Mid (6)**: â€œHelp me write better emails.â€
- **High (10)**:
  > â€œUsing step-by-step reasoning, generate three options for a persuasive email to a hesitant stakeholder, then choose the strongest and explain why.â€

---

## 3. ðŸ§­ **Specificity & Context Provided** (1â€“5)
*How much relevant detail or framing is included to guide the model?*

- **1**: No context; the model must guess everything.
- **2â€“3**: Minimal or generic context.
- **4**: Some concrete details (e.g., goals, tone, audience).
- **5**: Rich, focused context tailored to the task.

**Examples:**
- **Low (1)**: â€œHelp me make a presentation.â€
- **Mid (3)**: â€œHelp me make a presentation for my team.â€
- **High (5)**:
  > â€œHelp me make a 10-slide presentation for the marketing team on our Q1 metrics, emphasizing visual clarity and key takeaways.â€

---

## 4. ðŸ§© **CoT Instruction Appropriateness** (1â€“5)
*How well does the prompt cue chain-of-thought reasoning?*

- **1**: No cue to reason step-by-step.
- **2â€“3**: CoT is mentioned but weakly integrated.
- **4**: CoT instructions are structured and logical.
- **5**: Seamlessly embedded reasoning (e.g., â€œFirstâ€¦ thenâ€¦ finallyâ€¦â€).

---

## 5. âœï¸ **Clarity of Language & Structure** (1â€“5)
*How well-written and professional is the prompt?*

- **1**: Confusing, ungrammatical, or incoherent.
- **2â€“3**: Understandable but awkward or overly wordy.
- **4**: Clear, concise, and readable.
- **5**: Polished, well-structured, and professional.

---

## 6. ðŸŽ¨ **Creativity & Adaptability** (1â€“5)
*Does the prompt show originality or potential for reuse?*

- **1**: Generic or rigid.
- **2â€“3**: Mild creativity or moderate flexibility.
- **4**: Creative framing or reusable with minor changes.
- **5**: Highly adaptable and inventive.

---

Let me know if you want a **Notion cover image**, **icon suggestions**, or this reformatted as a **template block** for easy duplication.

## You (2025-03-26T23:18:26.661000+00:00)

Ok, now please give me a condensed version of this I can use in a single slide deck

## ChatGPT (2025-03-26T23:18:27.415864+00:00)

Hereâ€™s a **condensed, slide-ready version** of your prompt evaluation rubricâ€”ideal for a single slide in a deck. It keeps the essence of each criterion while trimming down the examples and explanations:

---

## ðŸ§  Prompt Evaluation Rubric (Slide Summary)

| **Criteria**                      | **Scale**     | **What to Look For**                                                                 |
|----------------------------------|---------------|--------------------------------------------------------------------------------------|
| **1. Clarity of Reasoning**      | 1â€“10          | Is structured, step-by-step thinking clearly guided?                                 |
| **2. Task Relevance**            | 1â€“10          | Is the prompt grounded in a real, valuable workplace use case?                       |
| **3. Specificity & Context**     | 1â€“5           | Does it include relevant details like goals, audience, or constraints?              |
| **4. CoT Instruction Quality**   | 1â€“5           | Is chain-of-thought reasoning cued clearly and naturally?                           |
| **5. Clarity of Language**       | 1â€“5           | Is the wording clear, concise, and professional?                                     |
| **6. Creativity & Adaptability** | 1â€“5           | Is the prompt original or reusable across contexts with small tweaks?               |

> ðŸŽ¯ **High-scoring prompts** are structured, realistic, richly contextualized, and elegantly worded with embedded reasoning.

---

Let me know if you want this adapted visually (e.g. in Figma, PowerPoint, or Notion-style graphics), or with colors/icons for each criterion.
