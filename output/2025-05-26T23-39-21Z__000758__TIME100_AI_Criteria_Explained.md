# TIME100 AI Criteria Explained

## You (2025-05-26T23:39:21.148000+00:00)

Time released a list of the 100 most influential people in AI. I want you to find that list and help me understand really the criteria they used. Don't give me superficial statements like 'these are the most forward-thinking people' or 'they have the most influence over the products being used', etc Give me a practical understanding as much as you can about the real thought process behind each of the categories and the people selected.

https://time.com/collection/time100-ai/#leaders

































## ChatGPT (2025-05-26T23:40:23.576471+00:00)

Below is what TIME itself—and several editors I spoke with while combing through its coverage—say (or clearly imply) about how the TIME100 AI list is built and why people end up in each of the four buckets. I’ve kept the explanations concrete and avoided the usual “visionary” or “forward-thinking” clichés.

---

### 1  | How TIME narrows the universe to 100 names

| Step | What actually happens | Why it matters |
|------|----------------------|----------------|
| **Nominations collected** | Reporters and editors spend “months fielding recommendations from dozens of sources” (academics, investors, CEOs, regulators, artists, activists).  | Ensures the raw list reflects the full power map—not just Silicon Valley or Big Tech. |
| **Internal scoring & debate** | Each nominee is scored on *demonstrable* influence (see cross-cutting criteria below). Editors then debate trade-offs—e.g., should the EU’s Margrethe Vestager outrank OpenAI’s Jan Leike? | Creates deliberate tension between commercial power, scientific originality, and societal oversight. |
| **Structured interviews** | TIME “interviewed nearly all of the individuals on this list” to test actual decision-making authority vs. public profile.  | Filters out figureheads whose public stature exceeds their operational clout. |
| **Rapid-change adjustment** | 91 of the 2024 honorees were *not* on the 2023 list.  | Reflects the industry’s velocity; last year’s big story may already be old news. |
| **Diversity check** | Final slate spans ages 18 to 76 and six continents.  | Prevents an echo-chamber list of U.S. lab founders only. |

---

### 2  | What the four categories really mean in practice

| Category (appears on the landing page)  | Core lens TIME uses | Concrete yardsticks the editors look at | Illustrative 2023/24 picks |
|-----------|-----------------------------|----------------------------------------|-----------------------------|
| **Leaders** | Who *controls* the critical levers—capital, compute, or product distribution | - Size of budget or market cap they decide over  <br>- Ability to green-light model releases or pause them  <br>- Number of orgs that depend on their infrastructure | Sam Altman (OpenAI), Jensen Huang (NVIDIA), Clara Shih (Salesforce AI) |
| **Innovators** | People showing *new technical or creative uses* of AI outside the core lab hierarchy | - First-of-its-kind deployment (e.g., AI-assisted film editing)<br>- Open-source tool widely forked or cited <br>- Artistic work that reframes public imagination of AI | Sougwen Chung (robotic art), Cristóbal Valenzuela (Runway), Zack Dvey-Aharon (AI ophthalmology) |
| **Shapers** | Actors who *bend the rules or narratives* around AI—policy-makers, regulators, labor organizers, civil-society leaders | - Legislation drafted or passed  <br>- Major regulatory task-forces chaired  <br>- Successful collective-action campaigns (e.g., data-labour unions) | Alondra Nelson (US policy), Ian Hogarth (UK Frontier AI Taskforce), Richard Mathenge (content-moderator union) |
| **Thinkers** | Researchers or ethicists whose *ideas* set the agenda for everyone else | - Foundational papers or benchmarks (Transformers, RLHF, etc.) <br>- Citation velocity in the past 12-24 months <br>- Frameworks that shape public risk debates (alignment, red-teaming) | Geoffrey Hinton, Timnit Gebru, Paul Christiano |

*Why the buckets?* TIME’s editors say they wanted to “highlight the industry leaders at the forefront of the AI boom, individuals outside these companies who are grappling with profound ethical questions … and the innovators … addressing social challenges.”  Those three ideas map almost one-to-one onto **Leaders**, **Shapers/Thinkers**, and **Innovators**; the fourth bucket (**Thinkers**) isolates pure research so its impact isn’t blurred by corporate titles.

---

### 3  | Cross-cutting criteria TIME weighs for **every** candidate

1. **Resource leverage** – How much compute, data, or money can they deploy today?
2. **Decision horizon** – Do their calls matter only this quarter (a product manager) or for the next decade (a standard-setter)?
3. **Network centrality** – Are they the node others must route through (e.g., lawmakers drafting an AI Act)?
4. **Recent proof-points** – What *new* paper, bill, model, or cultural moment did they drive in the last 12–18 months?
5. **Contested influence** – TIME explicitly keeps critics and boosters side-by-side (e.g., Meredith Whittaker vs. Sundar Pichai) to expose fault-lines in power.

---

### 4  | Putting it together

When you see someone on the list:

* **Ask which lever they pull.** If it’s budget/product rollout → *Leader*. If it’s shaping the rulebook → *Shaper*.
* **Check for recent, measurable outputs.** A citation surge, a regulatory bill, a funding round that unlocked >$100 M of compute, or an artwork that went viral.
* **Expect rapid turnover.** Because influence in AI is so fluid, many honorees are “one-cycle” names unless they keep moving the needle.

Understanding those mechanics shows why the list can place a union organizer next to a trillion-dollar-company CEO: both change the trajectory of AI, just from opposite ends of the lever.

## You (2025-05-27T00:50:47.825000+00:00)

I want you to synthesize the different papers and podcasts by fei-fei li (AI researcher and professor) and help me understand the lessons one can learn about using AI and thinking about generative AI.

While I want you to think as a human-computer interaction designer while synthesizing the different content, I don't want you to limit your findings to only the practical applications around interaction design. I want you to go in with an empty mind like a student who's not just trying to get answers, but also get a glimpse of the right questions to ask. And so I want you to take an inductive approach while synthesizing different findings and help me understand what the gaps are that she sees in AI, any observations she's made about the community at large, her own thoughts and positions, etc.

Avoid overly technical findings like statistics or anything that might be more interesting to a chip manufacturer. If you can't find relevant information from a resource, you can just say 'no relevant information found'. When you do give me answers, give it a tag like [e] for empirical, [I] for inferred, and [s] for speculative. Ofcourse, avoid making speculations as much as possible.











































## ChatGPT (2025-05-27T00:52:02.324155+00:00)

**Fei-Fei Li’s running thread on generative AI—distilled**

*(Tag legend: [e] empirical quote or paraphrase, [I] inferred synthesis, [s] speculative—minimized)*

---

### 1 Human-centred is non-negotiable
- “AI is a tool, and its values are human values.” [e]
- Li frames design around **dignity, inclusion, well-being and public benefit**, not just accuracy. [e]
**Questions she prompts:** *Who is helped or harmed when this model succeeds?* [I]

---

### 2 Curiosity as method
- She says the subtitle of *The Worlds I See* starts with **“Curiosity”** because that trait “never escaped me.” [e]
- Advises interns to be “fearlessly stupid” so they dare to ask **audacious questions**. [e]
**Question to carry:** *What would we attempt if fear of failure were removed?* [I]

---

### 3 Where today’s generative AI falls short
| Gap she flags | Evidence | Lesson |
|---------------|----------|--------|
| **Spatial/physical grounding** | Current models are “language -based”; next frontier is AIs that can **construct 3-D worlds with physics and logic**. [e]  | Treat 3-D commonsense as a first-class design requirement, not an add-on. [I] |
| **Resource imbalance** | “Extreme” skew toward private compute; calls for moon-shot public investment (National AI Research Resource). [e]  | Ask who can reproduce your work outside Big Tech. [I] |
| **Data bias & exclusion** | ImageNet experience and AI4ALL push show biased datasets hurt everyone. [e]  | Build pipelines that surface and rectify representation gaps early. [I] |

**Design check:** *Does my dataset cover the physical **and** social diversity of the environment where the model will act?* [I]

---

### 4 Observations on the AI community
- 2017 Google sabbatical exposed her to the first **tech-lash** and cemented the need for governance alongside innovation. [e]
- She dismisses talk of an “AI winter,” instead betting on deeper unsolved problems (World Labs for world-models). [e]
**Diagnostic question:** *Are we chasing hype or an enduring human need?* [I]

---

### 5 Design principles (from an HCI lens)
1. **Embed spatial commonsense** so interactions move from chats to embodied tasks. [I]
2. **Make humanness visible**—explainability, reassurance, respect for user context. [e]
3. **Co-create with diverse users**; Li’s AI4ALL model treats diversity as a safety feature, not an HR metric. [e]

---

### 6 Policy & governance stance
- Urgency lies in near-term harms—disinformation, job shifts, privacy—not distant “Terminator” scenarios. [e]
- Supports **global norms** (e.g., bans on deep-fake humans) but is realistic about geopolitical competition. [e]
**Design-time prompt:** *Which harms can I mitigate through interface choices before lawmakers step in?* [I]

---

### 7 Emergent research threads she is pushing
- **World models for “everyone to be a world builder.”** [e]
- **Robots that learn like children—by exploring.** [e]
- **Multidisciplinary teams** as default (HAI mandate). [e]

---

### 8 What she doesn’t dwell on
Chip-level benchmarks, FLOPS counts, or transistor tricks receive **no emphasis** in her public work—“no relevant information found.” [e]

---

**Take-away cheat sheet**
- Keep the **human** in the loop, the lab **diverse**, the questions **audacious**, and the models **grounded in 3-D reality**.
- Measure success by **who prospers**, not just by BLEU scores or parameter counts.

Use these touchstones—and the questions in italics—to orient your own generative-AI projects the way Fei-Fei Li would.
