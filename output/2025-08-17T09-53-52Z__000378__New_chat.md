# New chat

## You (2025-08-17T09:53:52.934000+00:00)

SYSTEM / ROLE
You are executing Stage 1 · Step 1 of a research program on “context engineering” for LLM-era systems.
Your ONLY job: OUTPUT a single, copy-pasteable **Deep Research Super-Prompt (DR-SP)** for Stage 1 · Step 2.
Return the DR-SP inside ONE fenced code block and NOTHING else—no preface, no afterword.
The DR-SP MUST require web browsing and citations and MUST NOT contain any Step-1-only constraints.

NORTH STAR (FOR STAGE 1/2 ONLY)
Build a rigorous foundation to understand context engineering across core and adjacent **scientific disciplines**, adjacently connected to GenAI (AI/NLP/IR, HCI, cognitive & behavioral science, data governance, systems/reliability). 


WHAT TO OUTPUT
Produce ONE executable prompt titled exactly:
DEEP RESEARCH SUPER-PROMPT (Stage 1 · Step 2)

The prompt must contain the following sections and content, phrased for Deep Research to run without clarification:

1) SYSTEM / ROLE (for Deep Research)
   - Identify Deep Research as the investigator executing Stage 1 · Step 2.


2) OBJECTIVE
   - Assemble a preliminary, citation-complete corpus and structured evidence bundle on **context engineering mechanisms** and their evaluated effects in LLM systems.
   - Mechanism levers to cover: Framing; Injection/RAG; Structuring; Weighting/Reranking; Boundaries/Guardrails. Optional if found: Memory/Long-context; Compression/Selection; Multimodal grounding.
   - Disciplines to canvass: AI/NLP/IR, HCI/interaction design, cognitive & behavioral science, data ethics & governance, systems/reliability engineering.
   - Outcomes to map: **accuracy, groundedness/faithfulness, robustness** (incl. prompt-injection/jailbreak), **latency, cost**. (Do NOT include novelty/expressivity/trust here.)

3) SCOPE
   - Timeframe: 2020–present (prefer 2023–2025).
   - Language: primarily English; note any non-English sources separately.
   - Domains: code assistants, enterprise QA/search, customer support, analytics/BI, scientific/technical QA, agent/tool-use systems.

4) SOURCE TIERS (priority order)
   - T1: peer-reviewed papers/surveys.
   - T2: preprints with explicit methods.
   - T3: official lab/enterprise technical posts (e.g., OpenAI, Anthropic, Databricks, IBM).
   - T4: practitioner/startup case studies **only** if method-transparent with metrics/artifacts.
   - Disallow: marketing/opinion without methods/metrics; unverifiable anecdotes; duplicates (keep the definitive source).

5) INCLUSION / EXCLUSION RULES
   - Include only LLM-era work relevant to the levers/outcomes with **method transparency** and **either metrics or artifacts** (code/data/eval harness).
   - Exclude pre-LLM unless foundational AND explicitly tied to LLM context usage; exclude unsubstantive commentary; dedupe overlapping paper/blog pairs.

6) DELIVERABLES
   6.1 **Master Sources (table):** Title | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Reason to Include.
   6.2 **Screening Log (PRISMA-lite):** counts + exclusion reasons at Title/Abstract and Full-Text; dedupe notes.
   6.3 **Evidence Table (CSV)** EXACTLY matching §7 schema; **n ≥ 40** total, hitting §8 quotas.
   6.4 **Methods Appendix:** actual Boolean search strings; engines/databases/sites; date ranges; how inclusion/exclusion were applied; limitations/paywalls.
   6.5 **Metric Crosswalk Instantiation:** how each included source defines key metrics; note heterogeneity; quote definitions where ambiguous.
   6.6 **Top-10 by Evidence Score** and **Gaps & Conflicts** list.
   6.7 **Contradictions & Adjudication Plan:** ≥3 conflicting findings + targeted follow-up briefs with 1-line hypotheses.
   6.8 **Archive Bundle:** URLs and (where permitted) PDFs/snapshots with access dates.

7) EVIDENCE TABLE SCHEMA (CSV, exact columns)
   Title | Authors | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Method Type | Datasets/Corpora | Metrics (as defined) | Reported Effect (direction/Δ) | Artifacts? (code/data/eval harness) | Risks/Governance | Notes | Evidence Score (0–8)

8) QUOTAS & COVERAGE
   - Minimum included items: 40.
   - Target by Tier: T1 ≥ 12, T2 ≥ 10, T3 ≥ 8, T4 ≥ 6 (flex if necessary; report gaps).
   - Balanced coverage across the five core levers (Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries).
   - Ensure discipline diversity across AI/NLP/IR, HCI, cognitive/behavioral, ethics/governance, systems/reliability.

9) SCORING RUBRIC
   - **Evidence Score (0–8)** = sum of four 0–2 subscores:
     • Source Type (T1..T4)
     • Method Clarity (datasets, baselines, ablations, eval protocol)
     • Recency (newer favored within scope)
     • Replicability Cues (code/data/eval harness released)

10) OPERATING RULES
   - Cite precisely; link to the specific section/page where feasible.
   - Quote metric definitions verbatim when ambiguous; otherwise summarize and cite.
   - Prefer T1–T3; include T4 only with explicit caveats.
   - Report negative/neutral results and failure modes, not just positive deltas.
   - Maintain lever/discipline balance; if a quota cannot be met, create a gap report and propose compensating searches.

11) ASSUMPTION POLICY
   - Do not ask the user clarifying questions unless you hit a hard blocker (e.g., web access disabled).
   - Proceed with best judgment; state assumptions; log gaps; keep momentum.

12) OUTPUT FORMAT
   - One Markdown report with sections mirroring §6 Deliverables.
   - Attach the Evidence Table as CSV (copy-pasteable).
   - Begin with an Executive Snapshot (coverage stats, key risks, biggest contradictions).
   - No chain-of-thought; concise methods and findings.

13) ACCEPTANCE CRITERIA
   - Quotas met or gap rationale provided.
   - Evidence Table schema exact; every row scored (Evidence Score).
   - Methods Appendix lists the actual search strings used.
   - ≥3 explicit contradictions with adjudication plans.
   - Archive Bundle present with access dates.

BEGIN
Execute the investigation now and return the specified outputs.

## ChatGPT (2025-08-17T09:53:55.753237+00:00)

```
DEEP RESEARCH SUPER-PROMPT (Stage 1 · Step 2)

1) SYSTEM / ROLE (for Deep Research)
You are Deep Research, the primary investigator executing **Stage 1 · Step 2** of a research program on “context engineering” for LLM-era systems. You will independently plan, search, critically appraise, and synthesize evidence using web browsing and produce a citation-complete report and artifacts as specified below.

2) OBJECTIVE
Assemble a preliminary, citation-complete corpus and structured evidence bundle on **context engineering mechanisms** and their evaluated effects in LLM systems.
- Mechanism levers to cover: **Framing**; **Injection/RAG**; **Structuring**; **Weighting/Reranking**; **Boundaries/Guardrails**. Optional if found: **Memory/Long-context**; **Compression/Selection**; **Multimodal grounding**.
- Disciplines to canvass: **AI/NLP/IR**, **HCI/interaction design**, **cognitive & behavioral science**, **data ethics & governance**, **systems/reliability engineering**.
- Outcomes to map (only): **accuracy**, **groundedness/faithfulness**, **robustness** (incl. prompt-injection/jailbreak), **latency**, **cost**. *(Do NOT include novelty/expressivity/trust.)*

3) SCOPE
- Timeframe: **2020–present**, prefer **2023–2025**.
- Language: **English-first**; note any **non‑English** sources separately.
- Domains: **code assistants**, **enterprise QA/search**, **customer support**, **analytics/BI**, **scientific/technical QA**, **agent/tool‑use systems**.

4) SOURCE TIERS (priority order)
- **T1**: peer‑reviewed papers/surveys.
- **T2**: preprints with explicit methods.
- **T3**: official lab/enterprise technical posts (e.g., OpenAI, Anthropic, Google, Meta, Microsoft, Databricks, IBM).
- **T4**: practitioner/startup case studies **only** if method‑transparent with metrics/artifacts.
- Disallow: marketing/opinion without methods/metrics; unverifiable anecdotes; duplicates (keep the definitive source).

5) INCLUSION / EXCLUSION RULES
Include only LLM‑era work relevant to the levers/outcomes with **method transparency** and **either metrics or artifacts (code/data/eval harness)**.
- Exclude pre‑LLM unless foundational **and** explicitly tied to LLM context usage.
- Exclude unsubstantive commentary; dedupe overlapping paper/blog pairs (retain canonical version).
- Include negative/neutral findings and failure modes.

6) DELIVERABLES
Produce **ONE Markdown report** with all sections below, using precise web citations.
6.1 **Master Sources (table):** `Title | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Reason to Include`.
6.2 **Screening Log (PRISMA‑lite):** counts + exclusion reasons at Title/Abstract and Full‑Text; dedupe notes.
6.3 **Evidence Table (CSV)** EXACTLY matching §7 schema; **n ≥ 40** total, satisfying §8 quotas where possible.
6.4 **Methods Appendix:** actual Boolean search strings; engines/databases/sites; date ranges; how inclusion/exclusion were applied; limitations/paywalls.
6.5 **Metric Crosswalk Instantiation:** how each included source defines key metrics; note heterogeneity; **quote** definitions where ambiguous.
6.6 **Top‑10 by Evidence Score** and **Gaps & Conflicts** list (state unmet quotas and why).
6.7 **Contradictions & Adjudication Plan:** ≥3 conflicting findings + targeted follow‑up briefs with 1‑line hypotheses and next steps.
6.8 **Archive Bundle:** list URLs and (where permitted) PDFs/snapshots with **access dates**.

7) EVIDENCE TABLE SCHEMA (CSV, exact columns)
`Title | Authors | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Method Type | Datasets/Corpora | Metrics (as defined) | Reported Effect (direction/Δ) | Artifacts? (code/data/eval harness) | Risks/Governance | Notes | Evidence Score (0–8)`

8) QUOTAS & COVERAGE
- Minimum included items: **40**.
- Target by Tier: **T1 ≥ 12**, **T2 ≥ 10**, **T3 ≥ 8**, **T4 ≥ 6** (flex if necessary; report gaps).
- Balanced coverage across the five core levers (**Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries**).
- Ensure discipline diversity across **AI/NLP/IR**, **HCI**, **cognitive/behavioral**, **ethics/governance**, **systems/reliability**.

9) SCORING RUBRIC — **Evidence Score (0–8)**
Sum four subscores (0–2 each):
- **Source Type (Tier)**: T1=2.0; T2=1.5; T3=1.0; T4=0.5; else=0.
- **Method Clarity**: 0=unclear; 1=basic (datasets or baselines, limited protocol); 2=clear (datasets, baselines, ablations and/or user study protocol).
- **Recency**: 2=2024–2025; 1=2022–2023; 0=2020–2021.
- **Replicability Cues**: 2=code/data/eval harness released; 1=partial artifacts; 0=none.

10) OPERATING RULES
- **Web Browsing is mandatory.** Use academic databases (ACL Anthology, ACM DL, IEEE Xplore, arXiv, Google Scholar), and official org blogs/docs. Capture **precise citations** and **link directly to sections/pages** when feasible.
- **Citation format:** numbered in‑text citations linking to a reference list with title, venue, year, URL, and **access date**. Quote metric definitions verbatim when ambiguous (use quotation marks and page/section anchors).
- Prefer T1–T3; include T4 only with explicit caveats on evidence strength.
- Report negative/neutral results and failure modes (e.g., prompt‑injection/jailbreak successes, brittleness, regressions).
- Maintain lever/discipline balance; if a quota cannot be met, create a **Gap Report** and propose compensating searches.
- De‑duplicate preprint vs. published versions; keep the definitive/camera‑ready where available.
- Use **snowballing** (backward/forward citations) and **query expansion**; document both.
- Extract and normalize **latency** (ms), **cost** (tokens/$), and **robustness** (attack success rates, jailbreak rates) where reported.
- Respect licenses; for gated PDFs, include citation plus abstracted notes; place PDFs/snapshots in the Archive Bundle only where permitted.

11) ASSUMPTION POLICY
Do not ask the user clarifying questions. Proceed with best judgment; explicitly log assumptions and remaining gaps. Keep momentum.

12) OUTPUT FORMAT
Produce **one Markdown report** with sections mirroring §6. Attach the **Evidence Table as CSV** (copy‑pasteable). Begin with an **Executive Snapshot** (coverage stats, key risks, biggest contradictions). Keep prose concise and analytical; **no chain‑of‑thought**.

13) ACCEPTANCE CRITERIA
- Quotas met **or** gap rationale provided.
- Evidence Table schema **exact**; every row scored (Evidence Score).
- Methods Appendix lists the **actual** search strings used.
- ≥3 explicit contradictions with adjudication plans.
- Archive Bundle present with access dates.

––– EXECUTE NOW –––
Perform the investigation with comprehensive web browsing and precise citations. Use Boolean search and controlled vocabularies aligned to §2–§3. Examples of **seed** query families (expand/adapt; document final strings in §6.4):
- ("context engineering" OR "prompt engineering" OR "system prompt") AND (evaluation OR "user study" OR ablation) AND (accuracy OR groundedness OR faithfulness OR robustness OR latency OR cost) AND (LLM OR "large language model").
- (RAG OR "retrieval augmented generation" OR "context injection" OR "in‑context retrieval") AND (bench* OR eval* OR metrics) AND (robustness OR "prompt injection" OR jailbreak OR "data poisoning").
- ("rerank*" OR "weighting" OR "document scoring") AND (LLM OR "hybrid search") AND (latency OR throughput OR cost).
- (guardrail* OR "safety filter" OR "jailbreak defense" OR "prompt injection defense") AND (eval* OR benchmark) AND (enterprise OR "customer support" OR "code assistant").
- ("schema" OR "template" OR "scaffold*" OR "toolformer" OR "function calling") AND (structured prompting OR "planning") AND (evaluation).
- ("long context" OR memory OR "context window" OR compression OR "selective retrieval") AND (evaluation OR ablation) AND (cost OR latency).
- (HCI OR "user study" OR "human factors") AND (prompt* OR "interaction design") AND (LLM) AND (accuracy OR groundedness OR robustness).
- ("data governance" OR provenance OR PII OR "sensitive data") AND (RAG OR "context") AND (risk OR policy OR compliance) AND (evaluation).
Cover domains in §3 and ensure Tier/lever/discipline quotas in §8. Produce the Markdown report with all §6 deliverables, CSV evidence table per §7, scoring per §9, and explicit citations and archive details throughout.
```

