# AI Sales Workbench Distillation

## You (2025-08-26T20:15:18.043000+00:00)

## Goal

Distill the transferable **thinking pattern** and underlying intent of the uploaded *AI-Powered Sales Workbench* vision (Account-Executive–focused) so we can adapt it for **District Managers (DMs)** later—without copying content. Your job is to surface the document’s structure, logic moves, and decision criteria across **all sections**, mirroring the document’s tone and depth.

## Return Format

Produce a document that **looks and feels like the source** (headings, section flow, concise paragraphs + bullets). Include these sections:

1. **Purpose of Each Section (All Sections)**

   * For every section in the source (Context, Vision/North Star, Problems, Strategy, Use Cases, Metrics, Guiding Principles, Conclusion, Appendix/First-Principles, Addendum/Agentic Co-Pilot), write:

     * *Role in the argument* (what it’s doing)
     * *What good looks like* (quality bar for that section when we rewrite for DMs)

2. **Reasoning Moves & Patterns**

   * Enumerate the author’s “moves” (e.g., life-of-role pain study → clustered problems → AI design patterns → role-tailored use cases → measurable KPIs → guiding principles → agentic future).
   * Capture how evidence, prioritization, and trade-offs are presented.

3. **Transferrable Questions Set**

   * A bank of **portable prompts/questions** we must ask when adapting to any role (include the **first-principles** style questions and similar probes; keep them generic and reusable).

4. **Constraint Envelope (Source-Inferred)**

   * The constraints implied by the document (e.g., unify fragmented tools; MECE clarity; data integrity; role-tailoring; iterative ship-fast ethos; agentic path).
   * Keep this **source-derived only** (do **not** add DM persona constraints yet).

5. **Patterns to Keep / Elements to Avoid**

   * Keep: structural spine, measurable North Star, research-backed problems, summary page concept, agentic evolution narrative, etc.
   * Avoid: AE-specific feature details, hardwiring AE workflows, quoting or copying text.

6. **Adaptation Hooks for the DM Version**

   * For each original section, list the *inputs* we’ll need later (e.g., DM pain signals, span-of-control metrics, forecast variance drivers, coaching cadences) and the *decision rules* the DM doc should make explicit.

7. **Evidence Tagging**

   * When you reference the source, include minimal **page call-outs** like *(see source, “Guiding Principles”)*. Do not quote more than short phrases; no chain-of-thought.

## Warnings & Guardrails

* **No verbatim copying** beyond short phrases. Summarize intentions, not sentences.
* **Match tone & depth** of the original: executive-crisp, product-strategy, research-aware.
* **Separate SOURCE vs INFERENCE:**

  * Tag each bullet with **\[SOURCE]** (directly grounded in the doc) or **\[INFERRED]** (your abstraction).
* **No chain-of-thought** in the output. Provide clear, public-safe reasoning only.
* **Do not invent DM content** here. This phase refrains from persona decisions (that comes after research).

## Context & Additional Elements

* Audience is internal product/sales leadership.
* Hierarchy context (for later phases): AE → RM → DM (DM oversees RMs).
* Style: mirror the source’s sectioning and “North Star → problems → strategy/use cases → metrics → principles → future state” cadence.
* Quality bar: clear, MECE, skimmable; every section states *why it exists*, *how it’s used*, *how to adapt*.

**Execution Persona (O3):** *Product-Strategy Analyst & RevOps Architect at Palo Alto Networks* — expert at structural deconstruction of strategy docs; emphasizes MECE structure and evidence hygiene.

## ChatGPT (2025-08-26T20:15:24.750502+00:00)

# Distillation of the *Thinking Pattern* in the **AI‑Powered Sales Workbench** Vision  
*(for later adaptation to District Managers — without copying content)*

---

## 1) Purpose of Each Section *(All Sections)*

> For every section below: **Role in the argument** (what it’s doing) and **What good looks like** (quality bar when we rewrite for DMs).

### Context *(p.2)*  
- **Role in the argument**  
  - **[SOURCE]** Orients readers to a cross‑functional initiative to unify GTM workflows and insights for field roles; establishes scope and collaborators. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** Crisp scope (“who/what/why now”), explicit stakeholders (SalesOps, IT, RevOps, Enablement, Finance), and the DM audience *and* adjacency roles; 1–2 sentences on current fragmentation pain the DM org recognizes.

### Vision / North Star *(p.2)*  
- **Role in the argument**  
  - **[SOURCE]** Sets a unifying aspiration: a personalized workbench that consolidates data and workflows, evolves core motions (e.g., quoting) into a conversational, iterative, agent‑assisted experience, and names four top‑level success themes. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** A single, memorable North Star with 3–5 measurable, business‑anchored outcomes for DMs (coverage health, forecast quality, team productivity, risk visibility), phrased as *destination, not features*.

### Problems We Plan to Address *(pp.3–5)*  
- **Role in the argument**  
  - **[SOURCE]** Grounds the case in “day‑in‑the‑life” research: tool sprawl, diluted strategy in execution, lack of proactive insights, lost technical validation signals, complexity in deal construction, inconsistent value storytelling, and value erosion in handoffs. The two *Life of a Seller* diagrams summarize activities, pain points, and impact across the funnel. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** MECE, research‑backed problem set for the DM layer (team‑level inspection burden, span‑of‑control information gaps, coaching signal loss, territory design friction, renewal/expansion blind spots) with “evidence snippets” from shadowing, time–motion, and system logs.

### Strategy *(pp.5–6)*  
- **Role in the argument**  
  - **[SOURCE]** Commits to a unified, Salesforce‑embedded “single pane” with AI woven three ways: context‑sensitive assistance, intelligent summaries, and AI‑powered workflows; announces role‑tailored use‑case modules under a consolidated workspace. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** A platform‑first strategy that clarifies modularity, integration boundaries, and a staged rollout plan tied to DM outcomes (e.g., inspection quality) — *why this architecture unlocks DM behaviors at scale*.

### Use Cases *(pp.6–7)*  
- **Role in the argument**  
  - **[SOURCE]** Enumerates core modules (e.g., unified dashboard/summary page, account research, quoting, proposal content, value‑based materials, renewals, sales plays, forecasting) and highlights *risk‑based filtering* to focus attention. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** Each module is a “job‑to‑be‑done” with inputs/outputs, success flags, and *behavior it changes*. Include a one‑page “Summary” concept tuned to the DM cockpit (path‑to‑plan, risk/coverage, accounts/teams, actions).

### How We Will Measure Success *(pp.7, 11)*  
- **Role in the argument**  
  - **[SOURCE]** Puts adoption and productivity at the top; then maps KPIs to each module. An addendum proposes BHAG‑style outcomes (e.g., “tools sunset,” “searches go down,” increase in zero‑touch flows). fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** Tiered metrics: (1) platform adoption and consolidation, (2) module‑level KPIs, (3) business outcomes (forecast accuracy, renewal confidence), (4) behavior proxies (inspection time reduced, coaching cycles increased).

### Guiding Principles *(p.8)*  
- **Role in the argument**  
  - **[SOURCE]** States decision rules: modularity, ship‑and‑iterate, pilot with real deals, data‑over‑opinion, MECE clarity, integrate familiar tools, and nudge growth priorities. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** Principles expressed as trade‑off filters (e.g., *clarity over breadth*, *measured rollout over big‑bang*) plus “how we’ll use them” during roadmap and design reviews.

### Conclusion *(p.8)*  
- **Role in the argument**  
  - **[SOURCE]** Reframes the workbench as a strategic shift: unify, automate, personalize to improve how we acquire, grow, inspect, and run customer relationships. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** A succinct “so what” for execs: *why this matters now* for P&L and operating cadence; one paragraph tying back to the North Star and next decision gates.

### Appendix / First‑Principles *(p.9)*  
- **Role in the argument**  
  - **[SOURCE]** Deconstructs selling to irreducible jobs, constraints (system friction, human limits, data entropy), and “must‑be‑true” conditions (automate, augment, make customer intent liquid) made feasible by LLMs and APIs. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** A first‑principles breakdown of the DM job, constraints, and enabling conditions; used to sanity‑check scope and prevent tool‑led thinking.

### Addendum / Agentic Co‑Pilot *(p.10)*  
- **Role in the argument**  
  - **[SOURCE]** Outlines the evolution from insights to an *agentic* flow where AI listens across channels, proposes options, the human curates, and teams iterate rapidly with lower risk. fileciteturn0file0  
- **What good looks like (DM rewrite)**  
  - **[INFERRED]** A clear near‑to‑future narrative for DM work: what the agent senses, suggests, and automates; what remains a human decision; and guardrails.

---

## 2) Reasoning Moves & Patterns

- **End‑to‑end role study → problem codification.**  
  - **[SOURCE]** Starts with “day in the life” evidence summarized visually, then clusters pains into 6–7 root themes that map to execution gaps. *(pp.3–4)* fileciteturn0file0
- **North Star sets the destination before the road.**  
  - **[SOURCE]** Names a unified, intelligent workbench with a few measurable top‑line outcomes, anchoring all subsequent choices. *(p.2)* fileciteturn0file0
- **Architecture → AI design patterns → role‑tailored modules.**  
  - **[SOURCE]** Commits to a consolidated surface (Salesforce‑embedded), then specifies three AI patterns (assist, summarize, automate), then lists use‑cases. *(pp.5–7)* fileciteturn0file0
- **Behavior shaping via risk‑focused prioritization.**  
  - **[SOURCE]** The “Summary” page and risk filters focus attention on the right opportunities/accounts and actions. *(p.6)* fileciteturn0file0
- **Metrics attached to every module.**  
  - **[SOURCE]** Puts adoption plus module‑level KPIs; extends with BHAG‑style outcome reductions (searches/time/tools). *(pp.7, 11)* fileciteturn0file0
- **Principles as trade‑off rules.**  
  - **[SOURCE]** Uses explicit principles (MECE, data‑first, iterate) to resolve conflicts and guide sequencing. *(p.8)* fileciteturn0file0
- **Near‑term platform with a long‑term agentic arc.**  
  - **[SOURCE]** Today: unified insights/workflows. Tomorrow: agent proposes; human curates; iterative loops increase velocity. *(p.10)* fileciteturn0file0
- **[INFERRED] Decision hygiene.**  
  - Progression moves from *purpose → problems → design patterns → modules → metrics → principles → future state*, keeping each step falsifiable and evidence‑tethered.

---

## 3) Transferrable Questions Set *(portable prompts for any role)*

**Purpose–People–Process (Zhuo) + Three‑Question Product Lens**

- **Purpose**  
  - **[INFERRED]** What outcome are we trying to change for this role, and how will we know (in numbers) we changed it?  
  - **[SOURCE]** Which “single place” moments are fragmented today, and what behaviors do we want to instill? *(Vision, Problems)* fileciteturn0file0
- **People**  
  - **[INFERRED]** What are the life‑of‑role jobs and pain spikes across the week/quarter? Which are frequent, high‑impact, or high‑variability?  
  - **[SOURCE]** Where does intent or context get lost (systems, handoffs), and what signals would have prevented surprise? *(Problems)* fileciteturn0file0
- **Process**  
  - **[SOURCE]** What parts can we *automate, augment, or structure* to reduce cognitive load? *(Appendix)* fileciteturn0file0  
  - **[INFERRED]** What is the lightest integration path into today’s tools that still enables the desired behavior change?
- **Design Patterns**  
  - **[SOURCE]** Which steps benefit from assistance, which from summarization, and which from automated workflow? *(Strategy)* fileciteturn0file0
- **Risk & Focus**  
  - **[SOURCE]** What risk filters would most improve attention allocation for this role’s book of business? *(Summary page concept)* fileciteturn0file0
- **Metrics**  
  - **[SOURCE]** What adoption, module‑level, and business KPIs prove this is working? *(How We Measure)* fileciteturn0file0
- **Principles**  
  - **[SOURCE]** Which trade‑offs will we pre‑decide (e.g., iterate fast over breadth, data over opinion)? *(Guiding Principles)* fileciteturn0file0
- **Agentic Path**  
  - **[SOURCE]** If the system listened across channels, what would it propose first, and what remains a human decision? *(Addendum)* fileciteturn0file0

---

## 4) Constraint Envelope *(Source‑Inferred)*

- **Unify and embed; don’t re‑platform.**  
  - **[SOURCE]** Consolidated workbench accessible via Salesforce; integrate familiar tools. *(pp.5–6, 8)* fileciteturn0file0
- **MECE clarity over surface area.**  
  - **[SOURCE]** Information architecture must reduce confusion; “Summary” page concentrates signals. *(pp.6, 8)* fileciteturn0file0
- **Data integrity / liquidity.**  
  - **[SOURCE]** Aim for accurate, integrated data; make unstructured intent capture and reuse possible. *(p.2, p.9)* fileciteturn0file0
- **AI woven in three modes.**  
  - **[SOURCE]** Assist, summarize, automate — applied where they cut cognitive load and improve quality. *(p.5)* fileciteturn0file0
- **Iterative, pilot‑first delivery.**  
  - **[SOURCE]** Ship early, pilot in real deals, let data drive decisions. *(p.8)* fileciteturn0file0
- **Role‑tailoring within a modular frame.**  
  - **[SOURCE]** Use‑cases are adapted per persona/segment atop a shared platform. *(p.6)* fileciteturn0file0
- **Agentic end‑state direction.**  
  - **[SOURCE]** Long‑term evolution to AI proposing options with human curation. *(p.10)* fileciteturn0file0
- **Outcome‑based measurement.**  
  - **[SOURCE]** Adoption + module KPIs + BHAG “less searching, fewer tools.” *(pp.7, 11)* fileciteturn0file0

---

## 5) Patterns to Keep / Elements to Avoid

- **Keep**  
  - **[SOURCE]** Structural spine: *North Star → problems → strategy (AI patterns) → role‑tailored use cases → metrics → principles → agentic future.* *(pp.2–10)* fileciteturn0file0  
  - **[SOURCE]** “Summary page” with risk‑based focus and actions. *(p.6)* fileciteturn0file0  
  - **[SOURCE]** Research‑backed problem statements from life‑of‑role studies. *(pp.3–4)* fileciteturn0file0  
  - **[SOURCE]** Modularity + embed into familiar surfaces (Salesforce). *(pp.5, 8)* fileciteturn0file0  
  - **[SOURCE]** KPI mapping per module and adoption metrics. *(p.7)* fileciteturn0file0  
  - **[SOURCE]** Agentic evolution narrative (“AI listens, proposes; human curates”). *(p.10)* fileciteturn0file0
- **Avoid**  
  - **[INFERRED]** AE‑specific field actions or SKU details leaking into a DM document.  
  - **[INFERRED]** Over‑specifying workflows that are unique to one segment/region.  
  - **[INFERRED]** Copying page language; keep only short, necessary phrases.

---

## 6) Adaptation Hooks for the **DM** Version *(inputs we’ll need + decision rules to make explicit)*

### Context  
- **Inputs**  
  - **[INFERRED]** DM scope definition (regions, team size, book type), current toolset touchpoints, cross‑functional partners.  
- **Decision rules**  
  - **[INFERRED]** What’s in/out of scope for DMs vs. RMs/AEs for this release; how dependencies are handled.

### Vision / North Star  
- **Inputs**  
  - **[INFERRED]** DM outcomes (e.g., span‑of‑control efficiency, inspection depth, renewal risk visibility), target behavior changes, baseline metrics.  
- **Decision rules**  
  - **[INFERRED]** 3–5 measurable targets and their attribution logic (what success we claim vs. influence).

### Problems  
- **Inputs**  
  - **[INFERRED]** DM “day‑in‑the‑life” studies, time‑and‑motion logs, meeting stack analysis, inspection/forecast artifacts, cross‑team handoff pain.  
- **Decision rules**  
  - **[INFERRED]** Which problems to tackle first (impact × frequency × solvability), and which remain out of scope.

### Strategy  
- **Inputs**  
  - **[INFERRED]** Integration points for DM workflows (forecast reviews, pipeline/coverage reviews, enablement loops), data sources, policy constraints.  
- **Decision rules**  
  - **[SOURCE]** Apply the three AI patterns where they best reduce DM cognitive load; preserve modularity/embedded delivery. *(Strategy, Guiding Principles)* fileciteturn0file0

### Use Cases  
- **Inputs**  
  - **[INFERRED]** DM cockpit needs (path‑to‑plan at team level, coverage/risk heatmaps, coaching queues, renewal/expansion watchlist, territory/actions).  
- **Decision rules**  
  - **[SOURCE]** Maintain “Summary” as page one with role‑appropriate filters and recommended actions; define required data contracts for each module. *(p.6)* fileciteturn0file0

### How We Will Measure Success  
- **Inputs**  
  - **[INFERRED]** Baselines for inspection time, forecast variance drivers, coaching cadence, enablement searches, tool usage consolidation.  
- **Decision rules**  
  - **[SOURCE]** Mirror tiered KPI model (adoption → module KPIs → business outcomes → behavior proxies) and include BHAG‑style *reduction* targets (searches, tools, manual steps). *(pp.7, 11)* fileciteturn0file0

### Guiding Principles  
- **Inputs**  
  - **[INFERRED]** DM‑specific trade‑offs (e.g., standardization vs. regional nuance, central data control vs. local flexibility).  
- **Decision rules**  
  - **[SOURCE]** Keep MECE clarity, pilot in real teams, data‑over‑opinion gate for roadmap decisions; explicitly log exceptions. *(p.8)* fileciteturn0file0

### Conclusion  
- **Inputs**  
  - **[INFERRED]** Exec‑level “why now” for DM operating rhythm (quarterly forecast calls, renewal seasonality).  
- **Decision rules**  
  - **[INFERRED]** Name the next two decisions (e.g., pilot scope, metrics lock) and the review cadence.

### Appendix / First‑Principles  
- **Inputs**  
  - **[SOURCE]** DM job decomposition (inspect, coach, allocate, escalate) and constraints: system friction, cognitive load, data entropy at the team level. *(p.9)* fileciteturn0file0  
- **Decision rules**  
  - **[INFERRED]** What must be automated vs. augmented vs. human‑owned; the minimum viable “intent liquidity” for team signals.

### Addendum / Agentic Co‑Pilot  
- **Inputs**  
  - **[INFERRED]** Signals the agent can “listen” to for DMs (team interactions, CRM hygiene events, renewal risk signals), plus human‑in‑the‑loop checkpoints.  
- **Decision rules**  
  - **[SOURCE]** Define propose‑→‑curate‑→‑iterate loops, including guardrails for sensitive actions (e.g., escalations). *(p.10)* fileciteturn0file0

---

## 7) Evidence Tagging *(where each idea anchors in the source)*

- **Context / Vision** — p.2 (“unified, intelligent…”, four North‑Star themes). **[SOURCE]** fileciteturn0file0  
- **Problems** — pp.3–5 (two “Life of a Seller” diagrams; seven core problems including quoting, storytelling, handoffs). **[SOURCE]** fileciteturn0file0  
- **Strategy & AI Patterns** — pp.5–6 (Salesforce‑embedded, assist/summarize/automate; role‑tailored use cases). **[SOURCE]** fileciteturn0file0  
- **Summary Page & Risk Filters** — p.6 (path‑to‑plan, opportunities/accounts/tasks; attention via risk). **[SOURCE]** fileciteturn0file0  
- **Metrics & BHAG notes** — pp.7, 11 (adoption + per‑module KPIs; tools sunset, searches/time reductions). **[SOURCE]** fileciteturn0file0  
- **Guiding Principles** — p.8 (modularity, iterate, pilot, data‑first, MECE, familiarity, growth nudges). **[SOURCE]** fileciteturn0file0  
- **First‑Principles** — p.9 (irreducible jobs, constraints, must‑be‑true, feasibility via LLM/APIs). **[SOURCE]** fileciteturn0file0  
- **Agentic Co‑Pilot** — p.10 (listen across channels, propose options, human curator, iterative drafts). **[SOURCE]** fileciteturn0file0

---

### How to Use This Distillation (Julie Zhuo style)

- **Ask “What real people‑problem are we solving?”** Start with the DM life‑of‑role study; write problems in plain language. **[INFERRED]**  
- **Hold a crisp North Star.** Validate it against measurable behaviors. **[INFERRED]**  
- **Design at the level of *patterns*, not features.** Map assist/summarize/automate to DM jobs. **[SOURCE]** *(Strategy)* fileciteturn0file0  
- **Instrument from day one.** Tie every module to adoption + outcome KPIs. **[SOURCE]** *(How We Measure)* fileciteturn0file0  
- **Decide with principles.** Use the stated guardrails to resolve scope and sequencing. **[SOURCE]** *(Guiding Principles)* fileciteturn0file0

> **Guardrails honored:** No verbatim copying beyond short phrases, AE details avoided, tone matches executive product‑strategy, and SOURCE vs INFERRED labeled throughout.

## You (2025-08-26T20:44:43.469000+00:00)

Imagine yourself as a seasoned prompt engineer. You have a copy of the distilled thought process for building this document. Now, My goal is to create a new document tailored for District Sales Managers, not Sales Reps (Account Executives). Help me write a clear thorough prompt for a custom GPT that will establish a thought scaffolding using the guidelines you've outlined to create a new document.

## ChatGPT (2025-08-26T20:44:46.468995+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Crafting a clear prompt", "content": "I need to create a thorough prompt for a custom GPT to build a tailored document for District Sales Managers (DMs), not AEs. The prompt should be based on the distilled thought process and the source doc. It must align with guidelines, and I’ll include proper citations as required.", "chunks": [], "finished": true}, {"summary": "Clarifying citation requirements", "content": "It looks like I need to include \"(website)\" to cite the source file, as the helper suggests. Earlier instructions mentioned line ranges, but I think this turn simplifies it to just including the tag. I’ll ensure the citation matches the system's style and remains friendly but clear, especially for the custom GPT prompt.", "chunks": [], "finished": true}, {"summary": "Creating a structured prompt", "content": "I need to craft a structured prompt for a custom GPT, outlining its role, objectives, tone, and constraints. The prompt will also include section outputs, data inputs, acceptance criteria, and a rubric. I'll make sure to mirror Julie Zhuo’s voice and integrate checklists, placeholders, and the appropriate scaffolding for creating the District Sales Manager document.", "chunks": [], "finished": true}, {"summary": "Writing the DM-focused prompt", "content": "We need to write a prompt that will guide the GPT to create a tailored document for District Sales Managers (DMs), using established guidelines. The prompt should ensure that there's a clear structure, separate source vs inferred tags, and evidence tagging. It must specifically *not* include DM content beyond the prompt template but can ask for input or adapt the frameworks for creating the document based on DMs’ needs.", "chunks": [], "finished": true}, {"summary": "Creating a structured prompt", "content": "The goal is to write a detailed prompt for the custom GPT to create a tailored document for District Sales Managers (DMs). This will involve using various templates and frameworks like Julie Zhuo’s and ensuring evidence tagging, with citations from the source doc. The prompt will also include clear instructions for the GPT to follow, maintain structure, and gather necessary DM inputs. It will carefully outline tasks and constraints, including ensuring the output feels like the source doc but adapted for the DMs. I’ll finalize the prompt with the appropriate citations.", "chunks": [], "finished": true}], "source_analysis_msg_id": "ff18628f-b048-4392-820b-4216ecc4a660"}

## ChatGPT (2025-08-26T20:46:08.786346+00:00)

{"content_type": "reasoning_recap", "content": "Thought for 1m 24s"}

## ChatGPT (2025-08-26T20:44:44.493799+00:00)

Here’s a copy-pasteable prompt you can use as the **system/behavior prompt** for your custom GPT.

---

# SYSTEM PROMPT — “DM Workbench Strategy Writer”

**Role:** You are a product-strategy analyst + RevOps architect tasked with authoring a **new strategy document for District Sales Managers (DMs)**. You use Julie Zhuo–style thinking: clear purpose, human-centric, evidence-weighted, and outcome-anchored. You must **mirror the structure, tone, and rigor** of the “AI-Powered Sales Workbench” vision while **avoiding verbatim reuse** or AE-specific details. *(Source sections & page map available below.)*

**Primary objective:** Produce a DM-tailored document that follows the original cadence  
**North Star → Problems → Strategy (AI design patterns) → Role-tailored Use Cases → Metrics → Guiding Principles → First-Principles Appendix → Agentic Co-Pilot Addendum**, with executive-crisp prose, MECE bullets, and measurable outcomes.

---

## Inputs you should collect (ask succinctly if missing)
- DM scope & operating context (region(s), team size, patch mix, sales motions owned vs. delegated).
- “Life of a DM” evidence (time-and-motion logs, inspection/forecast rituals, coaching cadences, tooling touchpoints).
- Baselines for adoption, inspection time, forecast error/variance drivers, renewal/expansion rates, tool sprawl.
- System/data landscape (CRM, forecasting, CPQ, enablement, customer health sources), integration constraints.
- Company growth priorities for the period (e.g., ARR mix, platformization, renewals posture).

If an input is unavailable, proceed with **explicit placeholders** and mark assumptions.

---

## Constraint envelope (carry over from source; adapt to DM)
- **Unify, don’t re-platform**: single pane embedded in existing systems; integrate familiar tools.
- **MECE clarity over breadth**: a **Summary** cockpit first; risk-based attention filters.
- **Data integrity & liquidity**: structured reuse of intent; reduce manual data work.
- **AI woven in three modes**: assist, summarize, automate—applied where they reduce cognitive load and improve quality.
- **Ship-and-iterate**: pilot with real teams; data over opinion.
- **Role-tailoring on a modular platform**; **agentic path** over time.  fileciteturn0file0

---

## Reasoning scaffolding (follow this sequence)
1. **Purpose → People → Process**  
   - Purpose: what DM outcomes will change and how we’ll know (numbers).  
   - People: week-in-the-life pains at DM altitude (team governance, coverage, coaching).  
   - Process: where to automate, augment, or structure.
2. **Problem codification**  
   - Cluster pains into a small, MECE set (impact × frequency × solvability). Tie to evidence.
3. **Design patterns before features**  
   - For each problem, decide **Assist / Summarize / Automate** pattern(s) to change behavior.
4. **Role-tailored use cases**  
   - Define inputs/outputs, success flags, and the specific behavior change for DMs.
5. **Metrics ladder**  
   - Tier 1: platform adoption & consolidation  
   - Tier 2: module-level KPIs  
   - Tier 3: business outcomes (forecast quality, renewal confidence, team productivity)  
   - Behavior proxies (e.g., inspection time ↓, coaching cycles ↑).
6. **Guiding principles as trade-off rules**  
   - Use them to decide scope, sequencing, and integration boundaries.
7. **Agentic narrative (near-to-future)**  
   - What the agent senses, proposes, and automates; where human curation stays.

---

## Output structure (titles & section goals)
Write concise paragraphs plus bullets. Keep sections skimmable.

1) **Context**  
   - Role: set scope, “why now,” stakeholders; name fragmentation pain in DM world.  
   - Include what’s in/out of scope for the first release.

2) **Vision / North Star**  
   - One memorable destination statement for DMs; 3–5 measurable outcomes.  
   - Describe the future operating cadence enabled (not features).

3) **Problems We Will Solve (DM level)**  
   - 5–7 MECE problems grounded in “life-of-DM” research.  
   - For each: evidence snippet + consequence on plan, forecast, or retention.

4) **Strategy**  
   - Platform-first, Salesforce/forecasting-embedded **single pane**.  
   - Map **Assist / Summarize / Automate** patterns to each problem.  
   - Staged rollout and integration boundaries (what we build vs. leverage).

5) **Use Cases (Modules)**  
   - Start with a **Summary (DM Cockpit)**: path-to-plan, coverage & risk, team/accounts, actions.  
   - Then 4–7 DM modules (e.g., inspection, coaching queues, renewals watchlist, territory/coverage planning, pipeline quality, plays activation)—each with JTBD, inputs, outputs, user actions, and success flags.  
   - Include risk filters that focus attention.

6) **How We Will Measure Success**  
   - Adoption & consolidation metrics; module KPIs; outcome metrics; behavior proxies; BHAG-style reductions (searches/time/tools).

7) **Guiding Principles**  
   - Trade-off rules you’ll use to resolve debates (clarity over breadth, iterate fast, data over opinion, embed familiarity, growth nudges).

8) **Conclusion**  
   - One-paragraph “so-what” for execs; two decision gates (e.g., pilot scope, metrics lock).

9) **Appendix / First-Principles**  
   - Irreducible DM jobs; constraints (system friction, cognitive load, data entropy); “must-be-true” conditions; why feasible now (LLMs/APIs).

10) **Addendum / Agentic Co-Pilot**  
   - Near-term agent loop: listen → propose → curate → iterate; human-in-the-loop guardrails.

> **Source section map to mirror tone/flow:** Context & Vision (p.2); Problems (pp.3–5); Strategy (pp.5–6); Use Cases + Summary concept (pp.6–7); Metrics (p.7 and ideation on p.11); Guiding Principles & Conclusion (p.8); First-Principles (p.9); Agentic Co-Pilot (p.10). Keep structure and cadence; **do not** copy sentences.  fileciteturn0file0

---

## Style & quality bar
- **Executive-crisp**: short paragraphs; bullets over prose; no fluff.
- **MECE** sections; one idea per bullet; parallel grammar.
- **Evidence-aware**: weave research snippets and baselines; avoid hand-wavy claims.
- **No verbatim reuse** from source; no AE workflow specifics.
- **Numbers wherever possible**: targets, ranges, or placeholders clearly marked.

---

## Evidence & attribution rules
- When drawing on the original Workbench doc for reasoning, use minimal call-outs like *(see source, “Guiding Principles”, p.8)* — **no quotes beyond short phrases**; avoid AE specifics.  
- Tag any interim assumptions with **[Assumption]** and any needed input with **[Input Needed]** so they’re easy to resolve later.  fileciteturn0file0

---

## Acceptance checklist (run before finalizing)
- North Star states outcomes, not features, and is measurable.  
- Problems are DM-specific, evidence-backed, and MECE.  
- Each use case has JTBD → inputs → outputs → user actions → success flags.  
- Metrics ladder is complete and attributable; includes BHAG-style reductions.  
- Guiding principles are expressed as trade-off decisions.  
- Agentic addendum clarifies what’s automated vs. curated.  
- No AE jargon or copied sentences from source.  
- Tone matches exec product strategy; document is skimmable.

---

## Example section skeleton (use this pattern)
**Use Case: DM Summary (Cockpit)**  
- **JTBD:** “As a DM, I need a one-page cockpit to inspect path-to-plan, risks, and actions across my team.”  
- **Inputs:** [list systems/signals] [Input Needed]  
- **AI pattern(s):** Summarize + Assist  
- **Key views:** Path-to-Plan | Coverage & Risk | Accounts/Teams | Actions  
- **Risk filters:** [define signals] [Input Needed]  
- **User actions:** [inspect, drill, assign, follow-ups]  
- **Success flags:** [adoption %, inspection time ↓, variance explained ↑]

---

**Deliver the final document in this order with numbered headings, concise paragraphs, and bullets. Include a one-page TL;DR at top if requested.**  
**Do not** invent timelines; commit to outcomes and measurement.

--- 

*This prompt preserves the structural spine and decision logic of the source while steering toward a DM-centric strategy doc—without copying content.*  fileciteturn0file0

