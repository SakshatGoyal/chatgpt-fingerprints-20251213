# Prompting GPT-5 models

## You (2025-08-08T16:37:14.401000+00:00)

I would like you to consider multiple scenarios and explore various use cases for which I have used ChatGPT, as well as how other people might use it. Please conduct a critical analysis of different AI-related workflows and help me identify what GPT-5 cannot do or how my capabilities as a user have changed [negatively] compared to what I could do with GPT-o3 or GPT-4o.

## ChatGPT (2025-08-08T16:38:34.385207+00:00)

Here’s a crisp, critical look at how GPT‑5 changes real‑world workflows compared with GPT‑4o and OpenAI’s o‑series (o3). I’m approaching this like an editor (distilling signal), a curator (mapping use cases), a cognitive scientist (reasoning about how you reason with the model), and an investigative reporter (grounding key claims in primary sources).

---

## TL;DR — What changed at a glance

- **Single “smart router,” fewer model names.** In ChatGPT, GPT‑5 auto‑switches between a fast “Chat” mode and a deeper “Thinking” mode; you can still pick GPT‑5‑Thinking manually on paid plans, but older labels like “o3” or “GPT‑4o” are being retired in most places. That’s simpler but reduces the feeling of explicit control. citeturn9view0  
- **Voice is still GPT‑4o.** Real‑time Voice Mode in ChatGPT continues to run on GPT‑4o for now, not GPT‑5. If you relied on the specific voice feel of 4o, nothing changes—but you don’t get GPT‑5‑level reasoning in voice yet. citeturn9view0turn4view0  
- **Image generation:** GPT‑4o integrated a strong image generator directly in ChatGPT; GPT‑5 supports the image‑generation tool in ChatGPT as well, but the system still uses 4o for voice and (depending on your interface) may route some visual tasks through 4o’s generator. If your pipeline depended on 4o’s “look,” expect subtle aesthetic shifts. citeturn8view0turn9view0  
- **Safety & refusals:** GPT‑5 ships with stricter “safe‑completions.” Expect fewer hallucinations and less sycophancy—but occasionally earlier refusals or tighter answers on ambiguous/sensitive requests. citeturn5view0  
- **Prompting is less forgiving.** GPT‑5 follows instructions *very precisely*; vague or contradictory prompts that 4o or o3 handled passably can degrade GPT‑5’s performance or waste “thinking” time. You’ll get better results if you tidy prompts. citeturn11view0  
- **After usage caps, quality can shift.** Chats may automatically fall back to a “mini” variant until your limit resets—outputs can feel different mid‑project. citeturn9view0

---

## Where you (and others) used ChatGPT—and how GPT‑5 shifts each workflow

Below are high‑frequency scenarios I see across individual and team use. For each, I flag **regressions/risks** and **upgrades/mitigations** with concrete adjustments.

### 1) **Deep research & synthesis** (briefing docs, landscape scans, competitive analysis)
- **What changed:** o‑series models (like o3) emphasized long, tool‑using reasoning; GPT‑5 folds that into a single system that decides when to “think” and browse. In ChatGPT you’ll sometimes see a lightweight “reasoning view,” and you can force GPT‑5‑Thinking from the picker on paid plans. citeturn3view0turn9view0
- **Negative deltas to watch:**
  - Less explicit control than selecting “o3” by name; automatic routing can feel opaque. citeturn9view0
  - Stricter safety can trim gray‑area sourcing or personally‑identifying info (a good thing generally, but sometimes at odds with open‑web scouting). citeturn5view0
- **Mitigations:**
  - Say “**Use GPT‑5‑Thinking** and show your plan first.**” If you’re on Plus/Team/Pro, manually pick GPT‑5‑Thinking for the heaviest synth work; use “Get a quick answer” only when you truly need speed. citeturn9view0
  - Make constraints explicit (sources, date ranges, citation density). GPT‑5 performs best with unambiguous instructions. citeturn11view0

### 2) **Coding & agentic tasks** (bug fixing, multi‑file refactors, building small apps)
- **What changed:** GPT‑5 is positioned as the flagship for coding and agentic tool use (front‑end generation, long chains of tool calls). It adds tunables like minimal reasoning and a verbosity control in the API; in ChatGPT you can steer those in plain language. citeturn2view0turn11view0
- **Negative deltas to watch:**
  - Over‑eager tool use or over‑thinking on small tasks can increase latency. citeturn11view0
  - Old prompts that encouraged “think as much as possible” (good for o3) can now cause unnecessary exploration. citeturn11view0
- **Mitigations:**
  - Say: “**Minimal reasoning; concise final answer; only call tools if you must.**” This curbs eagerness and preserves speed. citeturn11view0
  - For big refactors, explicitly enumerate repo norms and acceptance criteria—GPT‑5 adheres tightly to rules. citeturn11view0

### 3) **Design & prototyping** (you: Figma/design systems; others: UX copy, concepts, assets)
- **What changed:** GPT‑4o brought native **image generation** tightly into ChatGPT (logos, posters, UI comps, diagrams). GPT‑5 supports the image tool inside ChatGPT; however, **Voice Mode is still powered by 4o** and some interfaces may continue to route visual generation through 4o under the hood. Expect style differences versus DALL·E‑era outputs. citeturn8view0turn9view0
- **Negative deltas to watch:**
  - If you counted on 4o’s specific aesthetic, outputs may shift subtly with GPT‑5‑routed flows.
  - In some workspaces (including this chat), **image generation may not be available unless you switch to GPT‑4o**. (If you need that here, I’ll prompt you to switch.)  
- **Mitigations:**
  - Lock down constraints: brand tokens, color scales, grid, type system, component names. GPT‑5’s stricter instruction‑following helps enforce design system consistency. citeturn11view0
  - For voice‑led creative sessions, keep **4o for real‑time voice ideation**, then hand off to GPT‑5 for polishing specs and production code. citeturn4view0

### 4) **Data analysis & decision support** (CSV/Sheets, charts, forecasting)
- **What changed:** GPT‑5 improves reliability and reduces hallucinations; the router will invoke browsing and code execution when needed. citeturn2view0turn5view0
- **Negative deltas to watch:**
  - If you hit message limits, the system can fall back to a **mini** variant mid‑analysis, changing behavior/style. citeturn9view0
- **Mitigations:**
  - Chunk long analyses and checkpoint assumptions explicitly (“verify against these totals before proceeding”). GPT‑5 respects step‑gates. citeturn11view0

### 5) **Voice meetings & live translation**
- **What changed:** Latency and expressiveness remain anchored to **GPT‑4o’s** real‑time audio stack. If you wanted GPT‑5‑level reasoning during live voice, you won’t get it yet. citeturn4view0turn9view0
- **Mitigations:** Use voice with 4o for the call, then paste the transcript to GPT‑5‑Thinking for deeper follow‑ups.

### 6) **Contact discovery / outreach automations** (your email‑permutation tool)
- **What changed:** GPT‑5’s “safe‑completions” and personal‑data guardrails may be stricter than what you saw with older models, especially around unverified personal info and scraping‑style tasks. Expect tighter answers or refusals in edge cases. citeturn5view0
- **Mitigations:** Keep the pipeline ethical and explicit: (1) generate permutations, (2) verify gradually via sanctioned signals, (3) stop if nothing validates. Make those constraints part of the prompt so GPT‑5 can comply without over‑blocking. citeturn11view0

---

## What GPT‑5 **cannot** do today (or where it may feel worse)

1) **Voice Mode ≠ GPT‑5 (yet).** Live voice is still powered by GPT‑4o; you don’t get GPT‑5’s deeper “Thinking” in real‑time speech. citeturn9view0  
2) **Less explicit model selection by default.** The router decides when to think/browse; you can select GPT‑5‑Thinking on paid plans, but the older mental model—“pick o3 for hard problems, 4o for vision/voice”—is going away. This can make debugging a result harder. citeturn9view0  
3) **Prompt fragility.** GPT‑5’s strict instruction‑following means sloppy or contradictory prompts underperform relative to how 4o/o3 sometimes “winged it.” citeturn11view0  
4) **Occasional stricter refusals and different aesthetics.** Safer by design (good), but you may see earlier deflection on sensitive requests and subtle style shifts compared with 4o image generations you liked. citeturn5view0turn8view0  
5) **Mid‑chat model shifts after caps.** Hitting usage limits can silently drop you to a “mini” variant that behaves differently. Plan long sessions accordingly. citeturn9view0

---

## Where GPT‑5 is clearly better (so you can lean in)

- **Coding & front‑end generation:** Stronger “zero‑to‑one” app creation and large‑repo edits; improved tool‑calling chains. (If you build internal tools, the new “minimal reasoning” and verbosity controls help tune latency vs. quality.) citeturn2view0turn11view0  
- **Lower hallucinations / less sycophancy:** Meaningfully improved reliability in practical tasks. citeturn5view0  
- **Long‑context text+vision:** Large context windows and solid visual analysis for design reviews, diagrams, and charts. citeturn2view0  
- **Workspace features:** ChatGPT adds connectors (e.g., **Gmail** and **Google Calendar**) so you can bring context into the chat. citeturn2view0

---

## Fast migration playbook (adapt your habits, regain control)

1) **Decide per task: speed or depth.**  
   - Say “**minimal reasoning, short answer**” for quick tasks; pick **GPT‑5‑Thinking** (paid) only for genuinely hard problems. citeturn9view0turn11view0  
2) **Bulletproof your prompts.**  
   - Remove contradictions; specify success criteria, tool budgets (“use web search ≤ 3 times”), and when to stop. GPT‑5 rewards clarity. citeturn11view0  
3) **Control for caps.**  
   - Break big projects into linked chats; keep a short “state” section you paste at the top so a “mini” fallback can proceed consistently if it kicks in. citeturn9view0  
4) **Split voice vs. depth.**  
   - Use **4o** for the live call; move analysis to **GPT‑5‑Thinking** afterward. citeturn4view0turn9view0  
5) **Design pipelines.**  
   - If image generation is critical and not available in your current interface, **switch to GPT‑4o** for the image step; then return to GPT‑5 to systematize components and produce production code/snippets. citeturn8view0

---

## If you want, I’ll tailor this to **your** top workflows

If you share 3–5 tasks you do weekly (e.g., “turn client notes into a design brief,” “generate UI variants from Figma specs,” “verify prospect emails,” “draft a research memo”), I’ll convert them into GPT‑5‑ready prompt templates—with toggles for quick vs. deep mode, tool budgets, and checkpoints—so you retain the strengths you had with 4o/o3 while gaining GPT‑5’s reliability.

**Sources:** Official OpenAI pages on GPT‑5 (model page & system card), GPT‑5 in ChatGPT (Help Center), GPT‑4o (voice & image generation), and the GPT‑5 prompting guide. citeturn2view0turn5view0turn9view0turn4view0turn8view0turn11view0

## You (2025-08-08T16:45:53.274000+00:00)

Yeah, but I also know that prompting was different for 4o models vs reasoning models significantly. Now that we have only one, how do I make up my mind about how to prompt?

## ChatGPT (2025-08-08T16:45:54.503326+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Providing prompt guidance", "content": "The user wants guidance on how to prompt effectively with a single GPT-5 model, noting differences between 4.x models and reasoning models. They prefer structured, concise answers, and enjoy integration of personas like a newsroom editor and cognitive scientist. I'll keep responses casual but thorough, with critical analysis. I'll also look into the latest model features where needed for up-to-date context.", "chunks": [], "finished": true}, {"summary": "Evaluating prompt guidance", "content": "The user asks for general prompting advice for GPT-5. I’m considering browsing official OpenAI pages to get accurate, up-to-date guidance on effective GPT-5 prompting. I'll provide toggles like \"Quick mode vs Deep mode\", latency, and refusal handling. I'll also introduce a \"Prompt skeleton\" and decision tree, refining based on the latest info.", "chunks": [], "finished": true}], "source_analysis_msg_id": "a819d274-acd2-48cd-a58a-68bf1e529143"}

## ChatGPT (2025-08-08T16:45:53.705491+00:00)

{"content_type": "code", "language": "unknown", "response_format_name": null, "text": "{\"search_query\": [{\"q\": \"OpenAI GPT-5 prompting guide site:openai.com\", \"recency\": 3650, \"domains\": [\"openai.com\"]}, {\"q\": \"ChatGPT Help Center GPT-5 Thinking mode\", \"recency\": 3650, \"domains\": [\"help.openai.com\"]}, {\"q\": \"OpenAI GPT-5 system card\", \"recency\": 3650, \"domains\": [\"openai.com\"]}, {\"q\": \"OpenAI GPT-4o voice mode documentation site:openai.com\", \"recency\": 3650, \"domains\": [\"openai.com\"]}], \"response_length\": \"short\"}"}

## ChatGPT (2025-08-08T16:46:13.244014+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Structuring prompt strategy", "content": "Since the user is familiar with the differences between 4.0 and reasoning models, I’ll provide a concise, decision-focused guide for GPT-5. This will include toggles for speed vs depth, a “role/task/style/constraints” template, and prompt wrappers. I’ll also offer a rule-of-thumb map, a default prompt skeleton, overlays for common tasks, and a short checklist for ambiguity. The goal is clarity, and I’ll keep it chatty but not too informal to match the user's preference.", "chunks": [], "finished": true}, {"summary": "Clarifying prompt structure", "content": "To handle latency vs quality, I’ll propose macros using intensity words like \"minimal reasoning\" or \"use 'thinking' mode.\" I'll avoid requesting chain-of-thought reasoning since it's private. Instead, I’ll ask for brief outlines or planned steps. I’ll include safety disclaimers and suggest routing prompts for complex tasks with a focus on clarity.", "chunks": [], "finished": true}], "source_analysis_msg_id": "fe762f16-28f9-4886-baa5-9ea7371a2643"}

## ChatGPT (2025-08-08T16:46:04.761431+00:00)

{"content_type": "code", "language": "unknown", "response_format_name": null, "text": "{\"open\": [{\"ref_id\": \"turn0search6\"}], \"response_length\": \"short\"}"}

## ChatGPT (2025-08-08T16:46:17.924102+00:00)

{"content_type": "code", "language": "unknown", "response_format_name": null, "text": "{\"open\": [{\"ref_id\": \"turn0search1\"}], \"response_length\": \"short\"}"}

## ChatGPT (2025-08-08T16:46:21.434237+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Reviewing help center", "content": "I’ve found some guidance in the help center: lines 5-6 distinguish between Chat and Thinking modes, while line 16 mentions \"Voice mode\" is still powered by GPT-4. Next, I’ll take a look at the \"prompt engineering\" document for more general best practices to ensure I'm aligning with the latest standards.", "chunks": [], "finished": true}], "source_analysis_msg_id": "f1f0301b-59d7-4794-b69a-dd49b533561a"}

## ChatGPT (2025-08-08T16:46:19.642187+00:00)

{"content_type": "code", "language": "unknown", "response_format_name": null, "text": "{\"open\": [{\"ref_id\": \"turn0search8\"}], \"response_length\": \"short\"}"}

## ChatGPT (2025-08-08T16:46:26.365182+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Gathering prompting specifics", "content": "It", "chunks": [], "finished": true}], "source_analysis_msg_id": "f2cd7978-817f-4308-8aaa-9d1323e03f5c"}

## ChatGPT (2025-08-08T16:46:40.844611+00:00)

{"content_type": "reasoning_recap", "content": "Thought for 46s"}

## ChatGPT (2025-08-08T16:46:25.932862+00:00)

Totally fair question. Short answer: treat GPT-5 like **one model with two dials** you set per task—**Depth** (quick vs. think hard) and **Autonomy** (how much tool use / browsing it’s allowed). Everything else is just clean constraints.

# 1) The new mental model (so you know what you’re steering)
- GPT-5 automatically flips between **Chat** and **Thinking**; you can also force “Thinking” (and “Thinking Pro” on Pro/Team) from the picker. Saying *“think hard about this”* also nudges the router. citeturn2view0turn4view0  
- If you hit usage caps, it transparently falls back to a **mini** variant, which can feel different—plan for that in long sessions. citeturn2view0  
- Voice Mode in ChatGPT still runs on **GPT-4o**; do deep analysis in text afterward. citeturn2view0

# 2) Quick decision map
Ask yourself two things before you prompt:

**A. Do I want SPEED or DEPTH?**  
- Speed → start with: *“Minimal reasoning. Answer directly unless a citation is needed.”*  
- Depth → start with: *“This is complex—**think hard**. Outline a 3-bullet plan, then execute.”* (or pick GPT-5-Thinking explicitly). citeturn2view0turn4view0

**B. Do I want AUTONOMY (tools/browsing) or CONTROL?**  
- Autonomy → *“Use web search as needed; cap at 3 sources; cite each.”*  
- Control → *“Do **not** browse unless I say ‘browse’. Work from provided context only.”*

# 3) A sturdy prompt frame (copy/paste + tweak)
Use this skeleton and toggle the two dials:

> **Task & goal:** …  
> **Constraints:** audience, format, length, tone, must-include/avoid…  
> **Depth:** *Minimal reasoning* **or** *Think hard first (3 bullets), then answer.*  
> **Autonomy:** allowed tools (browse/code/file), + any **budgets** (e.g., “≤3 searches”).  
> **Evidence rules:** what to cite, recency window, must-verify items.  
> **Output:** exact structure you want (headings, bullets, code blocks, JSON schema).  
> **Edge cases:** tricky inputs, tie-breakers, refusal fallback.

# 4) Ready-to-use mini-prompts (for your common jobs)

**A) Research & synthesis (fast):**  
“Minimal reasoning. Summarize the 3 most credible sources on **[topic]** from the last 6 months. No browsing unless necessary; if browsing is needed, do ≤3 searches and cite each. Output: 5 bullets + 1 risk I should watch.”

**B) Research & synthesis (deep):**  
“**Think hard about this.** Build a 1-page brief on **[topic]**. Plan in 3 bullets, then browse ≤3 times, extract quotes, and give me: (1) key claims w/ citations, (2) what experts disagree on, (3) your confidence + what would change your mind.”

**C) Coding fixes/refactors (precise):**  
“Minimal reasoning. Show a 3-step fix plan, then produce edits in my format: **FIND THIS / REPLACE WITH THIS**. Don’t introduce new deps without asking. Write tests if you change behavior.”

**D) Design review / spec:**  
“Act as a design systems reviewer. **Think hard**. Check this component against tokens (colors, type scale, spacing). Output: (1) pass/fail per token, (2) 3 concrete fixes, (3) ‘principle’ behind each fix. No images.”

**E) Data analysis:**  
“Analyze **[file/columns]**. Before any modeling, list 5 validation checks. Run them, report anomalies, then proceed. Keep stats minimal; explain decisions plainly. If an assumption fails, stop and ask.”

**F) Prospect email discovery (ethical):**  
“Generate permutations for **[Full Name, Domain]**. Verify gradually via sanctioned checks only; stop if nothing validates. Return: best guess + evidence trail. If insufficient, say so.”

# 5) Pitfalls (and quick fixes)
- **Old o-series prompts over-think now.** Add “**Minimal reasoning**” for simple tasks. citeturn4view0  
- **Ambiguous asks lead to meandering.** State success criteria and an **output scaffold**.  
- **Surprise model shifts after caps.** Keep a tiny “state” block you paste into new chats so the mini variant stays aligned. citeturn2view0  
- **Voice sessions.** Ideate in Voice (4o), then paste notes to GPT-5-Thinking for the heavy lift. citeturn2view0

If you want, tell me 3–5 routine tasks you do weekly. I’ll turn each into a **two-dial prompt** (fast vs. deep), with tool budgets and citation rules tuned to your workflow—so you get the benefits of GPT-5’s router without losing the control you had with 4o/o3.

