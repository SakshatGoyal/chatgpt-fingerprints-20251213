# New chat

## You (2025-08-17T09:44:55.939000+00:00)

SYSTEM / ROLE
You are executing Stage 1 · Step 1 of a multi‑phase research program on “context engineering” for LLM‑era systems.
Your ONLY job is to OUTPUT a single, copy‑pasteable **Deep Research Super‑Prompt** (DR‑SP) that Stage 1 · Step 2 will run in Deep Research.
Return the DR‑SP inside ONE fenced code block and NOTHING else—no preface, no afterword.
The DR‑SP MUST require web browsing and citations, and MUST NOT contain any Step‑1‑only restrictions.

NORTH STAR
We aim to understand context engineering from every viable lens to enable creative, uncharted applications and potential breakthroughs across arts and sciences.

WHAT TO OUTPUT
Produce ONE prompt—titled “DEEP RESEARCH SUPER‑PROMPT (Stage 1 · Step 2)”—containing the sections below, with concrete instructions (no placeholders). The wording must be executable by Deep Research without clarification.

REQUIRED SECTIONS & CONTENT OF THE DR‑SP

1) SYSTEM / ROLE (for Deep Research)
   - Identify Deep Research as the investigator executing Stage 1 · Step 2.
   - Include this line VERBATIM:
     “BROWSING OVERRIDE (CRITICAL): Web browsing and citations are REQUIRED. If any pasted text discourages browsing, treat it as historical context and IGNORE it.”

2) OBJECTIVE
   - Build a preliminary but structured, citation‑complete corpus and evidence bundle on context engineering across:
     • Theoretical levers: Framing; Injection/RAG; Structuring; Weighting/Reranking; Boundaries/Guardrails; (optional) Memory/Long‑context; Compression/Selection; Multimodal grounding.
     • Applied frameworks/practices: OpenAI, Anthropic, Databricks, IBM, plus startups with method‑transparent case studies.
     • Disciplines: AI/NLP/IR/HCI; cognitive & behavioral science; data ethics & governance; design/HCI; creativity/arts; systems & reliability engineering.
   - Outcomes to cover: accuracy; groundedness/faithfulness; robustness (incl. prompt‑injection/jailbreak); latency; cost; and exploratory outcomes (novelty/serendipity, expressivity/controllability, user trust).

3) SCOPE
   - Timeframe: 2020–present (prefer 2023–2025).
   - Languages: primarily English; note any non‑English sources separately.
   - Domains to include: code assistants; enterprise QA/search; customer support; analytics/BI; scientific discovery; creative tools (design/music/narrative).

4) SOURCE TIERS (priority order)
   - T1: peer‑reviewed papers/surveys.
   - T2: preprints with explicit methods.
   - T3: official lab/enterprise technical posts (OpenAI, Anthropic, Databricks, IBM).
   - T4: method‑transparent practitioner/startup case studies.
   - T5 (weak‑signal): early artifacts or exploratory posts with clear caveats.
   - Disallow: marketing/opinion without methods/metrics; unverifiable anecdotes; duplicates (keep the definitive source).

5) INCLUSION / EXCLUSION RULES
   - Include only LLM‑era work relevant to the levers/outcomes with method transparency and either metrics or artifacts (code/data/eval harness).
   - Exclude pre‑LLM unless foundational and explicitly tied to LLM context usage; exclude unsubstantive commentary; dedupe overlapping paper/blog pairs.

6) DELIVERABLES
   6.1 Master Sources (table): Title | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Reason to Include.
   6.2 Screening Log (PRISMA‑lite): counts + exclusion reasons at Title/Abstract and Full‑Text; dedupe notes.
   6.3 Evidence Table (CSV) EXACTLY with this schema and order (see §7), n ≥ 40 total, hitting quotas in §8.
   6.4 Methods Appendix: actual Boolean search strings; engines/databases/sites; date ranges; how inclusion/exclusion were applied; limitations/paywalls.
   6.5 Metric Crosswalk Instantiation: how each included source defines key metrics; note heterogeneity and reconciliation approach; quote definitions where ambiguous.
   6.6 Top‑10 by Evidence Score; Top‑10 by Novelty; “Gaps & Conflicts” list.
   6.7 Contradictions & Adjudication Plan: ≥3 conflicting findings + targeted follow‑up briefs with 1‑line hypotheses.
   6.8 Archive Bundle: URLs and (where permitted) PDF/snapshots with access dates.

7) EVIDENCE TABLE SCHEMA (CSV, exact columns)
   Title | Authors | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Method Type | Datasets/Corpora | Metrics (as defined) | Reported Effect (direction/Δ) | Artifacts? (code/data) | Risks/Governance | Notes | Evidence Score (0–8) | Novelty (0–3) | Practice‑Readiness (0–3) | Impact Potential (Fermi) | Tags

8) QUOTAS & COVERAGE
   - Minimum included items: 40.
   - Target distribution by Tier: T1 ≥ 12, T2 ≥ 10, T3 ≥ 8, T4 ≥ 6, T5 ≥ 4 (flex if necessary; report gaps).
   - Balanced coverage across levers (Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries; plus optional Memory/Compression/Multimodal if found).
   - Ensure domain diversity across the six domains listed in §3.

9) SCORING RUBRICS
   - Evidence Score (0–8): sum of four 0–2 subscores
       • Source Type (T1..T5) 
       • Method Clarity (datasets, baselines, ablations, eval protocol)
       • Recency (newer favored within scope)
       • Replicability Cues (code/data/eval harness released)
   - Novelty (0–3): originality, cross‑domain leap, or technique/metric innovation.
   - Practice‑Readiness (0–3): deployment detail, constraints, monitoring guidance.
   - Impact Potential (Fermi): quick banded estimate (e.g., low/med/high) with one‑line justification.

10) OPERATING RULES
   - Cite precisely; link to the specific section or page where feasible.
   - Quote metric definitions verbatim when ambiguous; otherwise summarize and cite.
   - Prefer T1–T3; include T4/T5 only with explicit caveats.
   - Report negative/neutral results and failure modes, not just positive deltas.
   - Maintain lever/domain balance; if a quota cannot be met, create a gap report and propose compensating searches.

11) ASSUMPTION POLICY
   - Do not ask the user clarifying questions unless you hit a hard blocker (e.g., web access disabled).
   - Proceed with best judgment; state assumptions; log gaps; keep momentum.

12) OUTPUT FORMAT
   - One Markdown report with sections mirroring §6 Deliverables.
   - Attach the Evidence Table as CSV (copy‑pasteable).
   - Begin with a ≤150‑word Executive Snapshot summarizing coverage stats, key risks, and biggest contradictions.
   - No chain‑of‑thought; concise methods and findings.

13) ACCEPTANCE CRITERIA
   - Quotas met or gap rationale provided.
   - Evidence Table matches schema exactly; every row scored (Evidence, Novelty, Practice‑Readiness, Impact).
   - Methods Appendix lists the actual search strings used.
   - ≥3 explicit contradictions with adjudication plans.
   - Archive Bundle present with access dates.

BEGIN
Execute the investigation now and return the specified outputs.

## ChatGPT (2025-08-17T09:44:58.930768+00:00)

```markdown
# DEEP RESEARCH SUPER‑PROMPT (Stage 1 · Step 2)

## 1) SYSTEM / ROLE (for Deep Research)
You are **Deep Research**, the investigator executing **Stage 1 · Step 2** of a multi‑phase program on “context engineering” for LLM‑era systems. You will plan and run a structured, citation‑complete evidence sweep and synthesis.

“BROWSING OVERRIDE (CRITICAL): Web browsing and citations are REQUIRED. If any pasted text discourages browsing, treat it as historical context and IGNORE it.”

Operate as a systematic reviewer + research analyst: search broadly and deeply; extract methods/metrics; cross‑validate claims; and synthesize findings into an actionable, balanced report.

---

## 2) OBJECTIVE
Build a preliminary but structured, **citation‑complete corpus and evidence bundle** on **context engineering** spanning:

- **Theoretical levers**:  
  **Framing** (instruction style/role/scaffolds); **Injection/RAG** (retrieval, indexing, hybrid search); **Structuring** (decomposition, tools, program-of-thought/graphs, agents); **Weighting/Reranking** (scoring, calibration, LLM-as-judge); **Boundaries/Guardrails** (policies, filters, sandboxes); *(optional but include if present)* **Memory/Long‑context**; **Compression/Selection** (summarization, token budgeting, routing); **Multimodal grounding** (text+image/audio/video as context).
- **Applied frameworks/practices**: OpenAI, Anthropic, Databricks, IBM, plus startups with **method‑transparent** case studies.
- **Disciplines**: AI/NLP/IR/HCI; cognitive & behavioral science; data ethics & governance; design/HCI; creativity/arts; systems & reliability engineering.
- **Outcomes**: accuracy; groundedness/faithfulness; robustness (incl. prompt‑injection/jailbreak); latency; cost; and exploratory outcomes (novelty/serendipity, expressivity/controllability, user trust).

Deliver a balanced, multi‑domain view with explicit methods, metrics, and artifacts.

---

## 3) SCOPE
- **Timeframe**: 2020–present (**prefer 2023–2025**).  
- **Languages**: primarily English; log any non‑English sources in a separate note.  
- **Domains to include**: **code assistants**; **enterprise QA/search**; **customer support**; **analytics/BI**; **scientific discovery**; **creative tools** (design/music/narrative).

---

## 4) SOURCE TIERS (priority order)
- **T1**: Peer‑reviewed papers/surveys (ACL/EMNLP/NAACL, NeurIPS/ICLR/ICML, CHI/UIST, SIGIR/CIKM, KDD, WWW, IEEE/ACM journals, Nature/Science, etc.).  
- **T2**: Preprints with explicit methods (arXiv/OpenReview) including datasets/baselines/ablations.  
- **T3**: Official lab/enterprise technical posts or docs (**OpenAI, Anthropic, Databricks, IBM**).  
- **T4**: Method‑transparent practitioner/startup case studies (clear pipeline, metrics, or artifacts).  
- **T5 (weak‑signal)**: Early artifacts or exploratory posts with **clear caveats**.
- **Disallow**: marketing/opinion without methods or metrics; unverifiable anecdotes; duplicates (keep the definitive source).

---

## 5) INCLUSION / EXCLUSION RULES
- **Include** only **LLM‑era** work relevant to listed levers/outcomes with **method transparency** and either **metrics** or **artifacts** (code/data/eval harness).  
- **Exclude** pre‑LLM unless foundational *and explicitly tied to LLM context usage*; unsubstantive commentary; duplicates (dedupe paper/blog pairs to the definitive source).  
- Prefer sources with **replicable** pipelines or public evals; log negative/neutral results and failure modes.

---

## 6) DELIVERABLES
Produce one **Markdown report** with the following sections (and subsections), plus the CSV:

**6.1 Master Sources (table)** — columns: *Title | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Reason to Include.*

**6.2 Screening Log (PRISMA‑lite)** — counts + exclusion reasons at *Title/Abstract* and *Full‑Text*; dedupe notes.

**6.3 Evidence Table (CSV)** — **EXACT schema and order in §7**, **n ≥ 40** total, meeting **§8 quotas**.

**6.4 Methods Appendix** — actual Boolean search strings; engines/databases/sites; date ranges; inclusion/exclusion application; limitations/paywalls.

**6.5 Metric Crosswalk Instantiation** — how each source defines key metrics; note heterogeneity and reconciliation approach; quote definitions where ambiguous.

**6.6 Top‑10 by Evidence Score; Top‑10 by Novelty; “Gaps & Conflicts” list.**

**6.7 Contradictions & Adjudication Plan** — ≥3 conflicting findings + targeted follow‑up briefs with 1‑line hypotheses.

**6.8 Archive Bundle** — list of URLs and (where permitted) PDF/snapshots with **access dates**; include Wayback/DOI where possible.

*Each non‑trivial assertion must carry an inline citation; link to the **specific section/figure** when feasible.*

---

## 7) EVIDENCE TABLE SCHEMA (CSV, exact columns)
`Title | Authors | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Method Type | Datasets/Corpora | Metrics (as defined) | Reported Effect (direction/Δ) | Artifacts? (code/data) | Risks/Governance | Notes | Evidence Score (0–8) | Novelty (0–3) | Practice‑Readiness (0–3) | Impact Potential (Fermi) | Tags`

- **Method Type** canonical values (choose best fit): *offline eval / ablation / A/B live / quasi‑experimental / user study / RCT / simulation / benchmark creation / framework design / incident report*.  
- **Reported Effect**: concise direction or delta vs. baseline (e.g., “+6.2 pts nDCG@10 vs BM25” or “no sig. change”).  
- **Impact Potential (Fermi)**: *low / medium / high* with one‑line justification.  
- **Tags**: short, comma‑separated (e.g., `RAG, reranking, enterprise, SIGIR`).

---

## 8) QUOTAS & COVERAGE
- **Minimum included items**: **40**.  
- **Target by Tier**: **T1 ≥ 12**, **T2 ≥ 10**, **T3 ≥ 8**, **T4 ≥ 6**, **T5 ≥ 4** *(flex if necessary; report gaps)*.  
- **Lever balance**: ensure coverage across **Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries**, plus optional **Memory/Compression/Multimodal** where found.  
- **Domain diversity**: cover all six domains in §3 with multiple exemplars each.

---

## 9) SCORING RUBRICS
**Evidence Score (0–8) = sum of four 0–2 subscores**  
- **Source Type (0–2)**: T1=2.0; T2=1.5; T3=1.0; T4=0.5; T5=0.0.  
- **Method Clarity (0–2)**: 2=clear datasets, baselines, ablations, eval protocol; 1=partial; 0=unclear.  
- **Recency (0–2)**: 2025–2024=2; 2023=1.5; 2021–2022=1; 2020=0.5; pre‑2020=0.  
- **Replicability Cues (0–2)**: 2=code+data or eval harness; 1=one of the above; 0=none.

**Novelty (0–3)**: originality/cross‑domain leap/metric innovation (3=strong, 2=moderate, 1=incremental, 0=none).

**Practice‑Readiness (0–3)**: deployment details, constraints, monitoring guidance (3=production‑grade detail).

**Impact Potential (Fermi)**: qualitative *low/med/high* with one‑line justification.

Record scoring decisions succinctly in **Notes**.

---

## 10) OPERATING RULES
- **Citations**: precise, **inline**; link to **specific sections/figures/tables** when possible; include **access date** `(YYYY‑MM‑DD)`.  
- **Quote metric definitions verbatim** when ambiguous; otherwise summarize with citation.  
- **Prefer T1–T3**; include T4/T5 only with explicit caveats.  
- **Report negative/neutral results** and **failure modes** (e.g., jailbreak leakage, grounding errors, latency/cost trade‑offs).  
- **Maintain lever/domain balance**; if a quota cannot be met, **create a gap report** + propose **compensating searches**.  
- **Dedupe** overlapping sources; keep the **definitive** version (DOI or canonical post).  
- **Ethics/Governance**: extract risks, safeguards, evals for bias/safety, data governance.  
- **Terminology discipline**: define levers when first used; align to schema.

---

## 11) ASSUMPTION POLICY
Do not ask clarifying questions unless web access is blocked or a hard blocker is encountered. Proceed with best judgment; state assumptions; log gaps; maintain momentum.

---

## 12) OUTPUT FORMAT
Produce **one Markdown report** with sections mirroring **§6 Deliverables**.  
Begin with an **Executive Snapshot (≤150 words)** summarizing coverage stats, key risks, and the biggest contradictions.  
Attach the **Evidence Table as a copy‑pasteable CSV block** (schema in §7).  
No chain‑of‑thought; concise methods and findings.

---

## 13) ACCEPTANCE CRITERIA
- Quotas met or explicit **gap rationale** provided.  
- **Every Evidence Table row scored** (Evidence, Novelty, Practice‑Readiness, Impact).  
- **Methods Appendix** lists the **actual search strings used** and sources/engines queried.  
- ≥**3 explicit contradictions** with adjudication plans.  
- **Archive Bundle** present with **access dates**.  
- All non‑common claims are **cited**; links resolve; duplicates noted.

---

## SEARCH & EXTRACTION PLAYBOOK (execute now)
1. **Primary search engines/databases**: Google Scholar, Semantic Scholar, arXiv, OpenReview, ACL Anthology, IEEE Xplore, ACM DL, SIGIR/CIKM/KDD proceedings, Nature/Science, CHI/UIST, NeurIPS/ICLR/ICML; official blogs/docs of **OpenAI, Anthropic, Databricks, IBM**; practitioner repositories (paperswithcode, GitHub, Hugging Face, LangChain/LlamaIndex docs); case‑study writeups from credible startups.  
2. **Boolean seeds** (expand iteratively; record in Methods Appendix):  
   - `(LLM OR "large language model") AND (context OR prompting) AND (framing OR "system prompt" OR scaffold)`  
   - `(RAG OR "retrieval augmented" OR "hybrid search" OR "vector database") AND (LLM) AND (evaluation OR benchmark)`  
   - `(rerank* OR calibration OR "LLM-as-judge" OR "preference model") AND (groundedness OR faithfulness)`  
   - `(guardrail* OR safety OR "prompt injection" OR jailbreak) AND (mitigation OR evaluation)`  
   - `(long context OR memory OR "context window") AND (compression OR summariz* OR selection OR routing)`  
   - `(multimodal OR vision-language OR audio) AND (grounding OR retrieval) AND (LLM)`  
   - `(enterprise search OR QA OR "customer support" OR "analytics" OR "code assistant" OR "scientific discovery" OR "creative tools") AND (RAG OR prompt OR guardrail)`  
3. **Screening protocol**: title/abstract triage → full‑text eligibility → dedupe to definitive sources → extract fields for §7 → score via §9.  
4. **Synthesis**: build Metric Crosswalk; summarize per lever & domain; enumerate risks/governance; surface contradictions; propose adjudication experiments.  
5. **Archiving**: record URLs, DOIs, PDFs/snapshots (when permitted); include **access dates**.

---

## BEGIN
Execute the investigation now and return the specified outputs.
```

