# Custom GPT design comparison

## You (2025-08-17T07:49:21.285000+00:00)

I'm going to share a couple of exchanges between ChatGPT and me. For now, just internalize these, then I'll ask you some questions. 
---
---
---
Me:
So, I need you to put on your professional hat as an AI researcher and scientist who's special skillset involves designing custom GPTs. 

How much of making a custom GPT is about prompt engineering vs context engineering?


---
ChatGPT:
Excellent framing â€” let me answer with my **AI researcher / custom GPT designer hat on.**

---

### âš–ï¸ Prompt Engineering vs Context Engineering in Custom GPT Design

When building a **custom GPT**, youâ€™re not just writing a single prompt â€” youâ€™re designing a **whole operating environment** for the model. That environment is where **context engineering dominates**. Hereâ€™s how the balance usually plays out:

---

#### 1. **Prompt Engineering (â‰ˆ 20â€“30%)**

This is the **entry point design**:

* Crafting system instructions (the â€œpersonaâ€ / role).
* Designing example interactions (few-shot examples).
* Phrasing queries clearly and in the right style.

It matters because bad prompts â†’ confused outputs. But once a solid base is set, the marginal returns on further prompt tweaking diminish.

Think of prompt engineering here as **choosing the front door and signage** for the GPT.

---

#### 2. **Context Engineering (â‰ˆ 70â€“80%)**

This is the **real backbone** of custom GPTs. It includes:

* **Knowledge injection** â†’ Supplying reference docs, curated datasets, or retrieval-augmented memory.
* **Scaffolding** â†’ Designing workflows, structured inputs/outputs, reasoning steps.
* **Guardrails** â†’ Clear constraints (what to ignore, what to emphasize, failure modes to avoid).
* **Continuity mechanisms** â†’ Memory or context threading across sessions.
* **Persona depth** â†’ Framing with layered roles (e.g., â€œact as a scientist, but also highlight policy implications as a regulator wouldâ€).

In practice, the **context setup determines 80â€“90% of reliability**. You can give the same prompt to two GPTs with different context ecosystems, and one will perform like a polished professional while the other feels generic.

Think of context engineering as **designing the building, tools, and reference library that sit behind the front door.**

---

### ğŸ”¬ Why Context Engineering Dominates in Custom GPTs

* **Durability**: A prompt is one-shot, but context scaffolding applies across all user interactions.
* **Precision**: Fine-grained retrieval, constraints, and weighting stop drift and hallucinations.
* **Adaptability**: You can swap or update context layers (e.g., new documents, new memory rules) without rewriting the â€œprompt persona.â€
* **Scalability**: Context frameworks let your GPT handle varied inputs consistently, not just the narrow cases your prompt anticipated.

---

### ğŸ§ª Researcherâ€™s Analogy

If you think of **custom GPT design as building a lab experiment**:

* Prompt engineering = writing the experimental instructions.
* Context engineering = setting up the lab environment (equipment, controlled variables, data, constraints).

Without the environment, even the clearest instructions wonâ€™t yield reproducible results.

---

âœ… **Bottom line:**

* In custom GPTs, **prompt engineering is the seed**;
* **Context engineering is the soil, water, and ecosystem** that determines whether it grows into a robust, reliable model.

---

Would you like me to sketch out a **framework for evaluating your own custom GPTs** â€” like a scoring system for how much weight youâ€™ve put into prompt vs context design? That might give you a practical tool to assess balance.

---

Me:
Actually, I want to create an outline for research. My goal is to understand frameworks and any additional research professionals have done on the topic of context engineering. You were able to set up a good outline of the main levers: Contextual framing, context injection, context structuring, context weighting, contextual boundaries... But I'm sure most professional researchers may or may not have necessarily classified their research in these terms. If they have, that's great. But if not, then I'm sure it's something new, and if it has enough citations, then it's probably a novel idea or approach to context engineering that we don't know about. Could you think through how we could structure our research?

My goal is to create a research plan that centers around analyzing both research papers, as well as reputable sources like Databricks, OpenAI, Anthropic, IBM, and maybe others. In addition, it wouldn't be bad to also look at some practical applications from startups. 

Start by thinking through all the context you might need to understand more about my goals and objectives clearly, and ask me additional questions. I am looking to develop a comprehensive outline for a research project focused on understanding the various frameworks and research findings related to context engineering. In our previous discussions, you provided a solid foundation by outlining the main levers associated with this field, including contextual framing, context injection, context structuring, context weighting, and contextual boundaries. However, I recognize that many professional researchers may not categorize their work using these specific terminologies. If such classifications do exist within the literature, that would be highly beneficial. Conversely, if they do not, this could indicate a novel approach to context engineering that warrants attention, particularly if it has garnered significant citations or recognition in the field.

The objective of my research is twofold: first, to conduct a thorough analysis of academic research papers on context engineering, and second, to explore reputable sources and industry insights from organizations such as Databricks, OpenAI, Anthropic, IBM, and potentially others. This will allow for a well-rounded understanding of both theoretical and practical applications. Moreover, I plan to investigate how startups are putting context engineering principles into practice, as they often drive innovation in emerging areas.

To refine my research plan further, I would appreciate your assistance in identifying all the relevant contexts necessary for enhancing my understanding of my goals and objectives. I am particularly interested in any specific areas of focus, research methodologies, or key questions that should guide my exploration in this field. Please feel free to ask any additional questions that might help clarify my intentions and strengthen the overall research framework.

---
ChatGPT:
Perfect â€” I see what youâ€™re aiming for. Youâ€™re not just looking to *define* context engineering, but to treat it as a serious research area: gathering, classifying, and evaluating both academic and industry frameworks. That means we need to be careful about **scope, depth, and methodology** before we sketch the outline.

To make sure I really understand your intentions and to help sharpen your research framework, let me probe a bit with targeted questions:

---

### ğŸ” Clarifying Questions About Your Research Goals

1. **Theoretical vs. Practical Balance**

   * Do you want your research to lean more toward **academic theory** (e.g., grounding context engineering in cognitive science, HCI, or NLP literature), or **applied frameworks** (e.g., how Databricks, Anthropic, startups implement it in practice)?
   * Or do you want a **hybrid approach**, where you explicitly contrast academic and industry framings?

2. **Disciplinary Scope**

   * Should this research sit *primarily within AI/ML literature*, or should it also scan **adjacent fields** where â€œcontextâ€ has been rigorously studied â€” like information retrieval, distributed systems, communication theory, or cognitive science?
   * Example: retrieval-augmented generation (RAG) comes from IR traditions, not just LLM prompt work.

3. **Temporal Scope**

   * Are you focusing **exclusively on LLM-era research (2020â€“present)**, or do you want to look at **historical antecedents** (e.g., context windows in classical NLP, conversational AI, HCI frameworks)?

4. **Evaluation Angle**

   * Do you want to build a **taxonomy/framework** of context engineering (similar to how you already listed framing/injection/structuring/weighting/boundaries)?
   * Or do you want to run a **meta-analysis** of whatâ€™s been studied, which methods are cited, and where the gaps are?

5. **Methodology**

   * Do you see this project as primarily:

     * **Literature review + synthesis** (classifying and evaluating existing frameworks),
     * **Comparative case study** (e.g., contrast how OpenAI, Anthropic, Databricks each articulate context design),
     * **Exploratory grounded theory** (extracting new categories like the ones you proposed from raw literature), or
     * Some combination?

6. **Output Format**

   * What kind of artifact do you want to produce?

     * Academic-style **literature review / survey paper**?
     * A **practitionerâ€™s framework or guide** (applied angle)?
     * A **white paper / briefing** aimed at both researchers and practitioners?

7. **Application Layer**

   * You mentioned startups. Do you want to actually **map case studies** of how startups are using context engineering (e.g., LangChain, Pinecone, LlamaIndex, etc.), or just note them as supplementary evidence of trends?

8. **Novelty Aspiration**

   * Do you want to **coin and formalize â€œcontext engineeringâ€ as a structured term** (with your levers as a framework), or treat it descriptively (i.e., surveying what others call it, even if terminology varies)?
   * This determines whether your work is **synthesis** (mapping othersâ€™ work) or **conceptual innovation** (proposing your own classification and situating it against prior work).

---

### Why These Questions Matter

The answers will shape:

* **Scope of sources** (AI-only vs interdisciplinary).
* **Depth of analysis** (taxonomy vs applied examples vs meta-analysis).
* **Methodology** (systematic literature review vs thematic synthesis).
* **Contribution** (survey vs framework proposal).

---

ğŸ‘‰ My suggestion: letâ€™s refine your intent around those questions first, then I can draft a **research plan outline** with:

1. Research objectives,
2. Methodology & sources,
3. Thematic clusters (how to search/classify findings), and
4. Proposed structure of the final output.

---

Would you like me to walk you through a **decision tree** of these questions so you can pick your preferred direction? That way, weâ€™ll converge on a research framework thatâ€™s both rigorous *and* aligned with your goals.
---
Me:
**Theoretical vs. Practical Balance:**
I have a strong desire to learn about various concepts, but I also recognize that effective learning goes hand in hand with understanding practical applications in real-world scenarios. Therefore, I propose we adopt a hybrid approach that combines theoretical frameworks with hands-on practice. This will not only enhance our comprehension but also equip us with the necessary skills to apply our knowledge effectively.

**Disciplinary Scope:**
This topic is particularly intriguing. I believe itâ€™s essential to broaden our focus beyond just the traditional AI and machine learning (ML) literature. By scanning adjacent fieldsâ€”such as behavioral science, cognitive psychology, or data ethicsâ€”we can gain a more comprehensive view of the broader landscape. This interdisciplinary perspective can lead to innovative insights and applications.

**Temporal Scope:**
For our research, we should specifically concentrate on the era of Large Language Models (LLMs), which have significantly changed the landscape of AI research and applications in recent years. Focusing on this timeframe will allow us to analyze contemporary studies that are both rigorous and well-cited. The expectation is that these researchers must have referenced prior literature to substantiate their findings, thus creating a clear linkage between past and present research.

**Evaluation Angle:**
Determining the evaluation angle is quite complex. Our research should strike a balance, encompassing both basic research, which aims to expand general knowledge and understanding, and applied research, which focuses on addressing and solving practical, real-world challenges. However, our final synthesis should include a causal analysis that allows us to identify and understand the underlying reasons for successful outcomes without relying on conjecture. While I prefer to avoid the term 'meta-synthesis'â€”because it can overlook critical nuancesâ€”I do want to engage in some form of inductive thematic analysis. This will enable us to let patterns emerge organically while simultaneously implementing a degree of deductive framework creation. This structured approach will help us organize and present our insights in a valuable manner. I anticipate that this process may involve multiple phases to ensure thorough exploration and understanding.

**Methodology:**
I have previously touched upon aspects of methodology when discussing our evaluation angle. To further refine our approach, we will incorporate integrative and lateral synthesis. This will allow us to explore unconventional connections and ideas from various domains, broadening our exploration beyond just AI/ML research. By linking insights from different disciplines, we can foster creativity and innovation in our findings.

**Output Format:**
At present, we do not have a specific output format established; however, our primary goal is to thoroughly understand the current landscape so that we can formulate our own informed opinions. Additionally, we aim to create a practical guide that will assist us in applying some of these principles in real-world situations without oversimplifying the insights we gather. I am concerned that condensing a wealth of information into a single narrative may dilute its value, resulting in generalities that lack practical applicability. Consequently, we must design our research to address multiple different scenarios and contexts, which may involve exploring unique and novel use cases for generative AI technologies.

**Application Layer:**
I am particularly interested in mapping case studies that demonstrate how startups are leveraging the concept of context engineering. This involves understanding how these organizations effectively integrate and optimize contextual information within their products and services to create meaningful and personalized user experiences.

**Novelty Aspiration:**
Should our approach be descriptive in nature, surveying how various stakeholders characterize context engineering? I believe it is important to acknowledge that terminology can vary significantly across different fields, and embracing this diversity can enhance our understanding and application of the concept. Thus, identifying and analyzing these differing perspectives will be beneficial in creating a comprehensive overview.



---
Based on the information I have provided in this as well as the previous response, outline a comprehensive multi-phase research and synthesis plan. This should encompass the following stages:

1. **Approach to Gathering Data**: Define the research questions and objectives. Identify the methods for data collection, including qualitative and quantitative approaches. Specify the target population and sampling methods.

2. **Data Collection**: Describe the tools and techniques that will be used to gather data, such as surveys, interviews, focus groups, or observational studies. Provide a timeline for the data collection phase and any ethical considerations.

3. **Sense-Making and Synthesis**: Explain the process for analyzing the collected data. This may involve coding qualitative data, statistical analysis for quantitative data, or comparative analysis. Discuss how insights will be drawn from the data and how they will be synthesized into meaningful findings.

4. **Final Output**: Specify the format of the final deliverable, whether it be a report, presentation, or another format. Outline the key components that will be included, such as an executive summary, recommendations, and potentially actionable insights based on the research findings.

---

As you engage in this process, envision yourself as a seasoned research director at IDEO, a leader with extensive experience in overseeing research teams that tackle innovative and complex subjects. Throughout your career, you have honed your ability to distill vague and ambiguous insights into clear, actionable strategies that drive meaningful outcomes. 

In this particular project, you are taking on the challenge of guiding your team in the specialized field of context engineering. This involves not only identifying and understanding the nuances of various contexts but also translating those insights into practical applications that can enhance user experiences and inform design decisions. Your leadership will be crucial in helping the team navigate this intricate landscape, as you encourage collaboration, creativity, and critical thinking to uncover valuable insights that can be transformed into impactful solutions.
Each phase should detail specific tasks and outline the approaches to be considered for clarity and organization throughout the research process.

---

ChatGPT:
Absolutely. Iâ€™ll treat this like Iâ€™m your research director at IDEO, tighten the scope to **secondary research only**, and give you a **clear, multiphase plan** you can actually runâ€”no interviews, no surveys, no diary studies.



---

## Overview: What this plan does (secondary research only)

1. **Find** the best available public knowledge (papers, preprints, credible industry reports, technical blogs, standards).
2. **Filter** it systematically (transparent include/exclude rules; PRISMAâ€‘style tracking).
3. **Extract** comparable facts into one evidence table (simple spreadsheetâ€”no JSON required).
4. **Weigh** the evidence (recency, rigor, venue, method transparency).
5. **Synthesize** into a practical framework (taxonomy, patterns/antiâ€‘patterns, evaluation playbooks, governance implications).

Weâ€™ll lean on established review practices (PRISMA as a reporting scaffold) and reputable sources (e.g., PRISMA 2020 for methods; RAG literature; OpenAI/Anthropic/Databricks/IBM posts on retrieval and evaluation). ([PRISMA statement][1], [BMJ][2], [arXiv][3], [NeurIPS Proceedings][4], [OpenAI Platform][5], [OpenAI Cookbook][6], [Anthropic][7], [Databricks][8], [IBM TechXchange Community][9], [IBM][10])

---

## Phase 0 â€” Scope Lock & Operating Principles (90 minutes)

**Goal:** Freeze whatâ€™s â€œinâ€ vs â€œout,â€ so the team can move fast without secondâ€‘guessing.

* **Research questions (RQs):**
  RQ1: How do academia and industry **define and operationalize** context engineering (LLM era)?
  RQ2: Which **mechanisms/levers** (e.g., framing, injection/RAG, structuring, weighting/reranking, boundaries/guardrails) are described and with what **reported effects**?
  RQ3: How do major labs and platforms **evaluate** context choices (metrics, harnesses, benchmarks)?
  RQ4: What **risks and governance** practices are recommended (prompt injection, privacy/IP, provenance)?
  RQ5: What **practical patterns** do production teams use; what are the **antiâ€‘patterns**?

* **Time window:** 2020â€“present (LLM era) with a preference for 2023â€“2025 materials when available.

* **Source hierarchy (for weighting later):**

  1. Peerâ€‘reviewed papers & surveys â†’ 2) Preprints with methods (arXiv/ACL) â†’ 3) Official lab/enterprise technical posts (OpenAI, Anthropic, Databricks, IBM) â†’ 4) Highâ€‘signal practitioner writeâ€‘ups (clear methods, artifacts) â†’ 5) Aggregators/â€œawesomeâ€ lists (for discovery only). ([PRISMA statement][1], [arXiv][3], [NeurIPS Proceedings][4], [OpenAI Platform][5], [OpenAI Cookbook][6], [Anthropic][7], [Databricks][8], [IBM TechXchange Community][9])

* **Inclusion criteria (examples):** LLMâ€‘era work on retrieval/context/memory/guardrails/evaluation; explicit methods or measurable claims; reputable venue/source.
  **Exclusion:** Opinion pieces with no method; preâ€‘LLM unless foundational (e.g., RAG 2020); marketing without technical detail.

* **Outputs:** 1â€‘page scope note; search strings; sheet templates (see Phase 2).

---

## Phase 1 â€” Source Discovery (Identification) (Â½â€“1 day or sprintable in 2â€“3 hours)

**Goal:** Build a **comprehensive source list** across academia + industry.

* **Academic databases:** Google Scholar, arXiv, ACL Anthology, ACM DL, IEEE Xplore.
  Seed terms (combine/iterate): *â€œcontext engineeringâ€, â€œinâ€‘context learningâ€, retrievalâ€‘augmented generation / RAG, contextual retrieval, reranking, longâ€‘context, memory, guardrails, evaluation harnesses, groundedness/faithfulnessâ€*. Include Boolean variants and synonyms. ([arXiv][3], [NeurIPS Proceedings][4])

* **Industry sources:**

  * **OpenAI:** eval frameworks and cookbook docs (eval datasets, YAML-based evals). ([OpenAI Platform][5], [OpenAI Cookbook][6])
  * **Anthropic:** â€œContextual Retrievalâ€ (contextual embeddings + contextual BM25), longâ€‘context/model updates. ([Anthropic][7], [Tom's Guide][11])
  * **Databricks:** RAG evaluation & dataâ€‘foundation guidance. ([Databricks][8], [Databricks Community][12])
  * **IBM:** watsonx.governance metrics for RAG/agents. ([IBM TechXchange Community][9], [IBM][10])

* **Curated/aggregators (for discovery):** Current â€œawesomeâ€ lists and recent surveys on context engineering to mine citations forward/backward. ([arXiv][13], [GitHub][14])

* **Tracking (PRISMAâ€‘style):** Keep a running log of counts from each source and mark duplicatesâ€”no need to draw the flowchart yet; just capture the numbers. ([PRISMA statement][1])

**Deliverable:** â€œMaster Sourcesâ€ tab with: title, year, venue, URL, type (peerâ€‘reviewed / preprint / official blog), short reason to include.

---

## Phase 2 â€” Screening & Eligibility (Â½ day)

**Goal:** Narrow to a **clean, relevant corpus**; record why items were excluded.

* **Title/abstract screen** â†’ **Fullâ€‘text screen** with clear reasons (e.g., â€œno methods,â€ â€œnot LLMâ€‘era,â€ â€œnot about context mechanismsâ€).
* **Deâ€‘duplication:** Remove multiple posts that mirror the same work (e.g., blog + paper â†’ keep the definitive one).

**Deliverables:**

* â€œScreenedâ€‘Inâ€ tab (final set).
* â€œScreenedâ€‘Outâ€ tab (title + reason).
* PRISMA counts (ID â†’ Screened â†’ Eligible â†’ Included). ([PRISMA statement][1])

---

## Phase 3 â€” Data Extraction (1â€“2 days; sprintable in 2â€“3 hours)

**Goal:** Pull **comparable facts** from each item into a single spreadsheet (no JSON).

**Use a simple spreadsheet with these columns:**

* **Bibliography:** Title | Authors | Year | Venue/Source | URL
* **Context lever(s):** Framing | Injection (incl. RAG) | Structuring | Weighting/Reranking | Boundaries/Guardrails | (Optional: Compression, Memory, Multimodal)
* **Domain:** e.g., code, QA, customer support, health, legal
* **Method type:** Empirical study, ablation, benchmark eval, case writeâ€‘up, architecture post
* **Metrics reported:** e.g., accuracy, groundedness/faithfulness, retrieval precision/recall, latency, cost; note the exact definitions used
* **Evidence strength:** Peerâ€‘reviewed / preprint / official lab post / practitioner blog
* **Key claims & numbers:** Brief, with **verbatim snippet + page/section** where possible
* **Risks/Governance:** injection, PII, provenance, evaluation/monitoring recommendations
* **Notes/quotes:** concise

> Tip: Use ChatGPT to **draft** extraction, then paste in a **confirming quote** from the source to avoid LLM drift. Always keep the link to the specific section or PDF page. (OpenAI/Anthropic/Databricks/IBM pages and RAG papers are good practice material.) ([OpenAI Platform][5], [OpenAI Cookbook][6], [Anthropic][7], [Databricks][8], [arXiv][3], [NeurIPS Proceedings][4])

**Deliverable:** â€œEvidence Tableâ€ tab (one row per source).

---

## Phase 4 â€” Evidence Appraisal & Weighting (Â½ day)

**Goal:** Make the extraction **decisionâ€‘ready** by indicating confidence.

* **Scoring rubric (0â€“2 each):**
  *Source type* (peerâ€‘reviewed > preprint > official lab > blog); *method clarity* (datasets, baselines, ablations); *recency*; *replicability cues* (code/data, eval harness). Sum to a 0â€“8 â€œevidence strengthâ€ score.
* **Flag heterogeneity:** Note when metrics are incomparable (e.g., different groundedness proxies).
* **Normalize names:** Create a **metric crosswalk** (how different sources define/compute â€œgroundedness/faithfulness,â€ retrieval quality, etc.). ([PRISMA statement][1])

**Deliverables:** Evidence Table updated with scores + a â€œMetric Crosswalkâ€ tab.

---

## Phase 5 â€” Thematic Synthesis & Crossâ€‘Field Mapping (1 day; sprintable in 90 min)

**Goal:** Turn rows into **insights**.

* **Inductive themes** that appear repeatedly (e.g., â€œContextual retrieval improves recall in domainâ€‘specific corpora,â€ â€œExplicit boundary prompts reduce offâ€‘policy actionsâ€). ([Anthropic][7])
* **Deductive mapping** into your lever model (Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries).
* **Crosswalk of terminology** (what OpenAI/Anthropic/Databricks/IBM and academics call similar ideas). ([OpenAI Platform][5], [Anthropic][7], [Databricks][8], [IBM TechXchange Community][9])

**Deliverables:**

* â€œThemesâ€ tab (theme â†’ evidence bullets â†’ top sources).
* â€œTerm Crosswalkâ€ tab.

---

## Phase 6 â€” Comparative Frameworks & Landscape (1 day)

**Goal:** Compare **how different camps operationalize and evaluate** context engineering.

* **Matrix 1 â€” Mechanisms vs Outcomes:** Rows = mechanisms/levers; Columns = outcomes (accuracy, groundedness, robustness, latency, cost). Fill with **reported** effects + strength scores.
* **Matrix 2 â€” Frameworks:** OpenAI evals vs Anthropic contextual retrieval vs Databricks RAG evaluation vs IBM governance metricsâ€”what each emphasizes, data needed, typical failure modes each addresses. ([OpenAI Platform][5], [OpenAI Cookbook][6], [Anthropic][7], [Databricks][8], [IBM TechXchange Community][9], [IBM][10])
* **Matrix 3 â€” Domain patterns:** What shows up in code assistants vs QA vs support vs analytics.

**Deliverables:** Three sideâ€‘byâ€‘side matrices + oneâ€‘page interpretation.

---

## Phase 7 â€” Practical Synthesis (Patterns, Antiâ€‘patterns, Playbooks) (1 day)

**Goal:** Translate findings into **what to do** in real projects.

* **Design patterns** (with evidence citations):
  *E.g.,* â€œContextual embeddings + rerankingâ€ for domain RAG; â€œschemaâ€‘enforced structuringâ€ for tool calling; â€œboundary prompts + allow/deny listsâ€ for guardrails. ([Anthropic][7])
* **Antiâ€‘patterns:** Unbounded retrieval sets; reliance on uncalibrated LLM grading; mixing policies and knowledge in the same context block; undefined freshness policy.
* **Evaluation playbook:** How to run a lightweight, **secondaryâ€‘dataâ€‘only** evaluation (reusing public datasets, harness patterns from OpenAI Evals posts/cookbook; noting limitations). ([OpenAI Platform][5], [OpenAI Cookbook][6])
* **Governance checklist:** Risks, provenance, monitoring metrics surfaced by IBM/databricks posts (context relevance, answer support, hallucination rate proxies). ([IBM TechXchange Community][9], [IBM][10])

**Deliverables:**

* â€œPatterns & Antiâ€‘patternsâ€ deck pages.
* â€œEvaluation & Governanceâ€ quickâ€‘start sheets.

---

## Phase 8 â€” Final Outputs

* **Landscape Report (modular):** Executive summary; evidenceâ€‘backed definitions; mechanismâ€‘outcome matrices; patterns/antiâ€‘patterns; evaluation & governance quickâ€‘starts; annotated bibliography.
* **Practitioner Toolkit:** Sheets (Evidence Table, Metric Crosswalk, Term Crosswalk, Three Matrices); a â€œPrompt Packâ€ for ChatGPT to help with extraction and quoteâ€‘checking (secondaryâ€‘only).
* **Appendices:** PRISMA counts; search strings; evidence scoring rubric. ([PRISMA statement][1])

---

## â€œFast vs Thoroughâ€ Run Modes (pick one)

* **Sprint (â‰ˆ4 hours):**
  Phase 0 (30m) â†’ Phase 1 (60m) â†’ Phase 2 (30m) â†’ Phase 3 (60m on top sources) â†’ Phase 5 (45m) â†’ Phase 7 (45m). Output: a lean evidence table (\~25â€“40 items), 2 matrices, 1â€‘page playbook.
* **Rapid (2â€“3 days):** Full Phases 0â€“7 with \~80â€“120 items.
* **Deep (1â€“2 weeks):** Full Phases 0â€“8 with \~150â€“200 items and stronger weighting/bibliometrics.

---

## Risks & Limitations of Secondaryâ€‘Only Research (and how we mitigate)

1. **Publication/vendor bias** (only successes get written up; blogs can be promotional).
   *Mitigate:* weight by source type & method clarity; triangulate across independent sources; call out conflicts. ([PRISMA statement][1])
2. **Metric heterogeneity** (everyone defines â€œgroundednessâ€ differently).
   *Mitigate:* build a Metric Crosswalk; avoid apples-to-oranges comparisons; report effect directions rather than raw score diffs.
3. **Recency drift** (fast-moving field).
   *Mitigate:* tag each item with date; prefer 2023â€“2025; include a â€œwhat likely changed since publicationâ€ note; schedule a quick refresh pass before finalizing.
4. **Paywalls/access** (ACM/IEEE).
   *Mitigate:* favor arXiv preprints and official lab posts; use abstracts + related works if full text unavailable; note as â€œpartial access.â€ ([arXiv][3], [NeurIPS Proceedings][4])
5. **Ambiguity in claims** (no code, no data).
   *Mitigate:* require a **verbatim support snippet** and mark â€œconfidence lowâ€ when artifacts are absent.
6. **LLM extraction errors** when using ChatGPT as an assistant.
   *Mitigate:* enforce quoteâ€‘back verification; keep links to exact sections; human spotâ€‘checks on a random 10â€“20% sample.

---

## Tools & Templates (simple, no code)

* **Sheets (four tabs):** Master Sources; Evidence Table; Metric Crosswalk; Term Crosswalk.
* **PRISMA Log:** one small table capturing counts per step; you can draw the flow later if needed. ([PRISMA statement][1])
* **Prompt Pack (for ChatGPT):**

  1. â€œExtract bibliographic fields + lever tags + metrics; supply a confirming quote + link.â€
  2. â€œMap source terms to our lever taxonomy; suggest synonyms for the crosswalk.â€
  3. â€œSummarize reported effects and caveats in â‰¤4 bullets; no speculation.â€

---

## What success looks like

* A **traceable corpus** of 80â€“120 sources with clear include/exclude reasons.
* A **weighted evidence table** that lets you answer: which levers, under what conditions, for which outcomes.
* **Actionable artifacts**: mechanismâ€‘outcome matrices, patterns/antiâ€‘patterns, and a minimal evaluation & governance starter you can use immediately.

---

If youâ€™d like, I can **drop in a readyâ€‘toâ€‘copy Google Sheets structure** (columns preâ€‘named exactly as above) and a oneâ€‘page â€œPrompt Packâ€ you can paste into ChatGPT to run Phases 1â€“3 quickly.

[1]: https://www.prisma-statement.org/prisma-2020-statement?utm_source=chatgpt.com "PRISMA 2020 statement"
[2]: https://www.bmj.com/content/bmj/372/bmj.n71.full.pdf?utm_source=chatgpt.com "The PRISMA 2020 statement: an updated guideline for ..."
[3]: https://arxiv.org/abs/2005.11401?utm_source=chatgpt.com "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks"
[4]: https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf?utm_source=chatgpt.com "Retrieval-Augmented Generation for Knowledge-Intensive ..."
[5]: https://platform.openai.com/docs/guides/evals?utm_source=chatgpt.com "Evaluating model performance - OpenAI API"
[6]: https://cookbook.openai.com/examples/evaluation/getting_started_with_openai_evals?utm_source=chatgpt.com "Getting Started with OpenAI Evals"
[7]: https://www.anthropic.com/news/contextual-retrieval?utm_source=chatgpt.com "Introducing Contextual Retrieval"
[8]: https://www.databricks.com/blog/LLM-auto-eval-best-practices-RAG?utm_source=chatgpt.com "Best Practices for LLM Evaluation of RAG Applications"
[9]: https://community.ibm.com/community/user/watsonx/blogs/bob-reno/2024/11/12/evaluate-and-monitor-rag-in-watsonxgovernance?utm_source=chatgpt.com "Evaluate and Monitor RAG in watsonx.governance"
[10]: https://www.ibm.com/new/announcements/ibms-answer-to-governing-ai-agents-automation-and-evaluation-with-watsonx-governance?utm_source=chatgpt.com "IBM's answer to governing AI Agents: Automation and ..."
[11]: https://www.tomsguide.com/ai/claude-just-got-a-major-personalization-upgrade-heres-whats-new?utm_source=chatgpt.com "Claude just got a major personalization upgrade - here's what's new"
[12]: https://community.databricks.com/t5/technical-blog/six-steps-to-improve-your-rag-application-s-data-foundation/ba-p/97700?utm_source=chatgpt.com "Six steps to improve your RAG application's data foundation"
[13]: https://arxiv.org/html/2507.13334v1?utm_source=chatgpt.com "A Survey of Context Engineering for Large Language Models"
[14]: https://github.com/Meirtz/Awesome-Context-Engineering?utm_source=chatgpt.com "Meirtz/Awesome-Context-Engineering"

## ChatGPT (2025-08-17T07:49:26.038949+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Acknowledging userâ€™s message", "content": "Got it - Iâ€™ve internalized the exchanges, including the hybrid approach and interdisciplinary scope in the LLM-era, etc. Ready whenever you are! I wonâ€™t add new details or inquiries, just confirming everything is noted. Let's keep it light and easy to follow.", "chunks": [], "finished": true}], "source_analysis_msg_id": "fec620a9-05c0-4b83-b8e8-4370cdee823a"}

## ChatGPT (2025-08-17T07:49:37.048649+00:00)

{"content_type": "reasoning_recap", "content": "Thought for 11s"}

## ChatGPT (2025-08-17T07:49:24.707743+00:00)

Got itâ€”Iâ€™ve internalized the full exchange and your goals.

Key anchors Iâ€™m holding:
- Hybrid theory + practice, LLM-era focus, interdisciplinary scan (IR, HCI, cog psych, ethics).
- Inductive thematic analysis + deductive frameworking; evidence-weighted synthesis (not hand-wavy â€œmeta-synthesisâ€).
- Compare academia with OpenAI/Anthropic/Databricks/IBM and startup case studies.
- Outputs that are modular and actionable (patterns, anti-patterns, eval/governance playbooks)â€”not a single flattened narrative.

Ready for your questions.

## You (2025-08-17T07:57:55.532000+00:00)

Imagine you are a power user of ChatGPT, excelling in its capabilities. You are not only an authority in artificial intelligence but also a seasoned researcher who has harnessed advanced tools like GPT-5 Pro and Deep Research by OpenAI. Your expertise allows you to conduct groundbreaking research that produces novel insights unseen by others. Leveraging your experience with ChatGPT, you are tasked with developing a comprehensive four-phase blueprint to execute this innovative research plan using these powerful tools, ideally in a synergistic manner. 

### Phase 1: Defining the Data Gathering Approach
In this initial phase, your primary objective is to establish a clear and effective strategy for data gathering. Here are the steps involved:
1. **Identify Research Questions**: Formulate specific, focused research questions that will guide your data collection efforts.
2. **Determine Data Sources**: Decide on the types of data sources you will utilize, such as academic journals, online databases, or existing frameworks in your field of interest.
3. **Select Tools and Techniques**: Based on your research objectives, identify the appropriate features and capabilities of GPT-5 Pro and Deep Research that will facilitate efficient data collection. 
4. **Create a Preliminary Framework**: Develop a structured outline of how you plan to use each tool throughout the research process, ensuring a cohesive approach.

### Phase 2: Data Collection
This phase is dedicated to the extensive gathering of data, which could involve several specialized research prompts. The steps include:
1. **Crafting Deep Research Prompts**: Design and refine multiple prompts for Deep Research that will elicit comprehensive information related to your research questions. These prompts should be tailored to extract diverse perspectives and insights.
2. **Utilizing GPT-5 Pro for Enhanced Queries**: Employ GPT-5 Pro to generate advanced inquiries, summarizations, or specific content related to your topics, enhancing the breadth of data collected.
3. **Iterative Feedback Loop**: Establish a method for evaluating the responses gained from both tools. Use this feedback to pivot your approach if necessary, ensuring the data aligns with your research goals.
4. **Organizing Collected Data**: Once accumulated, categorize the data systematically for easy access and integration in the next phase.

### Phase 3: Sensemaking and Synthesis
In the third phase, your focus shifts to interpreting and synthesizing the collected data through various methods:
1. **Thematic Analysis**: Analyze the data by identifying patterns, themes, and significant findings that address your research questions.
2. **Cross-referencing Insights**: Utilize both GPT-5 Pro and Deep Research to cross-reference insights, ensuring accuracy and depth in your interpretations.
3. **Visual Synthesis**: Create visual representations (such as charts or mind maps) to illustrate the connections and relationships between different themes and insights.
4. **Collaborative Insights**: If applicable, engage with peers or experts to discuss and refine the synthesized findings, adding richer insights to your conclusions.

### Phase 4: Creating the Final Output
The final phase is about consolidating your research into a coherent output that communicates your findings effectively:
1. **Drafting the Research Document**: Begin writing the final report or paper, integrating your research questions, data analysis, and synthesized insights in a clear and organized manner.
2. **Utilizing GPT-5 Pro for Editing and Enhancement**: Leverage GPT-5 Pro to help edit, refine, and enhance the clarity and impact of your writing.
3. **Incorporating Visuals and References**: Ensure that your output includes relevant visuals and properly cited references to support your findings.
4. **Final Review and Presentation**: Review the document thoroughly to ensure accuracy and coherence, then prepare for presentation or publication, considering the most effective means to share your research findings with your intended audience.

---
Ensure you're not drifting too far away from all the context established in the previous message I've sent you.

## ChatGPT (2025-08-17T07:58:16.982912+00:00)

Below is a **fourâ€‘phase, endâ€‘toâ€‘end blueprint** that operationalizes your contextâ€‘engineering research plan using **Deep Research** (for breadth, citationâ€‘rich retrieval, multiâ€‘step planning) and **GPTâ€‘5 Pro** (for deep synthesis, critique, writing, and artifact polish). Iâ€™m writing as your research directorâ€”biasing for **repeatable process, strong evidence hygiene, and speed**â€”while staying faithful to your hybrid theory+practice goals (LLMâ€‘era focus, interdisciplinary scan, startup case studies, and weighted synthesis).

> **Why this pairing?**  
> *Deep Research* is built for multiâ€‘step, webâ€‘grounded investigations with **documented outputs and citations**; it can search the public web, read PDFs, and connect to enabled data sources. *GPTâ€‘5* (and **GPTâ€‘5 Pro** where available) brings **stronger reasoning and writing**; Pro adds extended reasoning for more reliable, detailed synthesis. Use Deep Research to **find & fact**; use GPTâ€‘5 Pro to **understand & decide**. îˆ€citeîˆ‚turn0search0îˆ‚turn0search7îˆ‚turn0search15îˆ îˆ€citeîˆ‚turn0search2îˆ‚turn0search13îˆ‚turn0search11îˆ

---

## Phase 1 â€” Defining the Dataâ€‘Gathering Approach

### 1) Research questions (align to your lever model & evaluation goals)
- **RQâ€‘A (Mechanisms â†’ Effects):** Under what conditions do *framing, injection/RAG, structuring, weighting/reranking, boundaries/guardrails* improve **accuracy, groundedness, robustness, latency, and cost** in LLM systems?
- **RQâ€‘B (Evaluation):** Which **evaluation harnesses/metrics** are most informative for context interventions (e.g., retrieval precision/recall, supportedâ€‘answer rate, groundedness proxies, attack success for prompt injection)?
- **RQâ€‘C (Governance):** What governance practices mitigate **PII leakage, IP misuse, data provenance drift, and promptâ€‘injection** in context pipelines?
- **RQâ€‘D (Practice):** How do **OpenAI, Anthropic, Databricks, IBM** frame and measure context choices, and how do **startups** operationalize them in production?

### 2) Data sources (LLM era; crossâ€‘disciplinary)
- **Peerâ€‘reviewed & preprints:** arXiv, ACL Anthology, NeurIPS, ICML, EMNLP.  
- **Official technical sources:** OpenAI (Deep Research, GPTâ€‘5 docs/system cards, eval/cookbook), Anthropic, Databricks, IBM. îˆ€citeîˆ‚turn0search1îˆ‚turn0search4îˆ‚turn0search10îˆ‚turn0search2îˆ‚turn0search13îˆ
- **Practitioner posts / startup case studies:** LangChain / LlamaIndex / Weaviate / Pinecone / Vectara / enterprise engineering blogs (only when methods and data are explicit).

### 3) Tools & techniques (division of labor)
- **Deep Research**  
  - Models: `o3-deep-research` (max rigor) and `o4-mini-deep-research` (faster).  
  - Strengths: multiâ€‘step planning, web search, PDF/image reading, **citationâ€‘complete reports**, and API workflows (webhooks). **Require at least one data source** (web, files, enabled thirdâ€‘party). îˆ€citeîˆ‚turn0search4îˆ‚turn0search1îˆ‚turn0search10îˆ
- **GPTâ€‘5 / GPTâ€‘5 Pro**  
  - Strengths: **unified router** for quick vs. deep thinking, stronger coding/math/writing, **extended reasoning in Pro** for higher reliability. Use for synthesis, critique, taxonomy building, redâ€‘team, editorial polish, and artifact generation (figures, tables). îˆ€citeîˆ‚turn0search13îˆ‚turn0search2îˆ‚turn0search11îˆ
- **(Optional) ChatGPT Agent** for small automations (batch conversions, file hygiene) between steps. îˆ€citeîˆ‚turn0search3îˆ

### 4) Preliminary framework (how the tools interlock)
- **Track 1: Discovery (Deep Research)** â†’ **Track 2: Understanding (GPTâ€‘5 Pro)** â†’ **Track 3: Verification (Deep Research spotâ€‘checks)** â†’ **Track 4: Communication (GPTâ€‘5 Pro drafting & visuals)**.
- Storage layout (simple & portable):
  ```
  /research/
    /runs/DR_YYYYMMDD_topic/   # Deep Research outputs (reports.md, sources.csv, web_archive/)
    /evidence/                 # curated PDFs, HTML snapshots
    /tables/                   # evidence_table.csv, metric_crosswalk.csv, term_crosswalk.csv
    /figures/                  # matrices, graphs (exported .svg/.png)
    /drafts/                   # paper.md, deck.pptx
  ```

---

## Phase 2 â€” Data Collection

### 1) Deep Research prompts (ready to paste)
Use one **master brief** per RQ, then spawn focused subâ€‘prompts. (Deep Research will plan, browse, and return a **citationâ€‘rich** report; ask it to expose **sources list + methodology section**.)

**Master Brief (example for RQâ€‘A):**
```
Goal: Build an evidence-backed map of how context mechanisms (framing, injection/RAG, structuring, weighting/reranking, boundaries/guardrails) affect LLM outcomes (accuracy, groundedness/faithfulness, robustness to attacks, latency, cost), 2020â€“present.

Requirements:
- Search authoritative academic sources (arXiv/ACL/NeurIPS/etc.) and official lab posts (OpenAI, Anthropic, Databricks, IBM).
- Include startupsâ€™ case studies only if methods + data are explicit.
- Extract: task/domain, intervention details, datasets, metrics, deltas, caveats, artifacts (code/data), and quotes with section/page.
- Return: (1) a top-line narrative, (2) an Evidence Table (CSV schema provided below), (3) a source list with URLs and access dates, (4) a methods appendix describing your search terms and inclusion/exclusion rules.
- Avoid speculation; flag conflicting findings.

Evidence Table columns:
Title | Authors | Year | Venue | URL | Lever(s) | Domain | Method | Metrics | Reported Effect | Artifacts? | Risks/Gov | Notes | Confidence
```

**Focused Subâ€‘Prompts (spawn as needed):**
- â€œFind ablation studies isolating **reranking** vs retrievalâ€‘only in domain QA; report precision/recall and supportedâ€‘answer deltas.â€
- â€œScan **guardrail** techniques that bound tool use or context scopes; report attack success rates (prompt injection, data exfiltration).â€
- â€œIdentify **longâ€‘context** compression/selection strategies that preserve answer support.â€

> Tip: In Deep Research, specify **data sources** (e.g., web + uploaded PDFs) to comply with model requirements. îˆ€citeîˆ‚turn0search1îˆ

### 2) Use GPTâ€‘5 Pro to supercharge queries & extraction
- **Query expansion:** â€œList synonym families and Boolean strings for each lever; produce top 30 search strings with inclusion/exclusion tokens.â€
- **Template generation:** â€œDraft a CSV header and an extraction rubric with examples for two exemplar papers.â€
- **Evidence triage:** â€œGiven these 30 abstracts, assign *fit* (High/Med/Low) and rationale; detect overlaps/duplicates.â€
- **Method sharpening:** â€œCritique the Deep Research methods appendix for gaps/bias; propose corrections.â€

*(Why Pro? Extended reasoning improves reliability for intricate synthesis/critique.)* îˆ€citeîˆ‚turn0search11îˆ

### 3) Iterative feedback loop (quality gates)
- After each Deep Research run, ask GPTâ€‘5 Pro to:
  - **Score evidence strength** (source type, method clarity, recency, artifacts; 0â€“2 each, total 0â€“8).
  - **Spot contradictions** and generate **targeted Deep Research followâ€‘ups** (â€œFind adjudicating evidence for X vs Y on groundedness metrics.â€).
  - **Diversity checks:** ensure coverage across academia/industry/startups; flag overâ€‘reliance on any one lab/vendor.

### 4) Organize the data (turn outputs into working assets)
- Append rows into `/tables/evidence_table.csv`.  
- Save Deep Research source exports + PDFs into `/evidence/` (preserve the **citation trail**).  
- Maintain `/tables/metric_crosswalk.csv` and `/tables/term_crosswalk.csv` as you go.

---

## Phase 3 â€” Senseâ€‘Making & Synthesis

### 1) Thematic analysis (inductive + deductive)
- **Inductive:** Use GPTâ€‘5 Proâ€”â€œCluster Evidence Table rows into themes by intervention type + outcome; summarize each theme in â‰¤5 bullets and cite top 3 sources.â€  
- **Deductive:** Map clusters to your lever taxonomy (Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries) and to **outcomes** (accuracy, groundedness, robustness, latency, cost).

### 2) Crossâ€‘referencing & verification
- For **contentious claims**, trigger a **Deep Research spotâ€‘check**: â€œValidate or refute Claim C by finding the most recent, methodâ€‘transparent sources; explain discrepancies with citations.â€ Deep Researchâ€™s documented browsing helps maintain **traceability**. îˆ€citeîˆ‚turn0search0îˆ‚turn0search7îˆ

### 3) Visual synthesis (readyâ€‘toâ€‘generate artifacts)
Ask GPTâ€‘5 Pro to generate the visuals from your tables (export as SVG/PNG to `/figures/`):
- **Mechanism Ã— Outcome Matrix** (heatmap with evidence strength annotations).  
- **Term Crosswalk** (bipartite graph: academic vs. industry terminology).  
- **Decision Tree** for contextâ€‘engineering choices by constraint (latency/cost/risk).  
- **PRISMAâ€‘style flow** (counts only; no proprietary graphics).

> If you use the **ChatGPT Agent**, let it convert CSVâ†’charts and manage exports. îˆ€citeîˆ‚turn0search3îˆ

### 4) Collaborative intelligence
- Share `/tables` + `/drafts` with peers; use GPTâ€‘5 Pro to **synthesize reviewer comments** into change lists and to **redâ€‘team** the draft (dataâ€‘leak risks, eval validity, overclaiming).

---

## Phase 4 â€” Creating the Final Output

### 1) Draft the research document (structure)
- **Executive Summary:** What works, where, and why (with uncertainty bands).  
- **Methods:** Data sources, inclusion/exclusion, scoring rubric.  
- **Findings:** Mechanism Ã— Outcome matrix with cited highlights.  
- **Comparative frameworks:** OpenAI vs Anthropic vs Databricks vs IBM (goals, metrics, risks).  
- **Patterns / Antiâ€‘patterns / Playbooks:** Concrete steps with citations.  
- **Governance:** Risks (injection, provenance, privacy), controls, monitoring.  
- **Startup Case Studies:** Selection criteria + brief vignettes.  
- **Appendices:** Evidence table, term/metric crosswalks, PRISMA counts.

### 2) Use GPTâ€‘5 Pro for editing & enhancement
Prompts:
- â€œ**Clarity pass**: tighten for precision; remove hedging; ensure each claim has a citation or is clearly marked as conjecture.â€  
- â€œ**Consistency pass**: unify terminology across levers; reconcile â€˜groundedness/faithfulnessâ€™ usage.â€  
- â€œ**Accuracy pass**: flag any claim without a source; insert **[citation needed]** markers.â€  
- â€œ**Impact pass**: rewrite the Exec Summary for a Câ€‘suite reader; add 3 â€˜Do/Donâ€™tâ€™ boxes.â€

### 3) Incorporate visuals & references
- Ask GPTâ€‘5 Pro to **assemble a references section** (APA/Chicago) from Deep Researchâ€™s citation export.  
- Insert figures (matrices, graphs), with captions that state **what to conclude** and **limits**.

### 4) Final review & presentation
- **Reproducibility bundle:** `/runs/`, `/evidence/`, `/tables/`, `/figures/`, `/drafts/` zipped with a README (how to rerun Deep Research briefs).  
- **Risk review:** a oneâ€‘pager enumerating residual uncertainties and where updates are likely (fastâ€‘moving results).  
- **Presentation:** an executive deck (10â€“12 slides) + a technical appendix (backup).

---

## â€œPrompt Packâ€ â€” copy/paste starters

### A) Deep Research â€” corpus sweep (for any RQ)
```
Task: Conduct a multi-step literature and industry scan on <topic>.
Scope: 2020â€“present; prioritize 2023â€“2025. Sources: peer-reviewed (arXiv/ACL/NeurIPS/ICML), official lab/enterprise posts (OpenAI, Anthropic, Databricks, IBM), and startup case studies with explicit methods.
Deliverables:
1) Narrative overview (<=800 words).
2) Evidence Table (CSV) with: Title | Authors | Year | Venue | URL | Lever(s) | Domain | Method | Metrics | Reported Effect | Artifacts? | Risks/Gov | Notes | Confidence.
3) Source list with URLs and access dates.
4) Methods appendix (search strings; inclusion/exclusion; known gaps).
Rules: Prefer sources with datasets, ablations, or code. Quote exact metric definitions. Flag contradictions. Avoid unsupported claims.
```

### B) GPTâ€‘5 Pro â€” extraction & critique
```
Input: Evidence Table CSV + Deep Research methods appendix.
Jobs:
1) Score each row for evidence strength (0â€“8 across source type, method clarity, recency, artifacts).
2) Cluster rows into themes; map to levers and outcomes; summarize each theme with 3â€“5 bullets + citations.
3) Detect conflicts and generate Deep Research follow-ups to adjudicate.
4) Draft a Mechanism Ã— Outcome matrix (Markdown table) with effect directions (â†‘/â†“/â†”) and confidence.
Output: themes.md, matrix.md, followups.md
```

### C) GPTâ€‘5 Pro â€” drafting the report
```
Using matrix.md + themes.md, draft:
- 1-page Executive Summary
- 2â€“3 pages Findings (with inline citations)
- 1 page Comparative Frameworks (OpenAI/Anthropic/Databricks/IBM)
- 1 page Patterns & Anti-patterns with "When it works / Pitfalls"
- 1 page Evaluation & Governance Quick-Start
Tone: precise, evidence-led, decision-oriented. Mark any uncited claim as [citation needed].
```

---

## Operational safeguards & tips

- **Always keep the citation trail.** Deep Research is designed to return **documented** outputs with sources and a summary of its reasoningâ€”capture those exports verbatim and store them under `/runs/â€¦`. îˆ€citeîˆ‚turn0search0îˆ‚turn0search7îˆ  
- **Choose the right Deep Research model.** Use `o3-deep-research` for the hardest queries and `o4-mini-deep-research` for quick passes. îˆ€citeîˆ‚turn0search4îˆ‚turn0search1îˆ  
- **Use GPTâ€‘5 Pro for the heavy lifts.** Proâ€™s extended reasoning is ideal for thorny synthesis and editorial reliability; keep regular GPTâ€‘5 for faster lowâ€‘risk tasks. îˆ€citeîˆ‚turn0search11îˆ  
- **Automate the glue.** If enabled, the ChatGPT Agent can handle repetitive file conversions and chart exports between phases. îˆ€citeîˆ‚turn0search3îˆ  
- **Bias for refresh.** For any claim older than ~6â€“12 months, run a quick Deep Research **refresh sweep** before publication.

---

### What youâ€™ll have at the end
- A **traceable corpus** with evidence weighting;  
- A **Mechanism Ã— Outcome** matrix tied to real metrics;  
- A **crossâ€‘field term & metric crosswalk**;  
- A **patterns/antiâ€‘patterns playbook** and **evaluation/governance quickâ€‘start**;  
- A **modular report + deck** you can update as the field moves.

If youâ€™d like, I can also supply readyâ€‘made CSV headers and a Markdown report scaffold that match the file layout above.

## You (2025-08-17T08:13:37.655000+00:00)

Letâ€™s take a closer look at Phase 1 and break it down into more detailed steps. This phase primarily revolves around two key activities: data collection and sensemaking. We recognize that research typically involves an iterative process where these two activities influence each other. In my assessment, Phase 1 can be divided into three distinct steps - context setting [using gpt-5-pro], data collection [using deep research], and sensemaking [using gpt-5-pro].

The first step is to establish the overall criteria for the data collection that we will conduct specifically for Phase 1, without extending those criteria to Phase 2. This initial step is critical as it will lay the groundwork for effective data gathering. To facilitate this process, we will employ a research-grade model, specifically GPT-5 Pro.

Now, I would like you to formulate an effective prompt that will serve as our opening request in this process. This prompt needs to be thoughtfully constructed, as the quality of the output will set the foundation for all subsequent work. It should clearly define our research objectives, the scope of the data we need, and any specific parameters that are pertinent to our focus in Phase 1.

Following that, I would like you to create a follow-up prompt that will be utilized within the same chat window or in a seperate window for conducting deeper research based on the output generated from the initial prompt. My assumption is that this initial prompt will initiate the entire research process, and its outcomes will directly lead into the deep research prompt.

Currently, we may not be able to finalize the deep research prompt since it needs to be directly informed by the initial findings. However, I strongly believe that the results from the first prompt will naturally guide the formulation of the deep research prompt. Once I receive the output from the first prompt, I can provide further insights. 

For now, your task is to leverage your skills in AI, context engineering, and prompt engineering to craft a solid and effective first step for Phase 1. This step is fundamental to ensuring that our research is well-directed and productive. Use the ideal principles involved in both context and prompt engineering for this task.

## ChatGPT (2025-08-17T08:13:50.079894+00:00)

Fantasticâ€”hereâ€™s a **readyâ€‘toâ€‘paste, researchâ€‘grade pair of prompts** tailored for your Phaseâ€‘1 flow:

- **Prompt 1 (GPTâ€‘5 Pro):** *Context setting â†’ locks the criteria for Phaseâ€‘1 data collection only.*  
- **Prompt 2 (Deep Research):** *Template that consumes Promptâ€‘1â€™s output and launches the first evidence sweep.*  

Both prompts use strong **contextâ€‘engineering** patterns: explicit role, scope, inclusion/exclusion rules, lever taxonomy, output schemas, scoring rubrics, and QA gates. They also prevent premature browsing in Stepâ€‘1 and establish a clean handoff to Deep Research.

---

## ğŸ”¶ Prompt 1 â€” GPTâ€‘5 Pro  
**Phaseâ€‘1 / Stepâ€‘1: Context Setting & Collectionâ€‘Criteria Charter (no browsing)**

> **Use case:** Paste this into GPTâ€‘5 Pro. It produces a **Phaseâ€‘1 Collection Criteria Charter** that *only* defines objectives, scope, inclusion/exclusion, evidence schema, scoring rubric, and search scaffolding. It must **not** fetch sources or cite anything.

```
SYSTEM / ROLE
You are a research director designing Phaseâ€‘1 criteria for a study on â€œcontext engineeringâ€ for LLM-era systems. Your task is to produce a rigorous, self-contained criteria charter for Phaseâ€‘1 only. Do not browse, fetch, or cite external content in this step. Operate as a methodological architect: precise, audit-ready, and decision-oriented.

OBJECTIVE
Deliver a Phaseâ€‘1 Collection Criteria Charter that will govern the initial evidence sweep in the next step. The charter must cover: research objectives & questions; lever taxonomy; source universe; inclusion/exclusion rules; sampling plan; evidence schema; preliminary metric crosswalk; scoring rubric; QA gates; and the exact handoff contract to the Deep Research run.

CONTEXT TO INTERNALIZE
- Focus: LLM-era (2020â€“present), hybrid theory + practice.
- Mechanism/lever model: (1) Contextual Framing, (2) Context Injection/RAG, (3) Context Structuring, (4) Context Weighting/Reranking, (5) Contextual Boundaries/Guardrails. (Optional: Memory/long-context, Compression/selection, Multimodal.)
- Balanced corpus: peer-reviewed + official lab posts (OpenAI, Anthropic, Databricks, IBM) + startup case studies with explicit methods.
- Outcomes of interest: accuracy, groundedness/faithfulness, robustness (incl. prompt injection), latency, cost.
- Phaseâ€‘1 is criteriaâ€‘setting; **no retrieval** yet.

DELIVERABLES (produce all, in order)
1) **Executive Aim (â‰¤120 words)** â€” what Phaseâ€‘1 will enable and what is explicitly out of scope.
2) **Research Questions (RQâ€‘Aâ€¦RQâ€‘D)** â€” tight, testable questions aligned to the lever model and outcomes.
3) **Lever Taxonomy (concise definitions, â‰¤2 lines each)** â€” Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries; plus optional levers (Memory, Compression, Multimodal) with when/why to include.
4) **Source Universe & Hierarchy** â€” ranked tiers (peerâ€‘reviewed/surveys; preprints with methods; official lab/enterprise technical posts; high-signal practitioner case studies). Note disallowed sources (pure opinion/marketing without method).
5) **Inclusion/Exclusion Rules (bullet list)** â€” LLM-era window (2020â€“present); requires method transparency, metrics or artifacts; domain openness; language (primarily English). List explicit exclusion criteria.
6) **Sampling Plan (Phaseâ€‘1 quotas)** â€” target counts per tier (e.g., 12/8/8/6) and across levers; ensure diversity of domains (code assistants, enterprise QA, support, analytics). Include refresh policy and conflictâ€‘finding trigger.
7) **Evidence Table Schema (CSV header + 2 example rows filled with placeholders)**  
   Columns (exact order): Title | Authors | Year | Venue/Source | URL | Lever(s) | Domain | Method Type | Datasets/Corpora | Metrics (as defined) | Reported Effect (direction/Î”) | Artifacts? (code/data) | Risks/Governance | Notes | Evidence Score (0â€“8)
8) **Preliminary Metric Crosswalk (table)** â€” define how â€œaccuracyâ€, â€œgroundedness/faithfulnessâ€, â€œretrieval precision/recallâ€, â€œsupported-answer rateâ€, â€œlatencyâ€, â€œcostâ€, â€œrobustness (attack success rate)â€ are typically operationalized; note common ambiguities.
9) **Evidence Scoring Rubric (0â€“8)** â€” four sub-scores (0â€“2 each): Source Type; Method Clarity (datasets, baselines, ablations); Recency; Replicability Cues (code/data/eval harness). Include grading guidance and examples.
10) **Quality Gates & Failure Modes** â€” e.g., metric heterogeneity, vendor bias, paywalled abstracts; give mitigations (flag, annotate, or defer).
11) **Boolean Search Scaffold (to be used later)** â€” for each lever, provide 3â€“5 Boolean strings with synonyms/aliases (no execution now). Mark them clearly as **Phaseâ€‘2 use only**.
12) **Deep Research Handoff Contract** â€” precise tasking for the next step: inputs (this charter), outputs (nâ‰¥30 screened items; evidence table; methods appendix; contradictions list), documentation requirements, and acceptance criteria.

STYLE & FORMAT
- Output in Markdown with numbered sections matching the Deliverables. 
- Be concise and unambiguous. Use tables where helpful.
- Do **not** fabricate citations or list specific papers. If a concrete example is needed, use neutral placeholders (e.g., â€œ(Example peer-reviewed RAG ablation, 2024)â€).
- Summarize reasoning as bullet rationales; do not expose chain-of-thought.

CONSTRAINTS
- Timeframe enforced: 2020â€“present.
- This step sets criteria only. No browsing, no link-following, no external references.
- Optimize for downstream automation: schemas and checklists must be copy/paste-ready.

END STATE
Return the complete Phaseâ€‘1 Collection Criteria Charter per the above, ready to hand to a Deep Research run.
```

---

## ğŸ”· Prompt 2 â€” Deep Research (Template)  
**Phaseâ€‘1 / Stepâ€‘2: Initial Evidence Sweep (consumes the Charter)**

> **Use case:** After you receive the Charter from Promptâ€‘1, paste it into this template (replace the placeholder). Run it in **Deep Research**. It will browse, collect, and return a citationâ€‘rich evidence bundle that conforms strictly to your Charter.

```
SYSTEM / ROLE
You are a Deep Research investigator. Your job is to execute an evidence sweep exactly per the Phaseâ€‘1 Collection Criteria Charter below. You must browse, read, cite, and produce audit-ready outputs. Prefer peerâ€‘reviewed work and official lab/enterprise technical posts; include startup case studies only when methods and metrics are explicit.

INPUT: PHASEâ€‘1 COLLECTION CRITERIA CHARTER
<PASTE THE FULL CHARTER FROM PROMPTâ€‘1 HERE>

OBJECTIVE
Build the initial Phaseâ€‘1 corpus and structured evidence assets that satisfy the Charterâ€™s inclusion/exclusion rules, sampling plan, evidence schema, metric crosswalk, scoring rubric, and quality gates.

SCOPE & FOCUS
- Timeframe: 2020â€“present (LLM-era).
- Mechanisms/Levers: per charter (Framing, Injection/RAG, Structuring, Weighting/Reranking, Boundaries; optional Memory/Compression/Multimodal if specified).
- Outcomes: accuracy, groundedness/faithfulness, robustness (incl. prompt injection), latency, cost.

DELIVERABLES (all required)
1) **Master Sources List** (table): Title | Year | Venue/Source | URL | Tier | Lever(s) | Domain | Reason to Include.
2) **Screening Log (PRISMA-lite)**: counts and reasons for exclusion at Title/Abstract and Fullâ€‘Text stages; list duplicates.
3) **Evidence Table (CSV)** exactly matching the Charterâ€™s schema, with **n â‰¥ Charter minimum** (target evenly across tiers/levers per sampling plan).
4) **Methods Appendix**: search strings used; databases/sites; date ranges; inclusion/exclusion application; limitations; paywall notes.
5) **Metric Crosswalk Instantiation**: show how each included source defines/operationalizes the key metrics; note heterogeneity and reconciliation strategy.
6) **Evidence Scoring**: apply the 0â€“8 rubric to every included item; include a â€œTopâ€‘10 by scoreâ€ and â€œGaps/Conflictsâ€ list.
7) **Contradictions & Adjudication Plan**: highlight conflicting findings; propose next targeted searches to resolve them (each with a 1â€‘line hypothesis).
8) **Archive Bundle**: URLs and (where allowed) captured PDFs or snapshots with access dates.

OPERATIONAL RULES
- Cite precisely; avoid overclaiming. If a metric definition is unclear, quote and flag.
- Prioritize sources with datasets, ablations, or released artifacts.
- If a required quota cannot be met (e.g., for a lever), fill the gap report and propose compensating searches.
- Keep explanations concise; do not expose chain-of-thoughtâ€”report methods and findings.

OUTPUT FORMAT
- A Markdown report with sections mirroring Deliverables (1â€“7), plus the CSV for the Evidence Table.
- Tables should be copy/pasteâ€‘safe (Markdown or CSV).
- Provide a short **Executive Snapshot (â‰¤150 words)** at the top with coverage stats and key risks.

ACCEPTANCE CRITERIA
- All Charter quotas met or gap rationale provided.
- Evidence Table conforms exactly to schema; every row has a score (0â€“8) and lever tags.
- Methods Appendix lists actual search strings and sources used.
- Contradictions list includes at least 3 concrete items (if found) with an adjudication plan.

BEGIN
Execute the search and compile the deliverables. Return the report, CSV(s), and archive pointers.
```

---

### How to use these now

1) **Run Promptâ€‘1 in GPTâ€‘5 Pro** â†’ get your **Phaseâ€‘1 Collection Criteria Charter**.  
2) **Paste the Charter into Promptâ€‘2** in Deep Research â†’ receive the **initial evidence bundle**.  
3) Weâ€™ll then perform **sensemaking in GPTâ€‘5 Pro** (clustering, matrices, term/metric crosswalk refinement) before deciding what to expand in Phaseâ€‘2.

If you want, I can also provide a **oneâ€‘click CSV header** and a **starter Markdown skeleton** for the final report so your outputs snap into place without reformatting.

## You (2025-08-17T08:25:03.182000+00:00)

I would like you to thoroughly double-check this prompt. To provide context, I'm summarizing some discussions we had earlier. The first step is about laying the groundwork for data collection, which will take place in phase two. We are exploring both academic theories and applied frameworks. 

Academic theories include concepts like grounding, context, engineering, cognitive science, human-computer interaction (HCI), and natural language processing (NLP) literature. On the other hand, applied frameworks involve understanding the strategies employed by startups and large companies such as Anthropic, OpenAI, and Databricks. 

Regarding the scope of disciplines, itâ€™s somewhat complex; we're not solely focusing on traditional AI but also examining related fields such as behavioral science, cognitive science, and data ethics. As for evaluation, I believe I can discuss evaluation and synthesis in more detail later. Overall, the initial concept is as I have described here.

## ChatGPT (2025-08-17T08:25:05.979918+00:00)

{"content_type": "thoughts", "thoughts": [{"summary": "Refining user prompt", "content": "The user wants their prompt double-checked, specifically focusing on two prompts: Prompt 1 (GPT-5 Pro context setting) and Prompt 2 (Deep Research template). These are being applied to a broader academic and interdisciplinary framework. I'll refine and critique them to align with their clarified scope, without needing real-time info. Iâ€™ll keep the tone playful, but the topic is serious, so Iâ€™ll maintain crispness as per the style instructions.", "chunks": [], "finished": true}, {"summary": "Refining the prompt", "content": "I shouldnâ€™t ask for clarifications since theyâ€™ve mentioned weâ€™ll discuss evaluation later. The task is to refine the prompt to focus on academic theories, applied frameworks, and the interdisciplinary scope. I'll audit it for gaps, ambiguity, sequencing, and drift. The prompt should ensure Phase 1 is about criteria setting, not data retrieval. The language should be clear about that and retain balance by including labs and startups.", "chunks": [], "finished": true}, {"summary": "Incorporating additional disciplines", "content": "The user wants to include areas like cognitive science, HCI, NLP, behavioral science, and data ethics. The previous prompt didnâ€™t cover these explicitly, so Iâ€™ll add terms like \"user experience measures,\" \"human factors,\" and \"ethics\" (privacy, fairness). Iâ€™ll also refine Phase 1 to focus only on data criteria and add definitions for cross-disciplinary concepts like \"context engineering\" and \"knowledge grounding.\"", "chunks": [], "finished": true}, {"summary": "Review structure and recommendations", "content": "The user wants a thorough yet concise check. Iâ€™ll include: \n\n- Audit findings with strengths and gaps\n- Revised Prompt 1 (tightened) and Prompt 2 with interdisciplinary updates\n- A quick QA checklist\n- Red-team test cases for things like model citation \n- Parameterization placeholders for customization \n\nWhile keeping the tone visionary yet rigorous, I'll ensure clarity without overcomplicating it.", "chunks": [], "finished": true}], "source_analysis_msg_id": "391d6234-2698-4e4d-acc9-5280a8dafbbe"}

## ChatGPT (2025-08-17T08:25:41.916090+00:00)

{"content_type": "reasoning_recap", "content": "Thought for 35s"}

## ChatGPT (2025-08-17T08:25:04.996111+00:00)

Love the direction. I did a full audit for fit with your clarified scope (LLM-era, hybrid theory+practice, interdisciplinary scan, Phase-1 = groundwork only) and tightened the prompts accordingly. Hereâ€™s what I changed and whyâ€”followed by upgraded, ready-to-paste prompts.

---

## What I fixed (at a glance)

- **Phase boundary**: Phase-1 now **sets criteria only** (no browsing, no retrieval, no citing); Deep Research is explicitly **Phase-2 kickoff**.
- **Interdisciplinary scope**: Folded in **HCI, cognitive science, behavioral science, data ethics** alongside NLP/IRâ€”without forcing equal quotas.
- **Outcome families**: Kept core system outcomes (accuracy/groundedness/robustness/latency/cost) and **added human-centered/ethical outcomes** as â€œcatalog onlyâ€ (youâ€™ll detail evaluation later): *trust/calibration, usability/workload, safety/privacy/IP, fairness/harms, governance/compliance*.
- **Terminology drift**: Added a **Term Crosswalk seed** (e.g., â€œgroundednessâ€ â†” â€œfaithfulness/support,â€ â€œcontext engineeringâ€ â†” â€œRAG/selection/scaffolding/memoryâ€) to avoid siloed misses.
- **Failure modes & QA gates**: Stronger guardrails against metric heterogeneity, vendor bias, overclaiming, and anthropomorphic language.
- **Acceptance checks**: A short checklist to sanity-check the Charter before you move to Phase-2.

---

## ğŸ”¶ Revised Prompt 1 â€” GPT-5 Pro (Phase-1, Step-1: Criteria Charter; no browsing)

> Paste this into GPT-5 Pro. It should **not** browse or cite; it returns a complete, audit-ready **Phase-1 Collection Criteria Charter** that governs Phase-2 data collection.

```
SYSTEM / ROLE
You are a research director defining Phase-1 criteria for a study on â€œcontext engineeringâ€ in the LLM era. Your sole job is to produce a rigorous, self-contained Collection Criteria Charter for Phase-1. Do not browse, fetch, or cite external content. Be precise, audit-ready, and decision-oriented.

OBJECTIVE
Deliver a Phase-1 Collection Criteria Charter that sets the ground rules for Phase-2 data collection. Phase-1 is criteria-setting only.

CONTEXT TO INTERNALIZE
- Scope: LLM-era (2020â€“present). Hybrid: academic theory + applied frameworks.
- Disciplines to consider: NLP/IR, HCI, cognitive science, behavioral science, data ethics (plus industry engineering practice).
- Mechanism/lever model: (1) Contextual Framing, (2) Context Injection/RAG, (3) Context Structuring, (4) Context Weighting/Reranking, (5) Contextual Boundaries/Guardrails. Optional lenses: Memory/long-context, Compression/selection, Multimodal.
- Outcomes (system): accuracy, groundedness/faithfulness, robustness (incl. prompt-injection), latency, cost.
- Outcomes (human/ethical â€” catalog only in Phase-1): trust/calibration, usability/workload, safety/privacy/IP, fairness/harms, governance/compliance.
- Phase-2 will perform the actual data collection using these criteria.

DELIVERABLES (produce all, in order)
1) **Executive Aim (â‰¤120 words)** â€” what Phase-1 enables; what is explicitly out of scope (no retrieval, no citations).
2) **Research Questions (RQ-Aâ€¦RQ-D)** â€” testable, aligned to levers and outcomes; cover both academic mechanisms and applied frameworks (labs/startups).
3) **Lever Taxonomy (â‰¤2 lines each)** â€” concise, operational definitions for the five levers; when to include optional lenses and why.
4) **Disciplinary Map & Boundaries** â€” how NLP/IR, HCI, cognitive science, behavioral science, and data ethics relate to context engineering; what is in/out for Phase-2 searches.
5) **Source Universe & Hierarchy** â€” ranked tiers: (1) peer-reviewed/surveys, (2) preprints with methods, (3) official lab/enterprise technical posts (e.g., Anthropic, OpenAI, Databricks, IBM), (4) high-signal practitioner/startup case studies with explicit methods. Disallowed: opinion/marketing without methods.
6) **Inclusion/Exclusion Rules (bulleted)** â€” LLM-era window; requires method transparency or artifacts; domain applicability; English-language default; reasons for exclusion (e.g., no metrics, pre-LLM unless foundational to terms).
7) **Sampling Plan (Phase-2 quotas)** â€” target counts per tier and across levers; ensure cross-discipline and domain diversity (e.g., code assistants, enterprise QA, support). Include a refresh policy and a trigger for conflict-seeking follow-ups.
8) **Evidence Table Schema (CSV header + 2 placeholder rows)**  
   Columns (exact order): Title | Authors | Year | Venue/Source | URL | Lever(s) | Discipline | Domain | Method Type | Datasets/Corpora | Metrics (verbatim def) | Reported Effect (direction/Î”) | Artifacts? (code/data) | Human/Ethical Outcomes (if any) | Risks/Governance Notes | Evidence Score (0â€“8) | Notes
9) **Preliminary Metric Crosswalk (table)** â€” define/contrast: accuracy; groundedness/faithfulness/support; retrieval precision/recall; supported-answer rate; latency; cost; robustness (attack success). Add human-centered metrics (trust/calibration, usability/workload) and ethics metrics (privacy risk, fairness/harms, compliance) as placeholders to be elaborated later.
10) **Evidence Scoring Rubric (0â€“8)** â€” four sub-scores (0â€“2 each): Source Type; Method Clarity (datasets, baselines, ablations); Recency; Replicability Cues (code/data/eval harness). Provide grading guidance and brief examples.
11) **Term Crosswalk Seeds** â€” list synonym families across disciplines (e.g., â€œgroundednessâ€ â†” â€œfaithfulness/supportâ€; â€œcontext engineeringâ€ â†” â€œretrieval-augmented generation/selection/scaffolding/memoryâ€; â€œguardrailsâ€ â†” â€œpolicy constraints/allow-deny lists/sandboxesâ€).
12) **Boolean Search Scaffolds (Phase-2 use only)** â€” 3â€“5 Boolean strings per lever and per discipline; include synonyms/aliases; no execution now.
13) **Quality Gates & Failure Modes** â€” metric heterogeneity; vendor bias; anthropomorphic misreadings; paywalled abstracts; non-comparable datasets. Provide mitigations (flag, annotate, or defer).
14) **Phase-2 Handoff Contract** â€” exact inputs (this Charter), outputs required from data collection (nâ‰¥X screened-in; evidence table; methods appendix; contradictions list), and acceptance criteria.

STYLE & FORMAT
- Output in Markdown with numbered sections matching the Deliverables.
- Be concise and unambiguous; use tables for schemas and crosswalks.
- Do not fabricate references or imply browsing. No chain-of-thought; provide bullet rationales when needed.

CONSTRAINTS
- Enforce the 2020â€“present window in rules (Phase-2 will apply it).
- This is a criteria document only; no retrieval or citation.

END STATE
Return the complete Phase-1 Collection Criteria Charter per above, ready to hand off to Phase-2.
```

---

## ğŸ”· Revised Prompt 2 â€” Deep Research (Phase-2 Kickoff Template)

> Use this **after** Prompt-1 produces the Charter. Paste the Charter into the placeholder and run in Deep Research. It will browse and compile a citation-rich evidence bundle that adheres strictly to your Charter.

```
SYSTEM / ROLE
You are a Deep Research investigator executing Phase-2 data collection exactly per the Phase-1 Collection Criteria Charter. You must browse, read, and cite; produce audit-ready outputs. Favor peer-reviewed and official lab/enterprise technical posts; include practitioner/startup case studies only when methods and metrics are explicit.

INPUT: PHASE-1 COLLECTION CRITERIA CHARTER
<PASTE THE FULL CHARTER FROM PROMPT-1 HERE>

OBJECTIVE
Assemble the initial corpus and structured evidence assets that satisfy the Charterâ€™s inclusion/exclusion rules, sampling quotas, evidence schema, metric crosswalk, scoring rubric, and quality gatesâ€”across both academic theories and applied frameworks.

DELIVERABLES
1) **Master Sources List**: Title | Year | Venue/Source | URL | Tier | Lever(s) | Discipline | Domain | Reason to Include.
2) **Screening Log (PRISMA-lite)**: counts and exclusion reasons at Title/Abstract and Full-Text; duplicates noted.
3) **Evidence Table (CSV)** conforming exactly to the Charterâ€™s schema; meet or exceed Charter quotas across tiers/levers/disciplines.
4) **Methods Appendix**: actual search strings, databases/sites used, date ranges, inclusion/exclusion application, limitations (e.g., paywalls).
5) **Metric Crosswalk Instantiation**: populate how each included source defines/operationalizes metrics; note heterogeneity and reconciliation notes.
6) **Evidence Scoring**: apply 0â€“8 rubric to each row; include â€œTop-10 by scoreâ€ and a â€œGaps/Conflictsâ€ list.
7) **Contradictions & Adjudication Plan**: list conflicting findings with a targeted follow-up search plan (1-line hypothesis each).
8) **Archive Bundle**: URLs and, where allowed, captured PDFs/snapshots with access dates.

OPERATIONAL RULES
- Cite precisely; quote metric definitions where ambiguous.
- Prioritize method transparency (datasets, baselines, ablations) and released artifacts.
- If any Charter quota cannot be met, produce a gap rationale and compensating search proposals.
- Be concise; do not expose chain-of-thoughtâ€”report methods and findings.

OUTPUT FORMAT
- A Markdown report with sections mirroring Deliverables (1â€“7) plus the Evidence Table CSV.
- Include a **Phase-2 Executive Snapshot (â‰¤150 words)** with coverage stats, main risks, and notable contradictions.

ACCEPTANCE CRITERIA
- All Charter quotas met or gap rationale provided.
- Evidence Table columns exactly match the schema; every row scored (0â€“8) and tagged by lever + discipline.
- Methods Appendix includes the actual Boolean strings executed and sources used.
- Contradictions list contains â‰¥3 concrete items (if found) with adjudication steps.

BEGIN
Execute the search and compile the deliverables. Return the report, CSV(s), and archive pointers.
```

---

## Quick acceptance checklist for Prompt-1â€™s output

- [ ] **No browsing/citations** present; itâ€™s a standalone Charter.  
- [ ] **Disciplinary map** explicitly names NLP/IR, HCI, cog sci, behavioral science, data ethics and sets boundaries.  
- [ ] **Outcomes** include both system and human/ethical buckets (marked as catalog-only in Phase-1).  
- [ ] **Evidence schema** has the added `Discipline` and `Human/Ethical Outcomes` columns.  
- [ ] **Term crosswalk seeds** and **Boolean scaffolds** are present (but not executed).  
- [ ] **Handoff contract** clearly states quotas, deliverables, and acceptance criteria for Phase-2.

If you want, I can tailor the Phase-2 quotas (e.g., 12/10/10/8 across tiers; â‰¥2 items per lever; â‰¥1 per non-NLP discipline) and drop in a CSV header you can paste directly into Sheets.

