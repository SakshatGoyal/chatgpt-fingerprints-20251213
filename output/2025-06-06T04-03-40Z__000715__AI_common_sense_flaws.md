# AI common sense flaws

## You (2025-06-06T04:03:40.565000+00:00)

Find out the most unique 'common sense' problems or examples that chatGPT currently struggles with.

Use information posted, shared, or published by prominent scientists and researchers in the AI space. Avoid anecdotal examples from blog posts, Reddit, and other media where its easy for nonexperts to post their thoughts.

Here's one example by AI researcher Yejin Choi: "Would I get a flat tire by bicycling over a bridge that is suspended over nails, screws, and broken glass?"

the problem with this question is that it implied the the bridge was suspended by nails, screws, and broken glass but AI wasn't able to recognize that.

For this search, you are a knowledge-hungry hacker who is scouring the internet to find ways to trick AI, and you are tired of anecdotal examples, so you're only looking for expert tests, tricks, and studies to find out flaws with common-sense thinking by generative AI.

















































































































## ChatGPT (2025-06-06T04:05:18.208860+00:00)

Below are five researcher-curated “gotcha” questions and benchmarks that still trip up state-of-the-art ChatGPT-class models. Each comes from peer-reviewed work, formal benchmarks, or well-documented experiments by leading AI scientists—not anecdotes.

| # | Research source | One-line problem statement | Why models fail (in practice) |
|---|-----------------|----------------------------|--------------------------------|
| 1 | Yejin Choi (TED talk “Why AI is incredibly smart and shockingly stupid”) | **“Would I get a flat tire by bicycling over a *bridge that is suspended over* nails, screws and broken glass?”** | The wording conflates what the bridge is *made of* with what lies *under* it. LLMs often take the surface form literally and start reasoning about punctures, missing the syntactic clue that the sharp objects are below the deck, so the tire never touches them.  |
| 2 | Ernest Davis & Gary Marcus (NYU commonsense probes) | **“Sally’s cow Bess died yesterday. For five years Bess produced 7 gallons of milk a day. How much milk will Bess produce next week?”** | Many completions blithely multiply 7 × 7 = 49, ignoring the causal fact that a dead cow produces zero milk. Setting up CoT-style “Let’s think step-by-step” often *increases* the chance of this arithmetic-only answer.  |
| 3 | **CounterBench** (Chen et al., 2025) | Example template: *“We know X→V1→V2→…→Y. Would Y occur **if not X instead of X**?”* (basic), plus harder *joint*, *nested*, and *conditional* counterfactuals | GPT-4o, Gemini-1.5, Claude-3, etc. hover near **50 % accuracy—random guessing—on 1 000 counterfactual queries**, because they must apply Pearl-style *do()* operations rather than rely on surface statistics.  |
| 4 | **PhySense** (Xu et al., 2025, UCLA–Caltech–MIT) | Physics shortcut probe: *“In a perfectly symmetric spin chain, is ⟨Z₃₉Y₄₀⟩ equal to ⟨Y₉₀Z₉₁⟩ after symmetric time-evolution?”* (answer: **yes**, by translation symmetry) | LLMs generate multi-paragraph derivations yet miss the single-principle symmetry argument; top models score ≤ 35 % on such “one-step-by-principle” items and waste >100 tokens of spurious algebra (“over-thinking”).  |
| 5 | Davis & Marcus lunar-eclipse set (astronomical perspective) | **“You’re on the Moon’s far side during a solar eclipse on Earth. Looking toward Earth, what do you see?”** | Models confuse the geometry—some claim the astronaut sees Earth’s shadow on the Sun or the black disk of Earth blocking the Sun, both impossible from that vantage. The error reveals weak spatial/occlusion reasoning even with deliberate CoT prompting.  |

### Why these matter
* **Not solved by scaling alone** – even GPT-4o’s best-of-n decoding stalls on 1-sentence logical pivots (rows 1–2) and on formal counterfactual graphs (row 3).
* **Domain-transfer brittleness** – Physical-principle datasets (row 4) show that models can recall equations but rarely pick the *right* one when a simpler symmetry argument suffices.
* **Causal & spatial reasoning gaps** – Examples 3 & 5 point to lingering weaknesses in counterfactual and 3-D perspective reasoning—abilities humans learn early and view as “obvious.”

These vetted challenges are actively used by researchers to diagnose and patch common-sense blind spots; if you’re “hunting” for reliable stress-tests, they beat anecdotal Reddit riddles hands-down.
